{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matrosee/Genre-Klassifikation-anhand-von-Liedtexten/blob/main/SVM_LGBM_Code_von_Jannic_M_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyb6mPpCJADk"
      },
      "source": [
        "# Installationen, Imports und Downloads:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdh2wodVNzZE"
      },
      "source": [
        "Installationen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKornHEzn-PV"
      },
      "outputs": [],
      "source": [
        "!pip install lightgbm\n",
        "!pip install spacy\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install flaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPehy4oEN208"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0HqVeIzJJrR"
      },
      "outputs": [],
      "source": [
        "import flaml\n",
        "import lightgbm as lgb\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from flaml import AutoML\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from lightgbm import LGBMClassifier\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7cRPYzKN4tt"
      },
      "source": [
        "Downloads:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACQnvRGSR2uc"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D48vtzWGJAxi"
      },
      "source": [
        "# Upload & Dataframe Deklaration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlVoFF0ynk6D",
        "outputId": "db355530-8e78-4a82-ddf4-7d3e6a42f487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zqKSYp3Be9l"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCMkjgqaEDOy"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('/content/gdrive/MyDrive/test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRr671g-UD2w"
      },
      "source": [
        "# 1890 Songs-pro-Datensatz erstellen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtUibkdaUD2w"
      },
      "outputs": [],
      "source": [
        "# Nur Englische-Lieder einstellen\n",
        "df = df.loc[df['Language']=='en']\n",
        "\n",
        "# Pro Genre ein eigenes Dataframe mit je 1890 Liedern\n",
        "df_rock_one = df[df['Genre']=='Rock']\n",
        "df_rock_one = df_rock_one.head(1890)\n",
        "\n",
        "df_pop_one = df[df['Genre']=='Pop']\n",
        "df_pop_one = df_pop_one.head(1890)\n",
        "\n",
        "df_metal_one = df[df['Genre']=='Metal']\n",
        "df_metal_one = df_metal_one.head(1890)\n",
        "\n",
        "df_hip_hop_one = df[df['Genre']=='Hip-Hop']\n",
        "df_hip_hop_one = df_hip_hop_one.head(1890)\n",
        "\n",
        "df_rnb_one = df[df['Genre']=='R&B']\n",
        "df_rnb_one = df_rnb_one.head(1890)\n",
        "\n",
        "df_indie_one = df[df['Genre']=='Indie']\n",
        "df_indie_one = df_indie_one.head(1890)\n",
        "\n",
        "df_electronic_one = df[df['Genre']=='Electronic']\n",
        "df_electronic_one = df_electronic_one.head(1890)\n",
        "\n",
        "df_jazz_one = df[df['Genre']=='Jazz']\n",
        "df_jazz_one = df_jazz_one.head(1890)\n",
        "\n",
        "df_folk_one = df[df['Genre']=='Folk']\n",
        "df_folk_one = df_folk_one.head(1890)\n",
        "\n",
        "df_country_one = df[df['Genre']=='Country']\n",
        "df_country_one = df_country_one.head(1890)\n",
        "\n",
        "# Alle einzelnen Dataframes zu einem zusammfügen\n",
        "df_one = pd.concat([df_pop_one, df_hip_hop_one, df_metal_one, df_rock_one, \n",
        "                    df_indie_one, df_country_one, df_electronic_one, df_rnb_one,\n",
        "                    df_jazz_one, df_folk_one])\n",
        "\n",
        "# Kürzeren Namen für zuküftige Arbeit auswählen\n",
        "df = df_one\n",
        "\n",
        "# Spalten ohne Beiträge löschen \n",
        "df.dropna()\n",
        "\n",
        "# Index reseten\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hbI7RZuVHsq"
      },
      "source": [
        "# Pre-Processing CountVectorizer & TF-IDF Vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEN8vKbOrymD"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnv0C9WqBwFd"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(subset=['Lyrics'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krrAcvz7VHsq"
      },
      "outputs": [],
      "source": [
        "# Laden Sie das spacy-Modell und den Englisch-Vokabular\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Stop-Wörter auf einer Variable deklarieren\n",
        "en_stops = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGWeBqPjVHsr"
      },
      "source": [
        "Datensatz säubern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8R7uEVeVHsr"
      },
      "outputs": [],
      "source": [
        "# Sonderzeichen entfernen\n",
        "df['Lyrics'] = [re.sub(r'^.*?Lyrics', '', str(lyric)) for lyric in df['Lyrics']]\n",
        "\n",
        "# Satzumbruch Zeichen entfernen\n",
        "df['cleaned_lyrics'] = [str(lyric).replace('\\n',' ') for lyric in df['Lyrics']]\n",
        "\n",
        "# Wörter in eckigen Klammern entfernen\n",
        "df['cleaned_lyrics'] = [re.sub(\"\\[.*?\\]\",\"\",lyric) for lyric in df['cleaned_lyrics']]\n",
        "\n",
        "# Language-, Song-Name- und Artist-Spalte löschen\n",
        "df = df.drop('Language',axis=1)\n",
        "df = df.drop('Song', axis=1)\n",
        "df = df.drop('Artist', axis=1)\n",
        "\n",
        "# Den Index resetten\n",
        "df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJe5rcJqVHsr"
      },
      "source": [
        "Stemming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "530_7fz4VHsr"
      },
      "outputs": [],
      "source": [
        "# Neue Spalte erstellen und die Liedtexte ohne Endungen in die Spalte speichen\n",
        "df['stemmed_lyrics'] = \"\"\n",
        "stemmer = PorterStemmer() \n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['cleaned_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for word in words:\n",
        "    tmp = tmp + space + stemmer.stem(word)\n",
        "  df['stemmed_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7SJ5PlDVHsr"
      },
      "source": [
        "Stop-Words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liu4uNZSVHsr"
      },
      "outputs": [],
      "source": [
        "# Neue Spalte erstellen und die Liedtexte ohne Stoppwörter in die Spalte speichen\n",
        "df['wosw_lyrics'] = \"\"\n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['cleaned_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for word in words:\n",
        "    if word not in en_stops:\n",
        "      tmp = tmp + space + word\n",
        "  df['wosw_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxB1Yp0xVHsr"
      },
      "source": [
        "Stemming -> Stop-Words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yl263qPuVHss"
      },
      "outputs": [],
      "source": [
        "# Neue Spalte erstellen und die Liedtexte ohne Endungen und ohne Stoppwörter in die Spalte speichen\n",
        "df['stemmed_wosw_lyrics'] = \"\"\n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['stemmed_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for word in words:\n",
        "    if word not in en_stops:\n",
        "      tmp = tmp + space + word\n",
        "  df['stemmed_wosw_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98giNuxoVHss"
      },
      "source": [
        "StopWords -> Stemming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLEpCAkrVHss"
      },
      "outputs": [],
      "source": [
        "# Neue Spalte erstellen und die Liedtexte ohne Stoppwörter und ohne Endungen in die Spalte speichen\n",
        "df['wosw_stemmed_lyrics'] = \"\"\n",
        "ps = PorterStemmer() \n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['wosw_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for x in words:\n",
        "    tmp = tmp + space + stemmer.stem(x)\n",
        "  df['wosw_stemmed_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing Spacy:"
      ],
      "metadata": {
        "id": "vKRlL6QLbRoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Laden Sie das spacy-Modell und den Englisch-Vokabular\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "#stopwords rausfiltern\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "_zSV5CxvEx1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwsiLnCBYXTa"
      },
      "outputs": [],
      "source": [
        "# Laden Sie die Trainings-Textdokumente und die Trainings-Klassenlabels\n",
        "text_train = df['Lyrics'].tolist() # Dataframe mit Textdokumenten\n",
        "labels_train = df['Genre'] # Dataframe mit Klassenlabels (eins pro Dokument)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Laden Sie die Test-Textdokumente und die Test-Klassenlabels\n",
        "text_test = df_test['Lyrics'].tolist() # Dataframe mit Textdokumenten\n",
        "labels_test = df_test['Genre'] # Dataframe mit Klassenlabels (eins pro Dokument)"
      ],
      "metadata": {
        "id": "w7aGBbfvbwxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainingsdatensatz:"
      ],
      "metadata": {
        "id": "NqmDv8UTfCt-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvJVy24BCKZW"
      },
      "outputs": [],
      "source": [
        "#entfernen von Satzzeichen, Sonderzeichen und überflüssigen Wörtern\n",
        "text_text = []\n",
        "for t in text_train:\n",
        "  t = t.lower()\n",
        "  text_text.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_YA6j3-BfZ8"
      },
      "outputs": [],
      "source": [
        "#entfernen von Satzzeichen, Sonderzeichen und überflüssigen Wörtern\n",
        "cleaned_text = []\n",
        "for t in text_train:\n",
        "  t = t.lower()\n",
        "  t = re.sub(r'[^\\w\\s]','',t)\n",
        "  cleaned_text.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igSBZvU1Bfcu"
      },
      "outputs": [],
      "source": [
        "stemmed_text = []\n",
        "stemmer = PorterStemmer() \n",
        "for t in cleaned_text:\n",
        "  stemmed_text.append(\" \".join([stemmer.stem(word) for word in t.split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hISMKCGFBkYo"
      },
      "outputs": [],
      "source": [
        "stopword_removed_text = []\n",
        "\n",
        "for t in cleaned_text:\n",
        "  t = t.lower()\n",
        "  t = re.sub(r'[^\\w\\s]','',t)\n",
        "  t = \" \".join([word for word in t.split() if word not in stop_words])\n",
        "  stopword_removed_text.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDrHLDAGBmlH"
      },
      "outputs": [],
      "source": [
        "stemmed_and_stopword_removed_text = []\n",
        "\n",
        "for t in stemmed_text:\n",
        "  t = \" \".join([stemmer.stem(word) for word in t.split() if word not in stop_words])\n",
        "  stemmed_and_stopword_removed_text.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-aAdPxwBfm_"
      },
      "outputs": [],
      "source": [
        "stopword_removed_and_stemmed_text = []\n",
        "\n",
        "for t in stopword_removed_text:\n",
        "  t = t.lower()\n",
        "  t = re.sub(r'[^\\w\\s]','',t)\n",
        "  t = \" \".join([word for word in t.split() if word not in stop_words])\n",
        "  t = \" \".join([stemmer.stem(word) for word in t.split()])\n",
        "  stopword_removed_and_stemmed_text.append(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testdatensatz:"
      ],
      "metadata": {
        "id": "EL_gX3iVfE02"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc9MoxesbY28"
      },
      "outputs": [],
      "source": [
        "#stopwords rausfiltern\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfpdxDYbbY28"
      },
      "outputs": [],
      "source": [
        "#entfernen von Satzzeichen, Sonderzeichen und überflüssigen Wörtern\n",
        "text_text_test = []\n",
        "for t in text_test:\n",
        "  t = t.lower()\n",
        "  text_text_test.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIXKRThzbY28"
      },
      "outputs": [],
      "source": [
        "#entfernen von Satzzeichen, Sonderzeichen und überflüssigen Wörtern\n",
        "cleaned_text_test = []\n",
        "for t in text_test:\n",
        "  t = t.lower()\n",
        "  t = re.sub(r'[^\\w\\s]','',t)\n",
        "  cleaned_text_test.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7WmZF_6bY28"
      },
      "outputs": [],
      "source": [
        "stemmed_text_test = []\n",
        "stemmer = PorterStemmer() \n",
        "for t in cleaned_text_test:\n",
        "  stemmed_text_test.append(\" \".join([stemmer.stem(word) for word in t.split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3NUifNtbY28"
      },
      "outputs": [],
      "source": [
        "stopword_removed_text_test = []\n",
        "\n",
        "for t in cleaned_text_test:\n",
        "  t = t.lower()\n",
        "  t = re.sub(r'[^\\w\\s]','',t)\n",
        "  t = \" \".join([word for word in t.split() if word not in stop_words])\n",
        "  stopword_removed_text_test.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPVO-aJObY29"
      },
      "outputs": [],
      "source": [
        "stemmed_and_stopword_removed_text_test = []\n",
        "\n",
        "for t in stemmed_text_test:\n",
        "  t = \" \".join([stemmer.stem(word) for word in t.split() if word not in stop_words])\n",
        "  stemmed_and_stopword_removed_text_test.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UVYVow0bY29"
      },
      "outputs": [],
      "source": [
        "stopword_removed_and_stemmed_text_test = []\n",
        "\n",
        "for t in stopword_removed_text_test:\n",
        "  t = t.lower()\n",
        "  t = re.sub(r'[^\\w\\s]','',t)\n",
        "  t = \" \".join([word for word in t.split() if word not in stop_words])\n",
        "  t = \" \".join([stemmer.stem(word) for word in t.split()])\n",
        "  stopword_removed_and_stemmed_text_test.append(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRpm6PPNA00O"
      },
      "source": [
        "# Einstellen, welche Lyrik-Spalten verwendet werden sollen:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer & TF-IDF Vectorizer:"
      ],
      "metadata": {
        "id": "ijZrLergeqjM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQd0Ei6EBBk6"
      },
      "outputs": [],
      "source": [
        "# Laden Sie die Textdokumente und die Klassenlabels\n",
        "df_train_lyrics = df['Lyrics'] # Dataframe mit Textdokumenten\n",
        "df_train_labels = df['Genre'] # Dataframe mit Klassenlabels (eins pro Dokument)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlMGVo92Qttx"
      },
      "outputs": [],
      "source": [
        "# Laden Sie die Textdokumente und die Klassenlabels\n",
        "df_test_lyrics = df_test['Lyrics'] # Dataframe mit Textdokumenten\n",
        "df_test_labels = df_test['Genre'] # Dataframe mit Klassenlabels (eins pro Dokument)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy-Vectorizer:"
      ],
      "metadata": {
        "id": "g1sFcTjFexow"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNxiqtywBuL8"
      },
      "outputs": [],
      "source": [
        "final_text_train = cleaned_text\n",
        "final_text_test = cleaned_text_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhYo2eIkBcec"
      },
      "source": [
        "# Grid-search SVM:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brKfkJobBced"
      },
      "source": [
        "CountVektorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKwP4Lo2jx5R"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df_train_lyrics)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df_train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [0.011, 0.012, 0.014], \n",
        "                  'gamma': [0.075, 0.079, 0.0795],\n",
        "                  'kernel': ['linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KYBnEosBced"
      },
      "source": [
        "TF-IDF-Vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkIa0_r4Bcee"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df_train_lyrics)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df_train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [4.5], \n",
        "                  'gamma': [0.95,1,3,5,10],\n",
        "                  'kernel': ['rbf']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npjNFl0tBcee"
      },
      "source": [
        "Spacy-Vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQrvhkn6AYTf"
      },
      "outputs": [],
      "source": [
        "vectors = []\n",
        "\n",
        "for t in text_train:\n",
        "  doc = nlp(t)\n",
        "  mean_vec = doc.vector\n",
        "  vectors.append(mean_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L8WkZaPAYTf"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(vectors, labels_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [1, 5, 10], \n",
        "                  'gamma': [1, 0.1, 0.001],\n",
        "                  'kernel': ['linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDfs9MKTB4NT"
      },
      "source": [
        "# Flaml LGBM:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q53vUc5TB4NU"
      },
      "source": [
        "CountVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCNF9IALB4NU"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df_train_lyrics)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df_train_labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1R6DEQB4NU"
      },
      "source": [
        "TfidfVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czTgM0s8B4NU"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df_train_lyrics)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df_train_labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=6000)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTjuXUEWB4NU"
      },
      "source": [
        "Spacy-Vektoren:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuobodsUajoW"
      },
      "outputs": [],
      "source": [
        "# Trainingsdatensatz in Spacy-Vektoren umwandeln\n",
        "vectors = []\n",
        "\n",
        "for t in text_train:\n",
        "  doc = nlp(t)\n",
        "  mean_vec = doc.vector\n",
        "  vectors.append(mean_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPdvx7ktajqz"
      },
      "outputs": [],
      "source": [
        "# vectors in ein Numpy array konvertieren\n",
        "vectors = np.array(vectors)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(vectors, labels_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=4000)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfXodprPmZkx"
      },
      "source": [
        "# MLA:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVj8EqhDmZky"
      },
      "source": [
        "CountVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MCVfGnQmZky"
      },
      "outputs": [],
      "source": [
        "# Vektorizer initialisiereen\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Variablen für MLA deklarieren und teils konvertieren\n",
        "X_train = vectorizer.fit_transform(df_train_lyrics)\n",
        "X_test = vectorizer.transform(df_test_lyrics)\n",
        "\n",
        "y_train = df_train_labels\n",
        "y_test = df_test_labels\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxuqlPugmZkz"
      },
      "outputs": [],
      "source": [
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier(n_estimators=200, num_leaves= 184, min_child_samples=3, \n",
        "                          learning_rate=0.08838576141784195, log_max_bin=9, \n",
        "                          colsample_bytree=0.39039795455449644, \n",
        "                          reg_alpha=0.014844095616079196, \n",
        "                          reg_lambda=0.05525097930389173)\n",
        "svm = SVC(C=0.01, gamma=0.07, kernel='linear')\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersage der Testdaten svm\n",
        "predictions_lgbm = lgbm.predict(X_test)\n",
        "predictions_svm = svm.predict(X_test)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = accuracy_score(y_test, predictions_lgbm)\n",
        "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
        "\n",
        "# Den Klassifikations-Report ausgeben\n",
        "print('LGBM:')\n",
        "print(classification_report(y_test, predictions_lgbm))\n",
        "print('SVM:')\n",
        "print(classification_report(y_test, predictions_svm))\n",
        "\n",
        "# Die Genauigkeit ausgeben\n",
        "print('Accuracy LGBM CouVec:', accuracy_lgbm)\n",
        "print('Accuracy SVM CouVec:', accuracy_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37dSSbhnmZk0"
      },
      "source": [
        "TF-IDF Vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWp6e2wkmZk0"
      },
      "outputs": [],
      "source": [
        "# Vektorizer initialisiereen\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Variablen für MLA deklarieren und teils konvertieren\n",
        "X_train = vectorizer.fit_transform(df_train_lyrics)\n",
        "X_test = vectorizer.transform(df_test_lyrics)\n",
        "\n",
        "y_train = df_train_labels\n",
        "y_test = df_test_labels\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBh8eQiemZk0"
      },
      "outputs": [],
      "source": [
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier(n_estimators=67, num_leaves= 184, min_child_samples=12, \n",
        "                          learning_rate=0.085537978248575, log_max_bin=9, \n",
        "                          colsample_bytree=0.7663773657187746, \n",
        "                          reg_alpha=0.006958608037974516, \n",
        "                          reg_lambda=0.4683303882185501)\n",
        "svm = SVC(C=4.5, gamma=0.95, kernel='rbf')\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersage der Testdaten svm\n",
        "predictions_lgbm = lgbm.predict(X_test)\n",
        "predictions_svm = svm.predict(X_test)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = accuracy_score(y_test, predictions_lgbm)\n",
        "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
        "\n",
        "# Den Klassifikations-Report ausgeben\n",
        "print('LGBM:')\n",
        "print(classification_report(y_test, predictions_lgbm))\n",
        "print('SVM:')\n",
        "print(classification_report(y_test, predictions_svm))\n",
        "\n",
        "# Die Genauigkeit ausgeben\n",
        "print('Accuracy LGBM CouVec:', accuracy_lgbm)\n",
        "print('Accuracy SVM CouVec:', accuracy_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy-Vectorizer:"
      ],
      "metadata": {
        "id": "rH2jlJfeeIu1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scDWd5kduWrn"
      },
      "outputs": [],
      "source": [
        "# Trainingsdatensatz in Spacy-Vektoren umwandeln\n",
        "vectors_train = []\n",
        "\n",
        "for t in final_text_train:\n",
        "  doc = nlp(t)\n",
        "  mean_vec = doc.vector\n",
        "  vectors_train.append(mean_vec)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testdatensatz in Spacy-Vektoren umwandeln\n",
        "vectors_test = []\n",
        "\n",
        "for t in final_text_test:\n",
        "  doc = nlp(t)\n",
        "  mean_vec = doc.vector\n",
        "  vectors_test.append(mean_vec)"
      ],
      "metadata": {
        "id": "_qPEnkTtdUxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKNxm2A3udrY"
      },
      "outputs": [],
      "source": [
        "# Variablen für MLA deklarieren\n",
        "X_train = vectors_train\n",
        "X_test = vectors_test\n",
        "\n",
        "y_train = labels_train\n",
        "y_test = labels_test\n",
        "\n",
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "svm = SVC(C=0.9, gamma=1.2, kernel='linear')\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = lgbm.score(X_test, y_test)\n",
        "accuracy_svm = svm.score(X_test, y_test)\n",
        "\n",
        "# Genauigkeit printen\n",
        "print(\"Accuracy LGBM: \", accuracy_lgbm)\n",
        "print(\"Accuracy SVM: \", accuracy_svm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fyb6mPpCJADk",
        "D48vtzWGJAxi",
        "rRr671g-UD2w",
        "_hbI7RZuVHsq",
        "vKRlL6QLbRoe",
        "cRpm6PPNA00O",
        "hhYo2eIkBcec",
        "QDfs9MKTB4NT",
        "qfXodprPmZkx"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}