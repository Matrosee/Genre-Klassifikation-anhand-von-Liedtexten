{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matrosee/Genre-Klassifikation-anhand-von-Liedtexten/blob/main/Code_von_Jannic_M_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyb6mPpCJADk"
      },
      "source": [
        "# Installationen, Imports und Downloads:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdh2wodVNzZE"
      },
      "source": [
        "Installationen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKornHEzn-PV",
        "outputId": "2c62e08e-da5c-4487-dd01-6debc9590f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.8/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from lightgbm) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from lightgbm) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from lightgbm) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->lightgbm) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->lightgbm) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-23 12:21:14.228831: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-md==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.10.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flaml\n",
            "  Downloading FLAML-1.1.1-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.1/216.1 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xgboost>=0.90 in /usr/local/lib/python3.8/dist-packages (from flaml) (0.90)\n",
            "Collecting lightgbm>=2.3.1\n",
            "  Downloading lightgbm-3.3.4-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.8/dist-packages (from flaml) (1.0.2)\n",
            "Requirement already satisfied: NumPy>=1.17.0rc1 in /usr/local/lib/python3.8/dist-packages (from flaml) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from flaml) (1.7.3)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.8/dist-packages (from flaml) (1.3.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from lightgbm>=2.3.1->flaml) (0.38.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.4->flaml) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.4->flaml) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24->flaml) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24->flaml) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.4->flaml) (1.15.0)\n",
            "Installing collected packages: lightgbm, flaml\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "Successfully installed flaml-1.1.1 lightgbm-3.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install lightgbm\n",
        "!pip install spacy\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install flaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPehy4oEN208"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0HqVeIzJJrR",
        "outputId": "e801e1a8-bca2-4e55-c9a8-3d52737a1897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "import flaml\n",
        "import lightgbm as lgb\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from flaml import AutoML\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from lightgbm import LGBMClassifier\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7cRPYzKN4tt"
      },
      "source": [
        "Downloads:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACQnvRGSR2uc",
        "outputId": "13a1b928-32f7-4d0d-b898-f5fe84a755e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D48vtzWGJAxi"
      },
      "source": [
        "# Upload & Dataframe Deklaration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlVoFF0ynk6D",
        "outputId": "db355530-8e78-4a82-ddf4-7d3e6a42f487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zqKSYp3Be9l"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCMkjgqaEDOy"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('/content/gdrive/MyDrive/test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRr671g-UD2w"
      },
      "source": [
        "# 1890 Songs-pro-Datensatz erstellen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtUibkdaUD2w"
      },
      "outputs": [],
      "source": [
        "# Nur Englische-Lieder einstellen\n",
        "df = df.loc[df['Language']=='en']\n",
        "\n",
        "# Pro Genre ein eigenes Dataframe mit je 1890 Liedern\n",
        "df_rock_one = df[df['Genre']=='Rock']\n",
        "df_rock_one = df_rock_one.head(1890)\n",
        "\n",
        "df_pop_one = df[df['Genre']=='Pop']\n",
        "df_pop_one = df_pop_one.head(1890)\n",
        "\n",
        "df_metal_one = df[df['Genre']=='Metal']\n",
        "df_metal_one = df_metal_one.head(1890)\n",
        "\n",
        "df_hip_hop_one = df[df['Genre']=='Hip-Hop']\n",
        "df_hip_hop_one = df_hip_hop_one.head(1890)\n",
        "\n",
        "df_rnb_one = df[df['Genre']=='R&B']\n",
        "df_rnb_one = df_rnb_one.head(1890)\n",
        "\n",
        "df_indie_one = df[df['Genre']=='Indie']\n",
        "df_indie_one = df_indie_one.head(1890)\n",
        "\n",
        "df_electronic_one = df[df['Genre']=='Electronic']\n",
        "df_electronic_one = df_electronic_one.head(1890)\n",
        "\n",
        "df_jazz_one = df[df['Genre']=='Jazz']\n",
        "df_jazz_one = df_jazz_one.head(1890)\n",
        "\n",
        "df_folk_one = df[df['Genre']=='Folk']\n",
        "df_folk_one = df_folk_one.head(1890)\n",
        "\n",
        "df_country_one = df[df['Genre']=='Country']\n",
        "df_country_one = df_country_one.head(1890)\n",
        "\n",
        "# Alle einzelnen Dataframes zu einem zusammfügen\n",
        "df_one = pd.concat([df_pop_one, df_hip_hop_one, df_metal_one, df_rock_one, \n",
        "                    df_indie_one, df_country_one, df_electronic_one, df_rnb_one,\n",
        "                    df_jazz_one, df_folk_one])\n",
        "\n",
        "# Kürzeren Namen für zuküftige Arbeit auswählen\n",
        "df = df_one\n",
        "\n",
        "# Spalten ohne Beiträge löschen \n",
        "df.dropna()\n",
        "\n",
        "# Index reseten\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hbI7RZuVHsq"
      },
      "source": [
        "# Pre-Processing CountVectorizer & TF-IDF Vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEN8vKbOrymD"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnv0C9WqBwFd"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(subset=['Lyrics'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krrAcvz7VHsq"
      },
      "outputs": [],
      "source": [
        "# Laden Sie das spacy-Modell und den Englisch-Vokabular\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Stop-Wörter auf einer Variable deklarieren\n",
        "en_stops = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGWeBqPjVHsr"
      },
      "source": [
        "Datensatz säubern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8R7uEVeVHsr"
      },
      "outputs": [],
      "source": [
        "# Sonderzeichen entfernen\n",
        "df['Lyrics'] = [re.sub(r'^.*?Lyrics', '', str(lyric)) for lyric in df['Lyrics']]\n",
        "\n",
        "# Satzumbruch Zeichen entfernen\n",
        "df['cleaned_lyrics'] = [str(lyric).replace('\\n',' ') for lyric in df['Lyrics']]\n",
        "\n",
        "# Wörter in eckigen Klammern entfernen\n",
        "df['cleaned_lyrics'] = [re.sub(\"\\[.*?\\]\",\"\",lyric) for lyric in df['cleaned_lyrics']]\n",
        "\n",
        "# Language-, Song-Name- und Artist-Spalte löschen\n",
        "df = df.drop('Language',axis=1)\n",
        "df = df.drop('Song', axis=1)\n",
        "df = df.drop('Artist', axis=1)\n",
        "\n",
        "# Den Index resetten\n",
        "df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJe5rcJqVHsr"
      },
      "source": [
        "Stemming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "530_7fz4VHsr"
      },
      "outputs": [],
      "source": [
        "# Neue Spalte erstellen und die Liedtexte ohne Endungen in die Spalte speichen\n",
        "df['stemmed_lyrics'] = \"\"\n",
        "stemmer = PorterStemmer() \n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['cleaned_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for word in words:\n",
        "    tmp = tmp + space + stemmer.stem(word)\n",
        "  df['stemmed_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7SJ5PlDVHsr"
      },
      "source": [
        "Stop-Words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liu4uNZSVHsr"
      },
      "outputs": [],
      "source": [
        "# Neue Spalte erstellen und die Liedtexte ohne Stoppwörter in die Spalte speichen\n",
        "df['wosw_lyrics'] = \"\"\n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['cleaned_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for word in words:\n",
        "    if word not in en_stops:\n",
        "      tmp = tmp + space + word\n",
        "  df['wosw_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxB1Yp0xVHsr"
      },
      "source": [
        "Stemming -> Stop-Words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yl263qPuVHss"
      },
      "outputs": [],
      "source": [
        "# Neue Spalte erstellen und die Liedtexte ohne Endungen und ohne Stoppwörter in die Spalte speichen\n",
        "df['stemmed_wosw_lyrics'] = \"\"\n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['stemmed_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for word in words:\n",
        "    if word not in en_stops:\n",
        "      tmp = tmp + space + word\n",
        "  df['stemmed_wosw_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98giNuxoVHss"
      },
      "source": [
        "StopWords -> Stemming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLEpCAkrVHss"
      },
      "outputs": [],
      "source": [
        "# Neue Spalte erstellen und die Liedtexte ohne Stoppwörter und ohne Endungen in die Spalte speichen\n",
        "df['wosw_stemmed_lyrics'] = \"\"\n",
        "ps = PorterStemmer() \n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['wosw_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for x in words:\n",
        "    tmp = tmp + space + stemmer.stem(x)\n",
        "  df['wosw_stemmed_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing Spacy:"
      ],
      "metadata": {
        "id": "vKRlL6QLbRoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Laden Sie das spacy-Modell und den Englisch-Vokabular\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "#stopwords rausfiltern\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "_zSV5CxvEx1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwsiLnCBYXTa"
      },
      "outputs": [],
      "source": [
        "# Laden Sie die Trainings-Textdokumente und die Trainings-Klassenlabels\n",
        "text_train = df['Lyrics'].tolist() # Dataframe mit Textdokumenten\n",
        "labels_train = df['Genre'] # Dataframe mit Klassenlabels (eins pro Dokument)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Laden Sie die Test-Textdokumente und die Test-Klassenlabels\n",
        "text_test = df_test['Lyrics'].tolist() # Dataframe mit Textdokumenten\n",
        "labels_test = df_test['Genre'] # Dataframe mit Klassenlabels (eins pro Dokument)"
      ],
      "metadata": {
        "id": "w7aGBbfvbwxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainingsdatensatz:"
      ],
      "metadata": {
        "id": "NqmDv8UTfCt-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvJVy24BCKZW"
      },
      "outputs": [],
      "source": [
        "#entfernen von Satzzeichen, Sonderzeichen und überflüssigen Wörtern\n",
        "text_text = []\n",
        "for t in text_train:\n",
        "  t = t.lower()\n",
        "  text_text.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_YA6j3-BfZ8"
      },
      "outputs": [],
      "source": [
        "#entfernen von Satzzeichen, Sonderzeichen und überflüssigen Wörtern\n",
        "cleaned_text = []\n",
        "for t in text_train:\n",
        "  t = t.lower()\n",
        "  t = re.sub(r'[^\\w\\s]','',t)\n",
        "  cleaned_text.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igSBZvU1Bfcu"
      },
      "outputs": [],
      "source": [
        "stemmed_text = []\n",
        "stemmer = PorterStemmer() \n",
        "for t in cleaned_text:\n",
        "  stemmed_text.append(\" \".join([stemmer.stem(word) for word in t.split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hISMKCGFBkYo"
      },
      "outputs": [],
      "source": [
        "stopword_removed_text = []\n",
        "\n",
        "for t in cleaned_text:\n",
        "  t = t.lower()\n",
        "  t = re.sub(r'[^\\w\\s]','',t)\n",
        "  t = \" \".join([word for word in t.split() if word not in stop_words])\n",
        "  stopword_removed_text.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDrHLDAGBmlH"
      },
      "outputs": [],
      "source": [
        "stemmed_and_stopword_removed_text = []\n",
        "\n",
        "for t in stemmed_text:\n",
        "  t = \" \".join([stemmer.stem(word) for word in t.split() if word not in stop_words])\n",
        "  stemmed_and_stopword_removed_text.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-aAdPxwBfm_"
      },
      "outputs": [],
      "source": [
        "stopword_removed_and_stemmed_text = []\n",
        "\n",
        "for t in stopword_removed_text:\n",
        "  t = t.lower()\n",
        "  t = re.sub(r'[^\\w\\s]','',t)\n",
        "  t = \" \".join([word for word in t.split() if word not in stop_words])\n",
        "  t = \" \".join([stemmer.stem(word) for word in t.split()])\n",
        "  stopword_removed_and_stemmed_text.append(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testdatensatz:"
      ],
      "metadata": {
        "id": "EL_gX3iVfE02"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc9MoxesbY28"
      },
      "outputs": [],
      "source": [
        "#stopwords rausfiltern\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfpdxDYbbY28"
      },
      "outputs": [],
      "source": [
        "#entfernen von Satzzeichen, Sonderzeichen und überflüssigen Wörtern\n",
        "text_text_test = []\n",
        "for t in text_test:\n",
        "  t = t.lower()\n",
        "  text_text_test.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIXKRThzbY28"
      },
      "outputs": [],
      "source": [
        "#entfernen von Satzzeichen, Sonderzeichen und überflüssigen Wörtern\n",
        "cleaned_text_test = []\n",
        "for t in text_test:\n",
        "  t = t.lower()\n",
        "  t = re.sub(r'[^\\w\\s]','',t)\n",
        "  cleaned_text_test.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7WmZF_6bY28"
      },
      "outputs": [],
      "source": [
        "stemmed_text_test = []\n",
        "stemmer = PorterStemmer() \n",
        "for t in cleaned_text_test:\n",
        "  stemmed_text_test.append(\" \".join([stemmer.stem(word) for word in t.split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3NUifNtbY28"
      },
      "outputs": [],
      "source": [
        "stopword_removed_text_test = []\n",
        "\n",
        "for t in cleaned_text_test:\n",
        "  t = t.lower()\n",
        "  t = re.sub(r'[^\\w\\s]','',t)\n",
        "  t = \" \".join([word for word in t.split() if word not in stop_words])\n",
        "  stopword_removed_text_test.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPVO-aJObY29"
      },
      "outputs": [],
      "source": [
        "stemmed_and_stopword_removed_text_test = []\n",
        "\n",
        "for t in stemmed_text_test:\n",
        "  t = \" \".join([stemmer.stem(word) for word in t.split() if word not in stop_words])\n",
        "  stemmed_and_stopword_removed_text_test.append(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UVYVow0bY29"
      },
      "outputs": [],
      "source": [
        "stopword_removed_and_stemmed_text_test = []\n",
        "\n",
        "for t in stopword_removed_text_test:\n",
        "  t = t.lower()\n",
        "  t = re.sub(r'[^\\w\\s]','',t)\n",
        "  t = \" \".join([word for word in t.split() if word not in stop_words])\n",
        "  t = \" \".join([stemmer.stem(word) for word in t.split()])\n",
        "  stopword_removed_and_stemmed_text_test.append(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRpm6PPNA00O"
      },
      "source": [
        "# Einstellen, welche Lyrik-Spalten verwendet werden sollen:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer & TF-IDF Vectorizer:"
      ],
      "metadata": {
        "id": "ijZrLergeqjM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQd0Ei6EBBk6"
      },
      "outputs": [],
      "source": [
        "# Laden Sie die Textdokumente und die Klassenlabels\n",
        "df_train_lyrics = df['Lyrics'] # Dataframe mit Textdokumenten\n",
        "df_train_labels = df['Genre'] # Dataframe mit Klassenlabels (eins pro Dokument)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlMGVo92Qttx"
      },
      "outputs": [],
      "source": [
        "# Laden Sie die Textdokumente und die Klassenlabels\n",
        "df_test_lyrics = df_test['Lyrics'] # Dataframe mit Textdokumenten\n",
        "df_test_labels = df_test['Genre'] # Dataframe mit Klassenlabels (eins pro Dokument)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy-Vectorizer:"
      ],
      "metadata": {
        "id": "g1sFcTjFexow"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNxiqtywBuL8"
      },
      "outputs": [],
      "source": [
        "final_text_train = cleaned_text\n",
        "final_text_test = cleaned_text_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhYo2eIkBcec"
      },
      "source": [
        "# Grid-search SVM:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brKfkJobBced"
      },
      "source": [
        "CountVektorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKwP4Lo2jx5R"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df_train_lyrics)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df_train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [0.011, 0.012, 0.014], \n",
        "                  'gamma': [0.075, 0.079, 0.0795],\n",
        "                  'kernel': ['linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KYBnEosBced"
      },
      "source": [
        "TF-IDF-Vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkIa0_r4Bcee"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df_train_lyrics)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df_train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [4.5], \n",
        "                  'gamma': [0.95,1,3,5,10],\n",
        "                  'kernel': ['rbf']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npjNFl0tBcee"
      },
      "source": [
        "Spacy-Vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQrvhkn6AYTf"
      },
      "outputs": [],
      "source": [
        "vectors = []\n",
        "\n",
        "for t in text_train:\n",
        "  doc = nlp(t)\n",
        "  mean_vec = doc.vector\n",
        "  vectors.append(mean_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L8WkZaPAYTf"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(vectors, labels_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [1, 5, 10], \n",
        "                  'gamma': [1, 0.1, 0.001],\n",
        "                  'kernel': ['linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDfs9MKTB4NT"
      },
      "source": [
        "# Flaml LGBM:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q53vUc5TB4NU"
      },
      "source": [
        "CountVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCNF9IALB4NU"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df_train_lyrics)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df_train_labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1R6DEQB4NU"
      },
      "source": [
        "TfidfVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czTgM0s8B4NU"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df_train_lyrics)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df_train_labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=6000)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTjuXUEWB4NU"
      },
      "source": [
        "Spacy-Vektoren:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuobodsUajoW"
      },
      "outputs": [],
      "source": [
        "# Trainingsdatensatz in Spacy-Vektoren umwandeln\n",
        "vectors = []\n",
        "\n",
        "for t in text_train:\n",
        "  doc = nlp(t)\n",
        "  mean_vec = doc.vector\n",
        "  vectors.append(mean_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPdvx7ktajqz"
      },
      "outputs": [],
      "source": [
        "# vectors in ein Numpy array konvertieren\n",
        "vectors = np.array(vectors)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(vectors, labels_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=4000)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfXodprPmZkx"
      },
      "source": [
        "# MLA:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVj8EqhDmZky"
      },
      "source": [
        "CountVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MCVfGnQmZky"
      },
      "outputs": [],
      "source": [
        "# Vektorizer initialisiereen\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Variablen für MLA deklarieren und teils konvertieren\n",
        "X_train = vectorizer.fit_transform(df_train_lyrics)\n",
        "X_test = vectorizer.transform(df_test_lyrics)\n",
        "\n",
        "y_train = df_train_labels\n",
        "y_test = df_test_labels\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxuqlPugmZkz"
      },
      "outputs": [],
      "source": [
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier(n_estimators=200, num_leaves= 184, min_child_samples=3, \n",
        "                          learning_rate=0.08838576141784195, log_max_bin=9, \n",
        "                          colsample_bytree=0.39039795455449644, \n",
        "                          reg_alpha=0.014844095616079196, \n",
        "                          reg_lambda=0.05525097930389173)\n",
        "svm = SVC(C=0.01, gamma=0.07, kernel='linear')\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersage der Testdaten svm\n",
        "predictions_lgbm = lgbm.predict(X_test)\n",
        "predictions_svm = svm.predict(X_test)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = accuracy_score(y_test, predictions_lgbm)\n",
        "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
        "\n",
        "# Den Klassifikations-Report ausgeben\n",
        "print('LGBM:')\n",
        "print(classification_report(y_test, predictions_lgbm))\n",
        "print('SVM:')\n",
        "print(classification_report(y_test, predictions_svm))\n",
        "\n",
        "# Die Genauigkeit ausgeben\n",
        "print('Accuracy LGBM CouVec:', accuracy_lgbm)\n",
        "print('Accuracy SVM CouVec:', accuracy_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37dSSbhnmZk0"
      },
      "source": [
        "TF-IDF Vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWp6e2wkmZk0"
      },
      "outputs": [],
      "source": [
        "# Vektorizer initialisiereen\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Variablen für MLA deklarieren und teils konvertieren\n",
        "X_train = vectorizer.fit_transform(df_train_lyrics)\n",
        "X_test = vectorizer.transform(df_test_lyrics)\n",
        "\n",
        "y_train = df_train_labels\n",
        "y_test = df_test_labels\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBh8eQiemZk0"
      },
      "outputs": [],
      "source": [
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier(n_estimators=67, num_leaves= 184, min_child_samples=12, \n",
        "                          learning_rate=0.085537978248575, log_max_bin=9, \n",
        "                          colsample_bytree=0.7663773657187746, \n",
        "                          reg_alpha=0.006958608037974516, \n",
        "                          reg_lambda=0.4683303882185501)\n",
        "svm = SVC(C=4.5, gamma=0.95, kernel='rbf')\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersage der Testdaten svm\n",
        "predictions_lgbm = lgbm.predict(X_test)\n",
        "predictions_svm = svm.predict(X_test)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = accuracy_score(y_test, predictions_lgbm)\n",
        "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
        "\n",
        "# Den Klassifikations-Report ausgeben\n",
        "print('LGBM:')\n",
        "print(classification_report(y_test, predictions_lgbm))\n",
        "print('SVM:')\n",
        "print(classification_report(y_test, predictions_svm))\n",
        "\n",
        "# Die Genauigkeit ausgeben\n",
        "print('Accuracy LGBM CouVec:', accuracy_lgbm)\n",
        "print('Accuracy SVM CouVec:', accuracy_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy-Vectorizer:"
      ],
      "metadata": {
        "id": "rH2jlJfeeIu1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scDWd5kduWrn"
      },
      "outputs": [],
      "source": [
        "# Trainingsdatensatz in Spacy-Vektoren umwandeln\n",
        "vectors_train = []\n",
        "\n",
        "for t in final_text_train:\n",
        "  doc = nlp(t)\n",
        "  mean_vec = doc.vector\n",
        "  vectors_train.append(mean_vec)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testdatensatz in Spacy-Vektoren umwandeln\n",
        "vectors_test = []\n",
        "\n",
        "for t in final_text_test:\n",
        "  doc = nlp(t)\n",
        "  mean_vec = doc.vector\n",
        "  vectors_test.append(mean_vec)"
      ],
      "metadata": {
        "id": "_qPEnkTtdUxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKNxm2A3udrY"
      },
      "outputs": [],
      "source": [
        "# Variablen für MLA deklarieren\n",
        "X_train = vectors_train\n",
        "X_test = vectors_test\n",
        "\n",
        "y_train = labels_train\n",
        "y_test = labels_test\n",
        "\n",
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "svm = SVC(C=0.9, gamma=1.2, kernel='linear')\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = lgbm.score(X_test, y_test)\n",
        "accuracy_svm = svm.score(X_test, y_test)\n",
        "\n",
        "# Genauigkeit printen\n",
        "print(\"Accuracy LGBM: \", accuracy_lgbm)\n",
        "print(\"Accuracy SVM: \", accuracy_svm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fyb6mPpCJADk",
        "D48vtzWGJAxi",
        "rRr671g-UD2w",
        "_hbI7RZuVHsq",
        "vKRlL6QLbRoe",
        "cRpm6PPNA00O",
        "hhYo2eIkBcec",
        "QDfs9MKTB4NT",
        "qfXodprPmZkx"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}