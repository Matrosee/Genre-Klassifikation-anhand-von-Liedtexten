{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matrosee/Genre-Klassifikation-anhand-von-Liedtexten/blob/main/Code_von_Jannic_M_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyb6mPpCJADk"
      },
      "source": [
        "# Installationen, Imports und Downloads:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installationen:"
      ],
      "metadata": {
        "id": "gdh2wodVNzZE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKornHEzn-PV"
      },
      "outputs": [],
      "source": [
        "!pip install lightgbm\n",
        "!pip install spacy\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install flaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports:"
      ],
      "metadata": {
        "id": "tPehy4oEN208"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "N0HqVeIzJJrR"
      },
      "outputs": [],
      "source": [
        "import flaml\n",
        "import lightgbm as lgb\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from flaml import AutoML\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from lightgbm import LGBMClassifier\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloads:"
      ],
      "metadata": {
        "id": "I7cRPYzKN4tt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACQnvRGSR2uc"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D48vtzWGJAxi"
      },
      "source": [
        "# Upload & Dataframe Deklaration:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "mlVoFF0ynk6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/train.csv')"
      ],
      "metadata": {
        "id": "6zqKSYp3Be9l"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/gdrive/MyDrive/test.csv')"
      ],
      "metadata": {
        "id": "dCMkjgqaEDOy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRr671g-UD2w"
      },
      "source": [
        "# 1890 Songs-pro-Datensatz erstellen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wtUibkdaUD2w"
      },
      "outputs": [],
      "source": [
        "# Nur Englische-Lieder einstellen\n",
        "df = df.loc[df['Language']=='en']\n",
        "\n",
        "# Pro Genre ein eigenes Dataframe mit je 1890 Liedern\n",
        "df_rock_one = df[df['Genre']=='Rock']\n",
        "df_rock_one = df_rock_one.head(1890)\n",
        "\n",
        "df_pop_one = df[df['Genre']=='Pop']\n",
        "df_pop_one = df_pop_one.head(1890)\n",
        "\n",
        "df_metal_one = df[df['Genre']=='Metal']\n",
        "df_metal_one = df_metal_one.head(1890)\n",
        "\n",
        "df_hip_hop_one = df[df['Genre']=='Hip-Hop']\n",
        "df_hip_hop_one = df_hip_hop_one.head(1890)\n",
        "\n",
        "df_rnb_one = df[df['Genre']=='R&B']\n",
        "df_rnb_one = df_rnb_one.head(1890)\n",
        "\n",
        "df_indie_one = df[df['Genre']=='Indie']\n",
        "df_indie_one = df_indie_one.head(1890)\n",
        "\n",
        "df_electronic_one = df[df['Genre']=='Electronic']\n",
        "df_electronic_one = df_electronic_one.head(1890)\n",
        "\n",
        "df_jazz_one = df[df['Genre']=='Jazz']\n",
        "df_jazz_one = df_jazz_one.head(1890)\n",
        "\n",
        "df_folk_one = df[df['Genre']=='Folk']\n",
        "df_folk_one = df_folk_one.head(1890)\n",
        "\n",
        "df_country_one = df[df['Genre']=='Country']\n",
        "df_country_one = df_country_one.head(1890)\n",
        "\n",
        "# Alle einzelnen Dataframes zu einem zusammfügen\n",
        "df_one = pd.concat([df_pop_one, df_hip_hop_one, df_metal_one, df_rock_one, \n",
        "                    df_indie_one, df_country_one, df_electronic_one, df_rnb_one,\n",
        "                    df_jazz_one, df_folk_one])\n",
        "\n",
        "# Kürzeren Namen für zuküftige Arbeit auswählen\n",
        "df = df_one\n",
        "\n",
        "# Spalten ohne Beiträge löschen \n",
        "df.dropna()\n",
        "\n",
        "# Index reseten\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hbI7RZuVHsq"
      },
      "source": [
        "# Pre-Processing:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(axis=1)"
      ],
      "metadata": {
        "id": "ZEN8vKbOrymD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "krrAcvz7VHsq"
      },
      "outputs": [],
      "source": [
        "# Laden Sie das spacy-Modell und den Englisch-Vokabular\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Stop-Wörter auf einer Variable deklarieren\n",
        "en_stops = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGWeBqPjVHsr"
      },
      "source": [
        "Datensatz säubern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8R7uEVeVHsr"
      },
      "outputs": [],
      "source": [
        "# Sonderzeichen entfernen\n",
        "df['Lyrics'] = [re.sub(r'^.*?Lyrics', '', str(lyric)) for lyric in df['Lyrics']]\n",
        "\n",
        "# Satzumbruch Zeichen entfernen\n",
        "df['cleaned_lyrics'] = [str(lyric).replace('\\n',' ') for lyric in df['Lyrics']]\n",
        "\n",
        "# Wörter in eckigen Klammern entfernen\n",
        "df['cleaned_lyrics'] = [re.sub(\"\\[.*?\\]\",\"\",lyric) for lyric in df['cleaned_lyrics']]\n",
        "\n",
        "# Language-, Song-Name- und Artist-Spalte löschen\n",
        "df = df.drop('Language',axis=1)\n",
        "df = df.drop('Song', axis=1)\n",
        "df = df.drop('Artist', axis=1)\n",
        "\n",
        "# Den Index resetten\n",
        "df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJe5rcJqVHsr"
      },
      "source": [
        "Stemming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "530_7fz4VHsr"
      },
      "outputs": [],
      "source": [
        "# Neue Spalte erstellen und die Liedtexte ohne Endungen in die Spalte speichen\n",
        "df['stemmed_lyrics'] = \"\"\n",
        "ps = PorterStemmer() \n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['cleaned_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for word in words:\n",
        "    tmp = tmp + space + ps.stem(word)\n",
        "  df['stemmed_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7SJ5PlDVHsr"
      },
      "source": [
        "Stop-Words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "liu4uNZSVHsr"
      },
      "outputs": [],
      "source": [
        "# Neue Spalte erstellen und die Liedtexte ohne Stoppwörter in die Spalte speichen\n",
        "df['wosw_lyrics'] = \"\"\n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['cleaned_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for word in words:\n",
        "    if word not in en_stops:\n",
        "      tmp = tmp + space + word\n",
        "  df['wosw_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxB1Yp0xVHsr"
      },
      "source": [
        "Stemming -> Stop-Words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yl263qPuVHss"
      },
      "outputs": [],
      "source": [
        "# Neue Spalte erstellen und die Liedtexte ohne Endungen und ohne Stoppwörter in die Spalte speichen\n",
        "df['stemmed_wosw_lyrics'] = \"\"\n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['stemmed_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for word in words:\n",
        "    if word not in en_stops:\n",
        "      tmp = tmp + space + word\n",
        "  df['stemmed_wosw_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98giNuxoVHss"
      },
      "source": [
        "StopWords -> Stemming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TLEpCAkrVHss"
      },
      "outputs": [],
      "source": [
        "# Neue Spalte erstellen und die Liedtexte ohne Stoppwörter und ohne Endungen in die Spalte speichen\n",
        "df['wosw_stemmed_lyrics'] = \"\"\n",
        "ps = PorterStemmer() \n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['wosw_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for x in words:\n",
        "    tmp = tmp + space + ps.stem(x)\n",
        "  df['wosw_stemmed_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRpm6PPNA00O"
      },
      "source": [
        "# Einstellen, welche Lyrik-Spalten verwendet werden sollen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "zQd0Ei6EBBk6"
      },
      "outputs": [],
      "source": [
        "# Laden Sie die Textdokumente und die Klassenlabels\n",
        "df_train_lyrics = df['Lyrics'].tolist() # Dataframe mit Textdokumenten\n",
        "df_train_labels = df['Genre'] # Dataframe mit Klassenlabels (eins pro Dokument)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Laden Sie die Textdokumente und die Klassenlabels\n",
        "df_test_lyrics = df_test['Lyrics'] # Dataframe mit Textdokumenten\n",
        "df_test_labels = df_test['Genre'] # Dataframe mit Klassenlabels (eins pro Dokument)"
      ],
      "metadata": {
        "id": "VlMGVo92Qttx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhYo2eIkBcec"
      },
      "source": [
        "# Grid-search SVM mit SKLEARN:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1trYSBced"
      },
      "source": [
        "Wie ich die optimalen Paramter rausfinde?\n",
        "Ich gebe grobe Werte für die Paramter an und werde immer genauer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brKfkJobBced"
      },
      "source": [
        "CountVektorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKwP4Lo2jx5R"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df_train_lyrics)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df_train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [0.011, 0.012, 0.014], \n",
        "                  'gamma': [0.075, 0.079, 0.0795],\n",
        "                  'kernel': ['linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KYBnEosBced"
      },
      "source": [
        "SVM mit TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkIa0_r4Bcee"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df_train_lyrics)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df_train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [4.5], \n",
        "                  'gamma': [0.95,1,3,5,10],\n",
        "                  'kernel': ['rbf']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npjNFl0tBcee"
      },
      "source": [
        "Spacy:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Funktion definieren, um die Liedtexte in Vektoren zu konvertieren\n",
        "# def document_vector(text):\n",
        "#     doc = nlp(text)\n",
        "#     vectors = []\n",
        "#     for token in doc:\n",
        "#         if token.has_vector:\n",
        "#             vectors.append(token.vector)\n",
        "#     if len(vectors) > 0:\n",
        "#         return np.mean(vectors, axis=0)\n",
        "#     else:\n",
        "#         raise ValueError(\"Keine Vektoren gefunden\")\n",
        "\n",
        "# # Herausfinden, in welchen Spalten keine Vektoren vorhanden sind\n",
        "# no_vector_docs = []\n",
        "# for i, text in enumerate(df_train_lyrics):\n",
        "#     try:\n",
        "#         vec = document_vector(text)\n",
        "#     except ValueError as e:\n",
        "#         print(f\"Fehler: {e} bei Dokument {i}\")\n",
        "#         no_vector_docs.append(i)\n",
        "# print(f\"Dokumente ohne Vektoren: {no_vector_docs}\")\n",
        "\n",
        "# # Alle Zeilen ohne Vektoren löschen\n",
        "# for i in no_vector_docs:\n",
        "#   df_train_labels = df_train_labels.drop(i)\n",
        "#   del df_train_lyrics[i]\n",
        "\n",
        "# # Das Dataframe für die Labels reindexen\n",
        "# df_train_labels = df_train_labels.reset_index(drop=True)  \n",
        "\n",
        "# # Compute the document vectors\n",
        "# documents_with_vectors = []\n",
        "# labels_with_vectors = []\n",
        "# for i in range(len(df_train_lyrics)):\n",
        "#     text = df_train_lyricss[i]\n",
        "#     df_train_labels = df_train_labels[i]\n",
        "#     try:\n",
        "#         vec = document_vector(text)\n",
        "#         documents_with_vectors.append(text)\n",
        "#         labels_with_vectors.append(label)\n",
        "#     except ValueError as e:\n",
        "#         print(f\"Fehler: {e}\")\n",
        "\n",
        "# # Compute the document vectors\n",
        "# X = [document_vector(text) for text in documents_with_vectors]\n",
        "\n",
        "# # Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, labels_with_vectors, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Erstellen Sie ein SVM-Modell\n",
        "# svm = SVC()\n",
        "\n",
        "# # Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# param_grid_svm = {'C': [1, 5], \n",
        "#                   'gamma': [10, 1, 0.1],\n",
        "#                   'kernel': ['linear']}\n",
        "\n",
        "# # Erstelle eine GridSearchCV-Instanz \n",
        "# grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# # Trainiere dein Modell mit der GridSearchCV\n",
        "# grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# # Auswähle den besten Parameter\n",
        "# best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# # Die besten Parameter ausgeben\n",
        "# print(best_params_svm)"
      ],
      "metadata": {
        "id": "g_3xTADD2Fey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDfs9MKTB4NT"
      },
      "source": [
        "# Gridsearch LGBM mit FLAML:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q53vUc5TB4NU"
      },
      "source": [
        "CountVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCNF9IALB4NU"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df_train_lyrics)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df_train_labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1R6DEQB4NU"
      },
      "source": [
        "TfidfVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czTgM0s8B4NU"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df_train_lyrics)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df_train_labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=6000)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTjuXUEWB4NU"
      },
      "source": [
        "Spacy-Vektoren:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y00uqUnJB4NU"
      },
      "outputs": [],
      "source": [
        "# # Funktion definieren, um die Liedtexte in Vektoren zu konvertieren\n",
        "# def document_vector(text):\n",
        "#     doc = nlp(text)\n",
        "#     vectors = []\n",
        "#     for token in doc:\n",
        "#         if token.has_vector:\n",
        "#             vectors.append(token.vector)\n",
        "#     if len(vectors) > 0:\n",
        "#         return np.mean(vectors, axis=0)\n",
        "#     else:\n",
        "#         raise ValueError(\"Keine Vektoren gefunden\")\n",
        "\n",
        "# # Herausfinden, in welchen Spalten keine Vektoren vorhanden sind\n",
        "# no_vector_docs = []\n",
        "# for i, text in enumerate(df_train_lyrics):\n",
        "#     try:\n",
        "#         vec = document_vector(text)\n",
        "#     except ValueError as e:\n",
        "#         print(f\"Fehler: {e} bei Dokument {i}\")\n",
        "#         no_vector_docs.append(i)\n",
        "# print(f\"Dokumente ohne Vektoren: {no_vector_docs}\")\n",
        "\n",
        "# # Alle Zeilen ohne Vektoren löschen\n",
        "# for i in no_vector_docs:\n",
        "#   df_train_labels = df_train_labels.drop(i)\n",
        "#   del df_train_lyrics[i]\n",
        "\n",
        "# # Das Dataframe für die Labels reindexen\n",
        "# df_train_labels = df_train_labels.reset_index(drop=True)  \n",
        "\n",
        "# # Compute the document vectors\n",
        "# documents_with_vectors = []\n",
        "# labels_with_vectors = []\n",
        "# for i in range(len(df_train_lyrics)):\n",
        "#     text = df_train_lyricss[i]\n",
        "#     df_train_labels = df_train_labels[i]\n",
        "#     try:\n",
        "#         vec = document_vector(text)\n",
        "#         documents_with_vectors.append(text)\n",
        "#         labels_with_vectors.append(label)\n",
        "#     except ValueError as e:\n",
        "#         print(f\"Fehler: {e}\")\n",
        "\n",
        "# # Compute the document vectors\n",
        "# X = [document_vector(text) for text in documents_with_vectors]\n",
        "\n",
        "# # Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, labels_with_vectors, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Das Flaml-Modell deklarieren und trainieren\n",
        "# automl = AutoML()\n",
        "# automl.fit(np.array(X_train), np.array(y_train), task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=6000)\n",
        "\n",
        "\n",
        "# # Die besten Hyperparameter ausgeben\n",
        "# print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLA ohne Hyperparameter"
      ],
      "metadata": {
        "id": "Nsx6d5Cl6lsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer:"
      ],
      "metadata": {
        "id": "FDxfDDTD6lsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vektorizer initialisiereen\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Variablen für MLA deklarieren und teils konvertieren\n",
        "X_train = vectorizer.fit_transform(df_train_lyrics)\n",
        "X_test = vectorizer.transform(df_test_lyrics)\n",
        "\n",
        "y_train = df_train_labels\n",
        "y_test = df_test_labels\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)"
      ],
      "metadata": {
        "id": "So6mdCTn6lsg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "svm = SVC()\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersage der Testdaten svm\n",
        "predictions_lgbm = lgbm.predict(X_test)\n",
        "predictions_svm = svm.predict(X_test)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = accuracy_score(y_test, predictions_lgbm)\n",
        "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
        "\n",
        "# Den Klassifikations-Report ausgeben\n",
        "print('LGBM:')\n",
        "print(classification_report(y_test, predictions_lgbm))\n",
        "print('SVM:')\n",
        "print(classification_report(y_test, predictions_svm))\n",
        "\n",
        "# Die Genauigkeit ausgeben\n",
        "print('Accuracy LGBM CouVec:', accuracy_lgbm)\n",
        "print('Accuracy SVM CouVec:', accuracy_svm)"
      ],
      "metadata": {
        "id": "wdbDjq8G6lsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Vectorizer:"
      ],
      "metadata": {
        "id": "CKicsJQq6lsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vektorizer initialisiereen\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Variablen für MLA deklarieren und teils konvertieren\n",
        "X_train = vectorizer.fit_transform(df_train_lyrics)\n",
        "X_test = vectorizer.transform(df_test_lyrics)\n",
        "\n",
        "y_train = df_train_labels\n",
        "y_test = df_test_labels\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)"
      ],
      "metadata": {
        "id": "Dba9oshM6lsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "svm = SVC()\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersage der Testdaten svm\n",
        "predictions_lgbm = lgbm.predict(X_test)\n",
        "predictions_svm = svm.predict(X_test)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = accuracy_score(y_test, predictions_lgbm)\n",
        "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
        "\n",
        "# Den Klassifikations-Report ausgeben\n",
        "print('LGBM:')\n",
        "print(classification_report(y_test, predictions_lgbm))\n",
        "print('SVM:')\n",
        "print(classification_report(y_test, predictions_svm))\n",
        "\n",
        "# Die Genauigkeit ausgeben\n",
        "print('Accuracy LGBM CouVec:', accuracy_lgbm)\n",
        "print('Accuracy SVM CouVec:', accuracy_svm)"
      ],
      "metadata": {
        "id": "uwVlA2UF6lsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLA mit Hyperparameter"
      ],
      "metadata": {
        "id": "qfXodprPmZkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer:"
      ],
      "metadata": {
        "id": "vVj8EqhDmZky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vektorizer initialisiereen\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Variablen für MLA deklarieren und teils konvertieren\n",
        "X_train = vectorizer.fit_transform(df_train_lyrics)\n",
        "X_test = vectorizer.transform(df_test_lyrics)\n",
        "\n",
        "y_train = df_train_labels\n",
        "y_test = df_test_labels\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)"
      ],
      "metadata": {
        "id": "0MCVfGnQmZky"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier(n_estimators=200, num_leaves= 184, min_child_samples=3, \n",
        "                          learning_rate=0.08838576141784195, log_max_bin=9, \n",
        "                          colsample_bytree=0.39039795455449644, \n",
        "                          reg_alpha=0.014844095616079196, \n",
        "                          reg_lambda=0.05525097930389173)\n",
        "svm = SVC(C=0.01, gamma=0.07, kernel='linear')\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersage der Testdaten svm\n",
        "predictions_lgbm = lgbm.predict(X_test)\n",
        "predictions_svm = svm.predict(X_test)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = accuracy_score(y_test, predictions_lgbm)\n",
        "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
        "\n",
        "# Den Klassifikations-Report ausgeben\n",
        "print('LGBM:')\n",
        "print(classification_report(y_test, predictions_lgbm))\n",
        "print('SVM:')\n",
        "print(classification_report(y_test, predictions_svm))\n",
        "\n",
        "# Die Genauigkeit ausgeben\n",
        "print('Accuracy LGBM CouVec:', accuracy_lgbm)\n",
        "print('Accuracy SVM CouVec:', accuracy_svm)"
      ],
      "metadata": {
        "id": "bxuqlPugmZkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Vectorizer:"
      ],
      "metadata": {
        "id": "37dSSbhnmZk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vektorizer initialisiereen\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Variablen für MLA deklarieren und teils konvertieren\n",
        "X_train = vectorizer.fit_transform(df_train_lyrics)\n",
        "X_test = vectorizer.transform(df_test_lyrics)\n",
        "\n",
        "y_train = df_train_labels\n",
        "y_test = df_test_labels\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)"
      ],
      "metadata": {
        "id": "nWp6e2wkmZk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier(n_estimators=67, num_leaves= 184, min_child_samples=12, \n",
        "                          learning_rate=0.085537978248575, log_max_bin=9, \n",
        "                          colsample_bytree=0.7663773657187746, \n",
        "                          reg_alpha=0.006958608037974516, \n",
        "                          reg_lambda=0.4683303882185501)\n",
        "svm = SVC(C=4.5, gamma=0.95, kernel='rbf')\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersage der Testdaten svm\n",
        "predictions_lgbm = lgbm.predict(X_test)\n",
        "predictions_svm = svm.predict(X_test)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = accuracy_score(y_test, predictions_lgbm)\n",
        "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
        "\n",
        "# Den Klassifikations-Report ausgeben\n",
        "print('LGBM:')\n",
        "print(classification_report(y_test, predictions_lgbm))\n",
        "print('SVM:')\n",
        "print(classification_report(y_test, predictions_svm))\n",
        "\n",
        "# Die Genauigkeit ausgeben\n",
        "print('Accuracy LGBM CouVec:', accuracy_lgbm)\n",
        "print('Accuracy SVM CouVec:', accuracy_svm)"
      ],
      "metadata": {
        "id": "DBh8eQiemZk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy-Vectorizer, leider nicht funktionsfähig"
      ],
      "metadata": {
        "id": "oC4Qy6-dpOTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ob diese Funktion funktionsfähig ist, wurde nicht getestet. Bei der Gird-SearchCV\n",
        "# und Flaml ist die Funktion so, wie sie mal eines Abends funktioniert hat aber das \n",
        "# Programm nach 6h Laufzeit abgestützt ist.\n",
        "\n",
        "def spacy_vectorizer(dataframe_lyrics, dataframe_label):\n",
        "  '''\n",
        "  input:\n",
        "    dataframe_lyrics: dataframe mit den Liedtexten\n",
        "    dataframe_label : dataframe mit den Genre\n",
        "\n",
        "  output:\n",
        "    result = array mit dem Liedtexte-Dataframe vektoisiert und mit den Genre\n",
        "  '''\n",
        "  # Funktion definieren, um die Liedtexte in Vektoren zu konvertieren\n",
        "  def document_vector(text):\n",
        "      doc = nlp(text)\n",
        "      vectors = []\n",
        "      for token in doc:\n",
        "          if token.has_vector:\n",
        "              vectors.append(token.vector)\n",
        "      if len(vectors) > 0:\n",
        "          return np.mean(vectors, axis=0)\n",
        "      else:\n",
        "          raise ValueError(\"Keine Vektoren gefunden\")\n",
        "\n",
        "  # Herausfinden, in welchen Spalten keine Vektoren vorhanden sind\n",
        "  no_vector_docs = []\n",
        "  for i, text in enumerate(dataframe_lyrics):\n",
        "      try:\n",
        "          vec = document_vector(text)\n",
        "      except ValueError as e:\n",
        "          print(f\"Fehler: {e} bei Dokument {i}\")\n",
        "          no_vector_docs.append(i)\n",
        "  print(f\"Dokumente ohne Vektoren: {no_vector_docs}\")\n",
        "\n",
        "  # Alle Zeilen ohne Vektoren löschen\n",
        "  for i in no_vector_docs:\n",
        "    dataframe_lyrics = dataframe_lyrics.drop(i)\n",
        "    dataframe_label = dataframe_label.drop(i)\n",
        "\n",
        "  # Das Dataframe für die Labels reindexen\n",
        "  dataframe_label = dataframe_label.reset_index(drop=True)  \n",
        "\n",
        "  # Compute the document vectors\n",
        "  documents_with_vectors = []\n",
        "  labels_with_vectors = []\n",
        "  for i in range(len(dataframe_lyrics)):\n",
        "      text = dataframe_lyrics[i]\n",
        "      dataframe_label = dataframe_label[i]\n",
        "      try:\n",
        "          vec = document_vector(text)\n",
        "          documents_with_vectors.append(text)\n",
        "          labels_with_vectors.append(dataframe_label)\n",
        "      except ValueError as e:\n",
        "          print(f\"Fehler: {e}\")\n",
        "\n",
        "  X = [document_vector(text) for text in documents_with_vectors]\n",
        "  y = labels_with_vectors\n",
        "  result = [X,y]"
      ],
      "metadata": {
        "id": "wYiMTV1DbNU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Laden Sie die Textdokumente und die Klassenlabels\n",
        "df_train_lyrics = df['cleaned_lyrics'] # Liste von Textdokumenten\n",
        "df_train_labels = df['Genre'] # Liste von Klassenlabels (eins pro Dokument)"
      ],
      "metadata": {
        "id": "q1SNWRx-o_aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = spacy_vectorizer(df_test_lyrics, df_test_labels)\n",
        "X_test = X[0]\n",
        "y_test = X[1]\n",
        "X_test = X_test.astype(np.float32)"
      ],
      "metadata": {
        "id": "8FTDMprulAB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = spacy_vectorizer(df_train_lyrics, df_train_labels)\n",
        "X_train = X[0]\n",
        "y_train = X[1]\n",
        "X_train = X_train.astype(np.float32)"
      ],
      "metadata": {
        "id": "4eeBT40wpE6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "svm = SVC()\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersage der Testdaten svm\n",
        "predictions_lgbm = lgbm.predict(X_test)\n",
        "predictions_svm = svm.predict(X_test)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = accuracy_score(y_test, predictions_lgbm)\n",
        "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
        "\n",
        "# Den Klassifikations-Report ausgeben\n",
        "print('LGBM:')\n",
        "print(classification_report(y_test, predictions_lgbm))\n",
        "print('SVM:')\n",
        "print(classification_report(y_test, predictions_svm))\n",
        "\n",
        "# Die Genauigkeit ausgeben\n",
        "print('Accuracy LGBM CouVec:', accuracy_lgbm)\n",
        "print('Accuracy SVM CouVec:', accuracy_svm)"
      ],
      "metadata": {
        "id": "vZzCwqgApM6Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fyb6mPpCJADk",
        "D48vtzWGJAxi",
        "rRr671g-UD2w",
        "_hbI7RZuVHsq",
        "cRpm6PPNA00O",
        "hhYo2eIkBcec",
        "QDfs9MKTB4NT",
        "Nsx6d5Cl6lsf",
        "qfXodprPmZkx",
        "oC4Qy6-dpOTh"
      ],
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}