{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matrosee/Genre-Klassifikation-anhand-von-Liedtexten/blob/main/Code\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyb6mPpCJADk"
      },
      "source": [
        "# Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKornHEzn-PV",
        "outputId": "1ddb7d2a-1bc2-42a6-dfb5-4828bdd784f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.8/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from lightgbm) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from lightgbm) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from lightgbm) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->lightgbm) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->lightgbm) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.11)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-14 12:08:39.515360: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-md==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.11)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.10.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flaml\n",
            "  Downloading FLAML-1.1.1-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.1/216.1 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.8/dist-packages (from flaml) (1.0.2)\n",
            "Requirement already satisfied: NumPy>=1.17.0rc1 in /usr/local/lib/python3.8/dist-packages (from flaml) (1.21.6)\n",
            "Collecting lightgbm>=2.3.1\n",
            "  Downloading lightgbm-3.3.4-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from flaml) (1.7.3)\n",
            "Requirement already satisfied: xgboost>=0.90 in /usr/local/lib/python3.8/dist-packages (from flaml) (0.90)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.8/dist-packages (from flaml) (1.3.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from lightgbm>=2.3.1->flaml) (0.38.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.4->flaml) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.4->flaml) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24->flaml) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24->flaml) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.4->flaml) (1.15.0)\n",
            "Installing collected packages: lightgbm, flaml\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "Successfully installed flaml-1.1.1 lightgbm-3.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install lightgbm\n",
        "!pip install spacy\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install flaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0HqVeIzJJrR",
        "outputId": "15546c7e-460f-4627-c9cc-8c8a44f2b81d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "import flaml\n",
        "import lightgbm as lgb\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from flaml import AutoML\n",
        "from google.colab import files\n",
        "from lightgbm import LGBMClassifier\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# import itertools\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn import preprocessing\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.metrics import (confusion_matrix, plot_confusion_matrix,\n",
        "#     accuracy_score, plot_roc_curve, roc_auc_score, recall_score,\n",
        "#     precision_score, f1_score, roc_curve, auc)\n",
        "# from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACQnvRGSR2uc",
        "outputId": "c7c51c3a-de30-43b3-88f5-0cb349215fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D48vtzWGJAxi"
      },
      "source": [
        "# Upload:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "OPItPM3BJmT6",
        "outputId": "820f8561-1c44-4b2e-d453-71417d23bcb7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-10eccc4e-0329-4d82-aeee-8cc64e59dc10\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-10eccc4e-0329-4d82-aeee-8cc64e59dc10\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "df_train_upload = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1VuxiEPXkXT"
      },
      "source": [
        "# 20 Songs-pro-Datensatz erstellen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTQBz8JP9J4e"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"train (1).csv\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10NkZOiv9WAj"
      },
      "outputs": [],
      "source": [
        "df = df.loc[df['Language']=='en']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN9uikavXk5P"
      },
      "outputs": [],
      "source": [
        "df_rock_zeroo = df[df['Genre']=='Rock']\n",
        "df_rock_zeroo = df_rock_zeroo.head(20)\n",
        "\n",
        "df_pop_zeroo = df[df['Genre']=='Pop']\n",
        "df_pop_zeroo = df_pop_zeroo.head(20)\n",
        "\n",
        "df_metal_zeroo = df[df['Genre']=='Metal']\n",
        "df_metal_zeroo = df_metal_zeroo.head(20)\n",
        "\n",
        "df_hip_hop_zeroo = df[df['Genre']=='Hip-Hop']\n",
        "df_hip_hop_zeroo = df_hip_hop_zeroo.head(20)\n",
        "\n",
        "df_rnb_zeroo = df[df['Genre']=='R&B']\n",
        "df_rnb_zeroo = df_rnb_zeroo.head(20)\n",
        "\n",
        "df_indie_zeroo = df[df['Genre']=='Indie']\n",
        "df_indie_zeroo = df_indie_zeroo.head(20)\n",
        "\n",
        "df_electronic_zeroo = df[df['Genre']=='Electronic']\n",
        "df_electronic_zeroo = df_electronic_zeroo.head(20)\n",
        "\n",
        "df_jazz_zeroo = df[df['Genre']=='Jazz']\n",
        "df_jazz_zeroo = df_jazz_zeroo.head(20)\n",
        "\n",
        "df_folk_zeroo = df[df['Genre']=='Folk']\n",
        "df_folk_zeroo = df_folk_zeroo.head(20)\n",
        "\n",
        "df_country_zeroo = df[df['Genre']=='Country']\n",
        "df_country_zeroo = df_country_zeroo.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkdFZewVXk7k"
      },
      "outputs": [],
      "source": [
        "df_zeroo = pd.concat([df_pop_zeroo, df_hip_hop_zeroo, df_metal_zeroo, df_rock_zeroo, \n",
        "                    df_indie_zeroo, df_country_zeroo, df_electronic_zeroo, df_rnb_zeroo,\n",
        "                    df_jazz_zeroo, df_folk_zeroo])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZfS7FVUeRsa"
      },
      "outputs": [],
      "source": [
        "#200\n",
        "df = df_zeroo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_le6rXlIeRuu"
      },
      "outputs": [],
      "source": [
        "df.dropna()\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrzHyXpSxgMt"
      },
      "source": [
        "# 1809 Songs-pro-Datensatz erstellen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7rEJomI9Ihu"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"train (1).csv\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMEnDnhp9Zpa"
      },
      "outputs": [],
      "source": [
        "df = df.loc[df['Language']=='en']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJwz0_YOLBTL"
      },
      "outputs": [],
      "source": [
        "df_rock_one = df[df['Genre']=='Rock']\n",
        "df_rock_one = df_rock_one.head(1890)\n",
        "\n",
        "df_pop_one = df[df['Genre']=='Pop']\n",
        "df_pop_one = df_pop_one.head(1890)\n",
        "\n",
        "df_metal_one = df[df['Genre']=='Metal']\n",
        "df_metal_one = df_metal_one.head(1890)\n",
        "\n",
        "df_hip_hop_one = df[df['Genre']=='Hip-Hop']\n",
        "df_hip_hop_one = df_hip_hop_one.head(1890)\n",
        "\n",
        "df_rnb_one = df[df['Genre']=='R&B']\n",
        "df_rnb_one = df_rnb_one.head(1890)\n",
        "\n",
        "df_indie_one = df[df['Genre']=='Indie']\n",
        "df_indie_one = df_indie_one.head(1890)\n",
        "\n",
        "df_electronic_one = df[df['Genre']=='Electronic']\n",
        "df_electronic_one = df_electronic_one.head(1890)\n",
        "\n",
        "df_jazz_one = df[df['Genre']=='Jazz']\n",
        "df_jazz_one = df_jazz_one.head(1890)\n",
        "\n",
        "df_folk_one = df[df['Genre']=='Folk']\n",
        "df_folk_one = df_folk_one.head(1890)\n",
        "\n",
        "df_country_one = df[df['Genre']=='Country']\n",
        "df_country_one = df_country_one.head(1890)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJY-hX_gxxvB"
      },
      "outputs": [],
      "source": [
        "df_one = pd.concat([df_pop_one, df_hip_hop_one, df_metal_one, df_rock_one, \n",
        "                    df_indie_one, df_country_one, df_electronic_one, df_rnb_one,\n",
        "                    df_jazz_one, df_folk_one])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "samyvcev9oVq"
      },
      "outputs": [],
      "source": [
        "df = df_one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpdsgcWfebpn"
      },
      "outputs": [],
      "source": [
        "df.dropna()\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L22lHiC4JA3p"
      },
      "source": [
        "# Pre-Processing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q24E_72Kxm9S"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC8ftyYrSLRr"
      },
      "outputs": [],
      "source": [
        "en_stops = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuAomwQ2LJve"
      },
      "source": [
        "Datensatz säubern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXwxkWHtJ_oq"
      },
      "outputs": [],
      "source": [
        "df['Lyrics'] = [re.sub(r'^.*?Lyrics', '', str(lyric)) for lyric in df['Lyrics']]\n",
        "df['cleaned_lyrics'] = [str(lyric).replace('\\n',' ') for lyric in df['Lyrics']]\n",
        "df['cleaned_lyrics'] = [re.sub(\"\\[.*?\\]\",\"\",lyric) for lyric in df['cleaned_lyrics']]\n",
        "df = df.drop('Language',axis=1)\n",
        "df = df.drop('Song', axis=1)\n",
        "df = df.drop('Artist', axis=1)\n",
        "df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_okGkwGfIlQ"
      },
      "source": [
        "Stemming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-RP10ueYqAI"
      },
      "outputs": [],
      "source": [
        "df['stemmed_lyrics'] = \"\"\n",
        "ps = PorterStemmer() \n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['cleaned_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for word in words:\n",
        "    tmp = tmp + space + ps.stem(word)\n",
        "  df['stemmed_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60LihYWhfMiN"
      },
      "source": [
        "Stop-Words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ym62KNbfWjk"
      },
      "outputs": [],
      "source": [
        "df['wosw_lyrics'] = \"\"\n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['cleaned_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for word in words:\n",
        "    if word not in en_stops:\n",
        "      tmp = tmp + space + word\n",
        "  df['wosw_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr6Vl8-8gMYX"
      },
      "source": [
        "Stemming -> Stop-Words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18Ajcowo_2Ey"
      },
      "outputs": [],
      "source": [
        "df['stemmed_wosw_lyrics'] = \"\"\n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['stemmed_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for word in words:\n",
        "    if word not in en_stops:\n",
        "      tmp = tmp + space + word\n",
        "  df['stemmed_wosw_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpjpCmgWgRkn"
      },
      "source": [
        "StopWords -> Stemming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4kDwNmD_2ek"
      },
      "outputs": [],
      "source": [
        "df['wosw_stemmed_lyrics'] = \"\"\n",
        "ps = PorterStemmer() \n",
        "space = \" \"\n",
        "tmp = \"\"\n",
        "count = 0\n",
        "for lyric in df['wosw_lyrics']:\n",
        "  words = word_tokenize(lyric)\n",
        "  tmp = \"\"\n",
        "  for x in words:\n",
        "    tmp = tmp + space + ps.stem(x)\n",
        "  df['wosw_stemmed_lyrics'].iloc[count] = tmp\n",
        "  count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS06d_5cesIX"
      },
      "source": [
        "# Einstellen, welche Lyrik-Spalten verwendet werden sollen +  Spacy-Model laden:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mADbEquke6kE"
      },
      "outputs": [],
      "source": [
        "# Laden Sie das spacy-Modell und den Englisch-Vokabular\n",
        "nlp = spacy.load('en_core_web_md')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwn-mtbmey5A"
      },
      "outputs": [],
      "source": [
        "# Laden Sie die Textdokumente und die Klassenlabels\n",
        "documents = df['cleaned_lyrics'].tolist() # Liste von Textdokumenten\n",
        "labels = df['Genre'] # Liste von Klassenlabels (eins pro Dokument)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPeiCQBuFvz9"
      },
      "source": [
        "# SVM & LGBM:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8UE9pGbi6uE"
      },
      "source": [
        "CountVektorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "9Ad7CyeOgWMW",
        "outputId": "a1fe7037-c26f-4831-ca97-fac1bec669be"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2604808d0713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Trainieren Sie die Modelle anhand der Trainingsdaten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    965\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n\u001b[0m\u001b[1;32m    968\u001b[0m                     \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_sample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                     \u001b[0meval_class_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_class_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_init_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;31m# construct booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2603\u001b[0m                 )\n\u001b[1;32m   2604\u001b[0m             \u001b[0;31m# construct booster object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2605\u001b[0;31m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2606\u001b[0m             \u001b[0;31m# copy the parameters from train_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2607\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1813\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m                 \u001b[0;31m# create train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1815\u001b[0;31m                 self._lazy_init(self.data, label=self.label,\n\u001b[0m\u001b[1;32m   1816\u001b[0m                                 \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m   1532\u001b[0m                 ctypes.byref(self.handle)))\n\u001b[1;32m   1533\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init_from_csr\u001b[0;34m(self, csr, params_str, ref_dataset)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0mptr_indptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_ptr_indptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_int_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m         \u001b[0mptr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_ptr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_float_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mMAX_INT32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mc_float_array\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mtype_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC_API_DTYPE_FLOAT64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected np.float32 or np.float64, met type({data.dtype})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown type({type(data).__name__})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected np.float32 or np.float64, met type(int64)"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "X = X.astype(np.float32)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "svm = SVC()\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersage der Testdaten svm\n",
        "predictions_lgbm = lgbm.predict(X_test)\n",
        "predictions_svm = svm.predict(X_test)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = accuracy_score(y_test, predictions_lgbm)\n",
        "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
        "\n",
        "# Den Klassifikations-Report ausgeben\n",
        "print(classification_report(y_test, predictions_lgbm))\n",
        "print(classification_report(y_test, predictions_svm))\n",
        "\n",
        "# Die Genauigkeit ausgeben\n",
        "print('Accuracy LGBM CouVec:', accuracy_lgbm)\n",
        "print('Accuracy SVM CouVec:', accuracy_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NoRpTmEi9OP"
      },
      "source": [
        "TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8FH-iF5gWTq",
        "outputId": "b19737f7-084d-4a50-da3a-0163034f10e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy SVM TFIDF: 0.24242424242424243\n",
            "Accuracy SVM TFIDF: 0.22727272727272727\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "svm = SVC()\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersage der Testdaten svm\n",
        "predictions_lgbm = lgbm.predict(X_test)\n",
        "predictions_svm = svm.predict(X_test)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = accuracy_score(y_test, predictions_lgbm)\n",
        "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
        "\n",
        "# Den Klassifikations-Report ausgeben\n",
        "print(classification_report(y_test, predictions_svm))\n",
        "print(classification_report(y_test, predictions_lgbm))\n",
        "\n",
        "# Die Genauigkeit ausgeben\n",
        "print('Accuracy SVM TFIDF:', accuracy_lgbm)\n",
        "print('Accuracy SVM TFIDF:', accuracy_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "156kX_feqOfD"
      },
      "source": [
        "Spacy-Vektoren:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIZS8bdiqOkZ"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem TfidfVectorizer\n",
        "def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "def document_vector(docs):\n",
        "    vectors = [token.vector for text in docs for token in nlp(text)]\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# Tokenize and lemmatize the documents\n",
        "X = [tokenize_and_lemmatize(text) for text in documents]\n",
        "\n",
        "# Compute the document vectors\n",
        "X = [document_vector(X) for X in X]\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie das SVM- und LGBM-Modell\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "svm = SVC()\n",
        "\n",
        "# Trainieren Sie die Modelle anhand der Trainingsdaten\n",
        "lgbm.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersage der Testdaten svm\n",
        "predictions_lgbm = lgbm.predict(X_test)\n",
        "predictions_svm = svm.predict(X_test)\n",
        "\n",
        "# Berechnen Sie die Genauigkeit\n",
        "accuracy_lgbm = accuracy_score(y_test, predictions_lgbm)\n",
        "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
        "\n",
        "# Den Klassifikations-Report ausgeben\n",
        "print(classification_report(y_test, predictions_svm))\n",
        "print(classification_report(y_test, predictions_lgbm))\n",
        "\n",
        "# Die Genauigkeit ausgeben\n",
        "print('Accuracy SVM Spacy-Vektoren:', accuracy_lgbm)\n",
        "print('Accuracy SVM Spacy-Vektoren:', accuracy_svm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqjjXcfyt9S5"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a function to tokenize and lemmatize the text\n",
        "def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "# Define a function to compute the document vectors\n",
        "def document_vector(documents):\n",
        "    vectors = [token.vector for text in documents for token in nlp(text)]\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# Tokenize and lemmatize the documents\n",
        "X = [tokenize_and_lemmatize(text) for text in documents]\n",
        "\n",
        "# Compute the document vectors\n",
        "X = [document_vector(X) for X in X]\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Initialize the classifier\n",
        "clf = SVC()\n",
        "\n",
        "# Fit the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print a classification report\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg9p62FehEgR"
      },
      "source": [
        "# 1. Grid-search SVM mit SKLEARN:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bL5SB9JT-O_"
      },
      "source": [
        "WIE ICH IN ZUKUNFT DIE OPTIMALEN PARAMETER RAUSFINDE:\n",
        "Ich taste mich erstmal mit den jetztigen ran, schau mir die optimalen an, dann\n",
        "werde ich eine Reihe von Paramtern die ungefähr um den optimalen Parameter liegen testen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG2EBzRIiyhg"
      },
      "source": [
        "CountVektorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "g52JXIkheP2j",
        "outputId": "1ce7fa32-3fc7-4ba3-a97b-89d66b7bcf35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "[CV 1/5] END ..C=0.5, gamma=0.75, kernel=linear;, score=0.429 total time= 3.0min\n",
            "[CV 2/5] END ..C=0.5, gamma=0.75, kernel=linear;, score=0.426 total time= 3.0min\n",
            "[CV 3/5] END ..C=0.5, gamma=0.75, kernel=linear;, score=0.428 total time= 3.1min\n",
            "[CV 4/5] END ..C=0.5, gamma=0.75, kernel=linear;, score=0.434 total time= 3.0min\n",
            "[CV 5/5] END ..C=0.5, gamma=0.75, kernel=linear;, score=0.424 total time= 3.1min\n",
            "[CV 1/5] END ...C=0.5, gamma=0.9, kernel=linear;, score=0.429 total time= 3.1min\n",
            "[CV 2/5] END ...C=0.5, gamma=0.9, kernel=linear;, score=0.426 total time= 3.0min\n",
            "[CV 3/5] END ...C=0.5, gamma=0.9, kernel=linear;, score=0.428 total time= 3.1min\n",
            "[CV 4/5] END ...C=0.5, gamma=0.9, kernel=linear;, score=0.434 total time= 3.0min\n",
            "[CV 5/5] END ...C=0.5, gamma=0.9, kernel=linear;, score=0.424 total time= 3.0min\n",
            "[CV 1/5] END .....C=0.5, gamma=1, kernel=linear;, score=0.429 total time= 3.0min\n",
            "[CV 2/5] END .....C=0.5, gamma=1, kernel=linear;, score=0.426 total time= 3.0min\n",
            "[CV 3/5] END .....C=0.5, gamma=1, kernel=linear;, score=0.428 total time= 3.1min\n",
            "[CV 4/5] END .....C=0.5, gamma=1, kernel=linear;, score=0.434 total time= 3.3min\n",
            "[CV 5/5] END .....C=0.5, gamma=1, kernel=linear;, score=0.424 total time= 3.1min\n",
            "[CV 1/5] END ..C=0.5, gamma=1.25, kernel=linear;, score=0.429 total time= 3.1min\n",
            "[CV 2/5] END ..C=0.5, gamma=1.25, kernel=linear;, score=0.426 total time= 3.0min\n",
            "[CV 3/5] END ..C=0.5, gamma=1.25, kernel=linear;, score=0.428 total time= 3.1min\n",
            "[CV 4/5] END ..C=0.5, gamma=1.25, kernel=linear;, score=0.434 total time= 3.0min\n",
            "[CV 5/5] END ..C=0.5, gamma=1.25, kernel=linear;, score=0.424 total time= 3.0min\n",
            "[CV 1/5] END ..C=0.9, gamma=0.75, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 2/5] END ..C=0.9, gamma=0.75, kernel=linear;, score=0.429 total time= 3.0min\n",
            "[CV 3/5] END ..C=0.9, gamma=0.75, kernel=linear;, score=0.427 total time= 3.0min\n",
            "[CV 4/5] END ..C=0.9, gamma=0.75, kernel=linear;, score=0.432 total time= 3.0min\n",
            "[CV 5/5] END ..C=0.9, gamma=0.75, kernel=linear;, score=0.423 total time= 3.0min\n",
            "[CV 1/5] END ...C=0.9, gamma=0.9, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 2/5] END ...C=0.9, gamma=0.9, kernel=linear;, score=0.429 total time= 3.1min\n",
            "[CV 3/5] END ...C=0.9, gamma=0.9, kernel=linear;, score=0.427 total time= 3.0min\n",
            "[CV 4/5] END ...C=0.9, gamma=0.9, kernel=linear;, score=0.432 total time= 3.0min\n",
            "[CV 5/5] END ...C=0.9, gamma=0.9, kernel=linear;, score=0.423 total time= 3.0min\n",
            "[CV 1/5] END .....C=0.9, gamma=1, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 2/5] END .....C=0.9, gamma=1, kernel=linear;, score=0.429 total time= 3.0min\n",
            "[CV 3/5] END .....C=0.9, gamma=1, kernel=linear;, score=0.427 total time= 3.0min\n",
            "[CV 4/5] END .....C=0.9, gamma=1, kernel=linear;, score=0.432 total time= 2.9min\n",
            "[CV 5/5] END .....C=0.9, gamma=1, kernel=linear;, score=0.423 total time= 3.0min\n",
            "[CV 1/5] END ..C=0.9, gamma=1.25, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 2/5] END ..C=0.9, gamma=1.25, kernel=linear;, score=0.429 total time= 3.1min\n",
            "[CV 3/5] END ..C=0.9, gamma=1.25, kernel=linear;, score=0.427 total time= 3.0min\n",
            "[CV 4/5] END ..C=0.9, gamma=1.25, kernel=linear;, score=0.432 total time= 3.0min\n",
            "[CV 5/5] END ..C=0.9, gamma=1.25, kernel=linear;, score=0.423 total time= 3.0min\n",
            "[CV 1/5] END ....C=1, gamma=0.75, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 2/5] END ....C=1, gamma=0.75, kernel=linear;, score=0.428 total time= 3.1min\n",
            "[CV 3/5] END ....C=1, gamma=0.75, kernel=linear;, score=0.426 total time= 3.0min\n",
            "[CV 4/5] END ....C=1, gamma=0.75, kernel=linear;, score=0.431 total time= 3.0min\n",
            "[CV 5/5] END ....C=1, gamma=0.75, kernel=linear;, score=0.423 total time= 3.0min\n",
            "[CV 1/5] END .....C=1, gamma=0.9, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 2/5] END .....C=1, gamma=0.9, kernel=linear;, score=0.428 total time= 3.1min\n",
            "[CV 3/5] END .....C=1, gamma=0.9, kernel=linear;, score=0.426 total time= 3.0min\n",
            "[CV 4/5] END .....C=1, gamma=0.9, kernel=linear;, score=0.431 total time= 3.0min\n",
            "[CV 5/5] END .....C=1, gamma=0.9, kernel=linear;, score=0.423 total time= 3.0min\n",
            "[CV 1/5] END .......C=1, gamma=1, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 2/5] END .......C=1, gamma=1, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 3/5] END .......C=1, gamma=1, kernel=linear;, score=0.426 total time= 3.0min\n",
            "[CV 4/5] END .......C=1, gamma=1, kernel=linear;, score=0.431 total time= 3.0min\n",
            "[CV 5/5] END .......C=1, gamma=1, kernel=linear;, score=0.423 total time= 3.0min\n",
            "[CV 1/5] END ....C=1, gamma=1.25, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 2/5] END ....C=1, gamma=1.25, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 3/5] END ....C=1, gamma=1.25, kernel=linear;, score=0.426 total time= 3.0min\n",
            "[CV 4/5] END ....C=1, gamma=1.25, kernel=linear;, score=0.431 total time= 2.9min\n",
            "[CV 5/5] END ....C=1, gamma=1.25, kernel=linear;, score=0.423 total time= 3.0min\n",
            "[CV 1/5] END ..C=1.5, gamma=0.75, kernel=linear;, score=0.428 total time= 2.9min\n",
            "[CV 2/5] END ..C=1.5, gamma=0.75, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 3/5] END ..C=1.5, gamma=0.75, kernel=linear;, score=0.424 total time= 3.0min\n",
            "[CV 4/5] END ..C=1.5, gamma=0.75, kernel=linear;, score=0.431 total time= 3.0min\n",
            "[CV 5/5] END ..C=1.5, gamma=0.75, kernel=linear;, score=0.420 total time= 3.0min\n",
            "[CV 1/5] END ...C=1.5, gamma=0.9, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 2/5] END ...C=1.5, gamma=0.9, kernel=linear;, score=0.428 total time= 3.1min\n",
            "[CV 3/5] END ...C=1.5, gamma=0.9, kernel=linear;, score=0.424 total time= 3.0min\n",
            "[CV 4/5] END ...C=1.5, gamma=0.9, kernel=linear;, score=0.431 total time= 2.9min\n",
            "[CV 5/5] END ...C=1.5, gamma=0.9, kernel=linear;, score=0.420 total time= 3.0min\n",
            "[CV 1/5] END .....C=1.5, gamma=1, kernel=linear;, score=0.428 total time= 2.9min\n",
            "[CV 2/5] END .....C=1.5, gamma=1, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 3/5] END .....C=1.5, gamma=1, kernel=linear;, score=0.424 total time= 3.1min\n",
            "[CV 4/5] END .....C=1.5, gamma=1, kernel=linear;, score=0.431 total time= 3.0min\n",
            "[CV 5/5] END .....C=1.5, gamma=1, kernel=linear;, score=0.420 total time= 3.1min\n",
            "[CV 1/5] END ..C=1.5, gamma=1.25, kernel=linear;, score=0.428 total time= 3.0min\n",
            "[CV 2/5] END ..C=1.5, gamma=1.25, kernel=linear;, score=0.428 total time= 3.1min\n",
            "[CV 3/5] END ..C=1.5, gamma=1.25, kernel=linear;, score=0.424 total time= 3.1min\n",
            "[CV 4/5] END ..C=1.5, gamma=1.25, kernel=linear;, score=0.431 total time= 3.0min\n",
            "[CV 5/5] END ..C=1.5, gamma=1.25, kernel=linear;, score=0.420 total time= 3.0min\n",
            "{'C': 0.5, 'gamma': 0.75, 'kernel': 'linear'}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [0.2,0.35, 0.5, 0.7], \n",
        "                  'gamma': [0.3, 0.5, 0.75, 0.8],\n",
        "                  'kernel': ['linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jVhjq4WiwuY"
      },
      "source": [
        "SVM mit TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBst34Yldi8V",
        "outputId": "30f413fa-99d8-4ce6-9341-61e6dffd3d94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "[CV 1/5] END .....C=8, gamma=0.9, kernel=linear;, score=0.443 total time= 3.0min\n",
            "[CV 2/5] END .....C=8, gamma=0.9, kernel=linear;, score=0.450 total time= 2.9min\n",
            "[CV 3/5] END .....C=8, gamma=0.9, kernel=linear;, score=0.450 total time= 3.0min\n",
            "[CV 4/5] END .....C=8, gamma=0.9, kernel=linear;, score=0.450 total time= 3.0min\n",
            "[CV 5/5] END .....C=8, gamma=0.9, kernel=linear;, score=0.460 total time= 2.9min\n",
            "[CV 1/5] END .......C=8, gamma=1, kernel=linear;, score=0.443 total time= 2.9min\n",
            "[CV 2/5] END .......C=8, gamma=1, kernel=linear;, score=0.450 total time= 2.9min\n",
            "[CV 3/5] END .......C=8, gamma=1, kernel=linear;, score=0.450 total time= 3.0min\n",
            "[CV 4/5] END .......C=8, gamma=1, kernel=linear;, score=0.450 total time= 3.0min\n",
            "[CV 5/5] END .......C=8, gamma=1, kernel=linear;, score=0.460 total time= 2.9min\n",
            "[CV 1/5] END .....C=8, gamma=1.2, kernel=linear;, score=0.443 total time= 2.9min\n",
            "[CV 2/5] END .....C=8, gamma=1.2, kernel=linear;, score=0.450 total time= 2.9min\n",
            "[CV 3/5] END .....C=8, gamma=1.2, kernel=linear;, score=0.450 total time= 3.0min\n",
            "[CV 4/5] END .....C=8, gamma=1.2, kernel=linear;, score=0.450 total time= 2.9min\n",
            "[CV 5/5] END .....C=8, gamma=1.2, kernel=linear;, score=0.460 total time= 2.9min\n",
            "[CV 1/5] END .......C=8, gamma=2, kernel=linear;, score=0.443 total time= 2.9min\n",
            "[CV 2/5] END .......C=8, gamma=2, kernel=linear;, score=0.450 total time= 2.9min\n",
            "[CV 3/5] END .......C=8, gamma=2, kernel=linear;, score=0.450 total time= 2.9min\n",
            "[CV 4/5] END .......C=8, gamma=2, kernel=linear;, score=0.450 total time= 2.9min\n",
            "[CV 5/5] END .......C=8, gamma=2, kernel=linear;, score=0.460 total time= 2.9min\n",
            "[CV 1/5] END ....C=10, gamma=0.9, kernel=linear;, score=0.443 total time= 2.9min\n",
            "[CV 2/5] END ....C=10, gamma=0.9, kernel=linear;, score=0.447 total time= 2.9min\n",
            "[CV 3/5] END ....C=10, gamma=0.9, kernel=linear;, score=0.452 total time= 2.9min\n",
            "[CV 4/5] END ....C=10, gamma=0.9, kernel=linear;, score=0.450 total time= 2.9min\n",
            "[CV 5/5] END ....C=10, gamma=0.9, kernel=linear;, score=0.453 total time= 2.9min\n",
            "[CV 1/5] END ......C=10, gamma=1, kernel=linear;, score=0.443 total time= 2.9min\n",
            "[CV 2/5] END ......C=10, gamma=1, kernel=linear;, score=0.447 total time= 2.9min\n",
            "[CV 3/5] END ......C=10, gamma=1, kernel=linear;, score=0.452 total time= 2.9min\n",
            "[CV 4/5] END ......C=10, gamma=1, kernel=linear;, score=0.450 total time= 2.9min\n",
            "[CV 5/5] END ......C=10, gamma=1, kernel=linear;, score=0.453 total time= 2.9min\n",
            "[CV 1/5] END ....C=10, gamma=1.2, kernel=linear;, score=0.443 total time= 2.9min\n",
            "[CV 2/5] END ....C=10, gamma=1.2, kernel=linear;, score=0.447 total time= 2.9min\n",
            "[CV 3/5] END ....C=10, gamma=1.2, kernel=linear;, score=0.452 total time= 2.9min\n",
            "[CV 4/5] END ....C=10, gamma=1.2, kernel=linear;, score=0.450 total time= 2.9min\n",
            "[CV 5/5] END ....C=10, gamma=1.2, kernel=linear;, score=0.453 total time= 2.9min\n",
            "[CV 1/5] END ......C=10, gamma=2, kernel=linear;, score=0.443 total time= 2.9min\n",
            "[CV 2/5] END ......C=10, gamma=2, kernel=linear;, score=0.447 total time= 2.9min\n",
            "[CV 3/5] END ......C=10, gamma=2, kernel=linear;, score=0.452 total time= 2.9min\n",
            "[CV 4/5] END ......C=10, gamma=2, kernel=linear;, score=0.450 total time= 2.9min\n",
            "[CV 5/5] END ......C=10, gamma=2, kernel=linear;, score=0.453 total time= 2.8min\n",
            "[CV 1/5] END ....C=15, gamma=0.9, kernel=linear;, score=0.446 total time= 2.8min\n",
            "[CV 2/5] END ....C=15, gamma=0.9, kernel=linear;, score=0.445 total time= 2.8min\n",
            "[CV 3/5] END ....C=15, gamma=0.9, kernel=linear;, score=0.453 total time= 3.0min\n",
            "[CV 4/5] END ....C=15, gamma=0.9, kernel=linear;, score=0.454 total time= 2.9min\n",
            "[CV 5/5] END ....C=15, gamma=0.9, kernel=linear;, score=0.447 total time= 2.9min\n",
            "[CV 1/5] END ......C=15, gamma=1, kernel=linear;, score=0.446 total time= 2.8min\n",
            "[CV 2/5] END ......C=15, gamma=1, kernel=linear;, score=0.445 total time= 2.9min\n",
            "[CV 3/5] END ......C=15, gamma=1, kernel=linear;, score=0.453 total time= 2.9min\n",
            "[CV 4/5] END ......C=15, gamma=1, kernel=linear;, score=0.454 total time= 2.9min\n",
            "[CV 5/5] END ......C=15, gamma=1, kernel=linear;, score=0.447 total time= 2.9min\n",
            "[CV 1/5] END ....C=15, gamma=1.2, kernel=linear;, score=0.446 total time= 2.8min\n",
            "[CV 2/5] END ....C=15, gamma=1.2, kernel=linear;, score=0.445 total time= 2.9min\n",
            "[CV 3/5] END ....C=15, gamma=1.2, kernel=linear;, score=0.453 total time= 2.9min\n",
            "[CV 4/5] END ....C=15, gamma=1.2, kernel=linear;, score=0.454 total time= 2.9min\n",
            "[CV 5/5] END ....C=15, gamma=1.2, kernel=linear;, score=0.447 total time= 2.9min\n",
            "[CV 1/5] END ......C=15, gamma=2, kernel=linear;, score=0.446 total time= 2.9min\n",
            "[CV 2/5] END ......C=15, gamma=2, kernel=linear;, score=0.445 total time= 2.9min\n",
            "[CV 3/5] END ......C=15, gamma=2, kernel=linear;, score=0.453 total time= 2.9min\n",
            "[CV 4/5] END ......C=15, gamma=2, kernel=linear;, score=0.454 total time= 2.9min\n",
            "[CV 5/5] END ......C=15, gamma=2, kernel=linear;, score=0.447 total time= 2.8min\n",
            "[CV 1/5] END ....C=50, gamma=0.9, kernel=linear;, score=0.448 total time= 2.8min\n",
            "[CV 2/5] END ....C=50, gamma=0.9, kernel=linear;, score=0.446 total time= 2.8min\n",
            "[CV 3/5] END ....C=50, gamma=0.9, kernel=linear;, score=0.444 total time= 2.8min\n",
            "[CV 4/5] END ....C=50, gamma=0.9, kernel=linear;, score=0.453 total time= 2.8min\n",
            "[CV 5/5] END ....C=50, gamma=0.9, kernel=linear;, score=0.446 total time= 2.8min\n",
            "[CV 1/5] END ......C=50, gamma=1, kernel=linear;, score=0.448 total time= 2.8min\n",
            "[CV 2/5] END ......C=50, gamma=1, kernel=linear;, score=0.446 total time= 2.8min\n",
            "[CV 3/5] END ......C=50, gamma=1, kernel=linear;, score=0.444 total time= 2.8min\n",
            "[CV 4/5] END ......C=50, gamma=1, kernel=linear;, score=0.453 total time= 2.9min\n",
            "[CV 5/5] END ......C=50, gamma=1, kernel=linear;, score=0.446 total time= 2.8min\n",
            "[CV 1/5] END ....C=50, gamma=1.2, kernel=linear;, score=0.448 total time= 2.8min\n",
            "[CV 2/5] END ....C=50, gamma=1.2, kernel=linear;, score=0.446 total time= 2.8min\n",
            "[CV 3/5] END ....C=50, gamma=1.2, kernel=linear;, score=0.444 total time= 2.8min\n",
            "[CV 4/5] END ....C=50, gamma=1.2, kernel=linear;, score=0.453 total time= 2.9min\n",
            "[CV 5/5] END ....C=50, gamma=1.2, kernel=linear;, score=0.446 total time= 2.8min\n",
            "[CV 1/5] END ......C=50, gamma=2, kernel=linear;, score=0.448 total time= 2.8min\n",
            "[CV 2/5] END ......C=50, gamma=2, kernel=linear;, score=0.446 total time= 2.8min\n",
            "[CV 3/5] END ......C=50, gamma=2, kernel=linear;, score=0.444 total time= 2.9min\n",
            "[CV 4/5] END ......C=50, gamma=2, kernel=linear;, score=0.453 total time= 2.9min\n",
            "[CV 5/5] END ......C=50, gamma=2, kernel=linear;, score=0.446 total time= 2.8min\n",
            "{'C': 8, 'gamma': 0.9, 'kernel': 'linear'}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [4,6,8,9], \n",
        "                  'gamma': [0.5, 0.7, 0.9, 0.95],\n",
        "                  'kernel': ['linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgJgToB7qsVF"
      },
      "source": [
        "Spacy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4ddQSNdqsZi",
        "outputId": "d7878cbc-650f-4f02-ba51-3f4686154a69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 651, in score\n",
            "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 791, in predict\n",
            "    y = super().predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 414, in predict\n",
            "    X = self._validate_for_predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 592, in _validate_for_predict\n",
            "    X = self._validate_data(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 566, in _validate_data\n",
            "    X = check_array(X, **check_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2533,) + inhomogeneous part.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END ............C=1, gamma=1, kernel=rbf;, score=nan total time=  42.5s\n",
            "[CV 2/5] END ............C=1, gamma=1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 3/5] END ............C=1, gamma=1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 4/5] END ............C=1, gamma=1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 5/5] END ............C=1, gamma=1, kernel=rbf;, score=nan total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 651, in score\n",
            "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 791, in predict\n",
            "    y = super().predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 414, in predict\n",
            "    X = self._validate_for_predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 592, in _validate_for_predict\n",
            "    X = self._validate_data(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 566, in _validate_data\n",
            "    X = check_array(X, **check_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2533,) + inhomogeneous part.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END .........C=1, gamma=1, kernel=linear;, score=nan total time= 2.4min\n",
            "[CV 2/5] END .........C=1, gamma=1, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 3/5] END .........C=1, gamma=1, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 4/5] END .........C=1, gamma=1, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 5/5] END .........C=1, gamma=1, kernel=linear;, score=nan total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 651, in score\n",
            "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 791, in predict\n",
            "    y = super().predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 414, in predict\n",
            "    X = self._validate_for_predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 592, in _validate_for_predict\n",
            "    X = self._validate_data(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 566, in _validate_data\n",
            "    X = check_array(X, **check_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2533,) + inhomogeneous part.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END ..........C=1, gamma=0.1, kernel=rbf;, score=nan total time=  42.2s\n",
            "[CV 2/5] END ..........C=1, gamma=0.1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 3/5] END ..........C=1, gamma=0.1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 4/5] END ..........C=1, gamma=0.1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 5/5] END ..........C=1, gamma=0.1, kernel=rbf;, score=nan total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 651, in score\n",
            "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 791, in predict\n",
            "    y = super().predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 414, in predict\n",
            "    X = self._validate_for_predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 592, in _validate_for_predict\n",
            "    X = self._validate_data(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 566, in _validate_data\n",
            "    X = check_array(X, **check_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2533,) + inhomogeneous part.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END .......C=1, gamma=0.1, kernel=linear;, score=nan total time= 2.4min\n",
            "[CV 2/5] END .......C=1, gamma=0.1, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 3/5] END .......C=1, gamma=0.1, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 4/5] END .......C=1, gamma=0.1, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 5/5] END .......C=1, gamma=0.1, kernel=linear;, score=nan total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 651, in score\n",
            "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 791, in predict\n",
            "    y = super().predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 414, in predict\n",
            "    X = self._validate_for_predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 592, in _validate_for_predict\n",
            "    X = self._validate_data(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 566, in _validate_data\n",
            "    X = check_array(X, **check_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2533,) + inhomogeneous part.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END .........C=1, gamma=0.01, kernel=rbf;, score=nan total time=  31.2s\n",
            "[CV 2/5] END .........C=1, gamma=0.01, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 3/5] END .........C=1, gamma=0.01, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 4/5] END .........C=1, gamma=0.01, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 5/5] END .........C=1, gamma=0.01, kernel=rbf;, score=nan total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 651, in score\n",
            "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 791, in predict\n",
            "    y = super().predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 414, in predict\n",
            "    X = self._validate_for_predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 592, in _validate_for_predict\n",
            "    X = self._validate_data(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 566, in _validate_data\n",
            "    X = check_array(X, **check_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2533,) + inhomogeneous part.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END ......C=1, gamma=0.01, kernel=linear;, score=nan total time= 2.3min\n",
            "[CV 2/5] END ......C=1, gamma=0.01, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 3/5] END ......C=1, gamma=0.01, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 4/5] END ......C=1, gamma=0.01, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 5/5] END ......C=1, gamma=0.01, kernel=linear;, score=nan total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 651, in score\n",
            "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 791, in predict\n",
            "    y = super().predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 414, in predict\n",
            "    X = self._validate_for_predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 592, in _validate_for_predict\n",
            "    X = self._validate_data(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 566, in _validate_data\n",
            "    X = check_array(X, **check_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2533,) + inhomogeneous part.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END ...........C=10, gamma=1, kernel=rbf;, score=nan total time=  40.1s\n",
            "[CV 2/5] END ...........C=10, gamma=1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 3/5] END ...........C=10, gamma=1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 4/5] END ...........C=10, gamma=1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 5/5] END ...........C=10, gamma=1, kernel=rbf;, score=nan total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 651, in score\n",
            "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 791, in predict\n",
            "    y = super().predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 414, in predict\n",
            "    X = self._validate_for_predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 592, in _validate_for_predict\n",
            "    X = self._validate_data(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 566, in _validate_data\n",
            "    X = check_array(X, **check_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2533,) + inhomogeneous part.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END ........C=10, gamma=1, kernel=linear;, score=nan total time=31.7min\n",
            "[CV 2/5] END ........C=10, gamma=1, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 3/5] END ........C=10, gamma=1, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 4/5] END ........C=10, gamma=1, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 5/5] END ........C=10, gamma=1, kernel=linear;, score=nan total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 651, in score\n",
            "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 791, in predict\n",
            "    y = super().predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 414, in predict\n",
            "    X = self._validate_for_predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 592, in _validate_for_predict\n",
            "    X = self._validate_data(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 566, in _validate_data\n",
            "    X = check_array(X, **check_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2533,) + inhomogeneous part.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END .........C=10, gamma=0.1, kernel=rbf;, score=nan total time=  38.0s\n",
            "[CV 2/5] END .........C=10, gamma=0.1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 3/5] END .........C=10, gamma=0.1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 4/5] END .........C=10, gamma=0.1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 5/5] END .........C=10, gamma=0.1, kernel=rbf;, score=nan total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 651, in score\n",
            "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 791, in predict\n",
            "    y = super().predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 414, in predict\n",
            "    X = self._validate_for_predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 592, in _validate_for_predict\n",
            "    X = self._validate_data(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 566, in _validate_data\n",
            "    X = check_array(X, **check_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2533,) + inhomogeneous part.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END ......C=10, gamma=0.1, kernel=linear;, score=nan total time=31.6min\n",
            "[CV 2/5] END ......C=10, gamma=0.1, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 3/5] END ......C=10, gamma=0.1, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 4/5] END ......C=10, gamma=0.1, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 5/5] END ......C=10, gamma=0.1, kernel=linear;, score=nan total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 651, in score\n",
            "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 791, in predict\n",
            "    y = super().predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 414, in predict\n",
            "    X = self._validate_for_predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 592, in _validate_for_predict\n",
            "    X = self._validate_data(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 566, in _validate_data\n",
            "    X = check_array(X, **check_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2533,) + inhomogeneous part.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END ........C=10, gamma=0.01, kernel=rbf;, score=nan total time=  36.9s\n",
            "[CV 2/5] END ........C=10, gamma=0.01, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 3/5] END ........C=10, gamma=0.01, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 4/5] END ........C=10, gamma=0.01, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 5/5] END ........C=10, gamma=0.01, kernel=rbf;, score=nan total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 651, in score\n",
            "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 791, in predict\n",
            "    y = super().predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 414, in predict\n",
            "    X = self._validate_for_predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 592, in _validate_for_predict\n",
            "    X = self._validate_data(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 566, in _validate_data\n",
            "    X = check_array(X, **check_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2533,) + inhomogeneous part.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END .....C=10, gamma=0.01, kernel=linear;, score=nan total time=31.5min\n",
            "[CV 2/5] END .....C=10, gamma=0.01, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 3/5] END .....C=10, gamma=0.01, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 4/5] END .....C=10, gamma=0.01, kernel=linear;, score=nan total time=   0.0s\n",
            "[CV 5/5] END .....C=10, gamma=0.01, kernel=linear;, score=nan total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 651, in score\n",
            "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 791, in predict\n",
            "    y = super().predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 414, in predict\n",
            "    X = self._validate_for_predict(X)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\", line 592, in _validate_for_predict\n",
            "    X = self._validate_data(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 566, in _validate_data\n",
            "    X = check_array(X, **check_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2533,) + inhomogeneous part.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END ..........C=100, gamma=1, kernel=rbf;, score=nan total time=  39.9s\n",
            "[CV 2/5] END ..........C=100, gamma=1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 3/5] END ..........C=100, gamma=1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 4/5] END ..........C=100, gamma=1, kernel=rbf;, score=nan total time=   0.0s\n",
            "[CV 5/5] END ..........C=100, gamma=1, kernel=rbf;, score=nan total time=   0.0s\n"
          ]
        }
      ],
      "source": [
        "def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "X = [tokenize_and_lemmatize(text) for text in documents]\n",
        "\n",
        "def document_vector(doc):\n",
        "    return np.mean([token.vector for token in doc], axis=0)\n",
        "\n",
        "X = [document_vector(nlp(text)) for text in documents]\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [1, 10, 100], \n",
        "                  'gamma': [1, 0.1, 0.01],\n",
        "                  'kernel': ['rbf', 'linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCG7-T1DoyEC"
      },
      "source": [
        "# 1. Gridsearch LGBM mit FLAML:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHLcjHGjQCiH"
      },
      "source": [
        "CountVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4MhmXP9CBSb",
        "outputId": "a9e4487a-3330-41c6-95e1-9b0a36cc16f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[flaml.automl: 12-26 11:53:36] {2599} INFO - task = classification\n",
            "INFO:flaml.automl:task = classification\n",
            "[flaml.automl: 12-26 11:53:36] {2601} INFO - Data split method: stratified\n",
            "INFO:flaml.automl:Data split method: stratified\n",
            "[flaml.automl: 12-26 11:53:36] {2604} INFO - Evaluation method: holdout\n",
            "INFO:flaml.automl:Evaluation method: holdout\n",
            "[flaml.automl: 12-26 11:53:37] {2726} INFO - Minimizing error metric: 1-accuracy\n",
            "INFO:flaml.automl:Minimizing error metric: 1-accuracy\n",
            "[flaml.automl: 12-26 11:53:37] {2870} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
            "INFO:flaml.automl:List of ML learners in AutoML Run: ['lgbm']\n",
            "[flaml.automl: 12-26 11:53:37] {3166} INFO - iteration 0, current learner lgbm\n",
            "INFO:flaml.automl:iteration 0, current learner lgbm\n",
            "[flaml.automl: 12-26 11:53:39] {3296} INFO - Estimated sufficient time budget=22004s. Estimated necessary time budget=22s.\n",
            "INFO:flaml.automl:Estimated sufficient time budget=22004s. Estimated necessary time budget=22s.\n",
            "[flaml.automl: 12-26 11:53:39] {3343} INFO -  at 2.4s,\testimator lgbm's best error=0.7302,\tbest estimator lgbm's best error=0.7302\n",
            "INFO:flaml.automl: at 2.4s,\testimator lgbm's best error=0.7302,\tbest estimator lgbm's best error=0.7302\n",
            "[flaml.automl: 12-26 11:53:39] {3166} INFO - iteration 1, current learner lgbm\n",
            "INFO:flaml.automl:iteration 1, current learner lgbm\n",
            "[flaml.automl: 12-26 11:53:40] {3343} INFO -  at 4.0s,\testimator lgbm's best error=0.7302,\tbest estimator lgbm's best error=0.7302\n",
            "INFO:flaml.automl: at 4.0s,\testimator lgbm's best error=0.7302,\tbest estimator lgbm's best error=0.7302\n",
            "[flaml.automl: 12-26 11:53:40] {3166} INFO - iteration 2, current learner lgbm\n",
            "INFO:flaml.automl:iteration 2, current learner lgbm\n",
            "[flaml.automl: 12-26 11:53:43] {3343} INFO -  at 7.1s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "INFO:flaml.automl: at 7.1s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl: 12-26 11:53:44] {3166} INFO - iteration 3, current learner lgbm\n",
            "INFO:flaml.automl:iteration 3, current learner lgbm\n",
            "[flaml.automl: 12-26 11:53:50] {3343} INFO -  at 13.6s,\testimator lgbm's best error=0.6399,\tbest estimator lgbm's best error=0.6399\n",
            "INFO:flaml.automl: at 13.6s,\testimator lgbm's best error=0.6399,\tbest estimator lgbm's best error=0.6399\n",
            "[flaml.automl: 12-26 11:53:50] {3166} INFO - iteration 4, current learner lgbm\n",
            "INFO:flaml.automl:iteration 4, current learner lgbm\n",
            "[flaml.automl: 12-26 11:53:53] {3343} INFO -  at 16.8s,\testimator lgbm's best error=0.6399,\tbest estimator lgbm's best error=0.6399\n",
            "INFO:flaml.automl: at 16.8s,\testimator lgbm's best error=0.6399,\tbest estimator lgbm's best error=0.6399\n",
            "[flaml.automl: 12-26 11:53:53] {3166} INFO - iteration 5, current learner lgbm\n",
            "INFO:flaml.automl:iteration 5, current learner lgbm\n",
            "[flaml.automl: 12-26 11:54:00] {3343} INFO -  at 23.2s,\testimator lgbm's best error=0.6399,\tbest estimator lgbm's best error=0.6399\n",
            "INFO:flaml.automl: at 23.2s,\testimator lgbm's best error=0.6399,\tbest estimator lgbm's best error=0.6399\n",
            "[flaml.automl: 12-26 11:54:00] {3166} INFO - iteration 6, current learner lgbm\n",
            "INFO:flaml.automl:iteration 6, current learner lgbm\n",
            "[flaml.automl: 12-26 11:54:03] {3343} INFO -  at 26.5s,\testimator lgbm's best error=0.6399,\tbest estimator lgbm's best error=0.6399\n",
            "INFO:flaml.automl: at 26.5s,\testimator lgbm's best error=0.6399,\tbest estimator lgbm's best error=0.6399\n",
            "[flaml.automl: 12-26 11:54:03] {3166} INFO - iteration 7, current learner lgbm\n",
            "INFO:flaml.automl:iteration 7, current learner lgbm\n",
            "[flaml.automl: 12-26 11:54:06] {3343} INFO -  at 29.7s,\testimator lgbm's best error=0.6399,\tbest estimator lgbm's best error=0.6399\n",
            "INFO:flaml.automl: at 29.7s,\testimator lgbm's best error=0.6399,\tbest estimator lgbm's best error=0.6399\n",
            "[flaml.automl: 12-26 11:54:06] {3166} INFO - iteration 8, current learner lgbm\n",
            "INFO:flaml.automl:iteration 8, current learner lgbm\n",
            "[flaml.automl: 12-26 11:54:13] {3343} INFO -  at 37.1s,\testimator lgbm's best error=0.5953,\tbest estimator lgbm's best error=0.5953\n",
            "INFO:flaml.automl: at 37.1s,\testimator lgbm's best error=0.5953,\tbest estimator lgbm's best error=0.5953\n",
            "[flaml.automl: 12-26 11:54:13] {3166} INFO - iteration 9, current learner lgbm\n",
            "INFO:flaml.automl:iteration 9, current learner lgbm\n",
            "[flaml.automl: 12-26 11:54:18] {3343} INFO -  at 41.7s,\testimator lgbm's best error=0.5953,\tbest estimator lgbm's best error=0.5953\n",
            "INFO:flaml.automl: at 41.7s,\testimator lgbm's best error=0.5953,\tbest estimator lgbm's best error=0.5953\n",
            "[flaml.automl: 12-26 11:54:18] {3166} INFO - iteration 10, current learner lgbm\n",
            "INFO:flaml.automl:iteration 10, current learner lgbm\n",
            "[flaml.automl: 12-26 11:54:55] {3343} INFO -  at 78.6s,\testimator lgbm's best error=0.4882,\tbest estimator lgbm's best error=0.4882\n",
            "INFO:flaml.automl: at 78.6s,\testimator lgbm's best error=0.4882,\tbest estimator lgbm's best error=0.4882\n",
            "[flaml.automl: 12-26 11:54:55] {3166} INFO - iteration 11, current learner lgbm\n",
            "INFO:flaml.automl:iteration 11, current learner lgbm\n",
            "[flaml.automl: 12-26 11:55:25] {3343} INFO -  at 108.7s,\testimator lgbm's best error=0.4882,\tbest estimator lgbm's best error=0.4882\n",
            "INFO:flaml.automl: at 108.7s,\testimator lgbm's best error=0.4882,\tbest estimator lgbm's best error=0.4882\n",
            "[flaml.automl: 12-26 11:55:25] {3166} INFO - iteration 12, current learner lgbm\n",
            "INFO:flaml.automl:iteration 12, current learner lgbm\n",
            "[flaml.automl: 12-26 11:55:51] {3343} INFO -  at 134.8s,\testimator lgbm's best error=0.4882,\tbest estimator lgbm's best error=0.4882\n",
            "INFO:flaml.automl: at 134.8s,\testimator lgbm's best error=0.4882,\tbest estimator lgbm's best error=0.4882\n",
            "[flaml.automl: 12-26 11:55:51] {3166} INFO - iteration 13, current learner lgbm\n",
            "INFO:flaml.automl:iteration 13, current learner lgbm\n",
            "[flaml.automl: 12-26 11:56:15] {3343} INFO -  at 158.1s,\testimator lgbm's best error=0.4882,\tbest estimator lgbm's best error=0.4882\n",
            "INFO:flaml.automl: at 158.1s,\testimator lgbm's best error=0.4882,\tbest estimator lgbm's best error=0.4882\n",
            "[flaml.automl: 12-26 11:56:15] {3166} INFO - iteration 14, current learner lgbm\n",
            "INFO:flaml.automl:iteration 14, current learner lgbm\n",
            "[flaml.automl: 12-26 11:57:11] {3343} INFO -  at 215.0s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "INFO:flaml.automl: at 215.0s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "[flaml.automl: 12-26 11:57:11] {3166} INFO - iteration 15, current learner lgbm\n",
            "INFO:flaml.automl:iteration 15, current learner lgbm\n",
            "[flaml.automl: 12-26 11:58:19] {3343} INFO -  at 282.4s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "INFO:flaml.automl: at 282.4s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "[flaml.automl: 12-26 11:58:19] {3166} INFO - iteration 16, current learner lgbm\n",
            "INFO:flaml.automl:iteration 16, current learner lgbm\n",
            "[flaml.automl: 12-26 11:59:04] {3343} INFO -  at 327.8s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "INFO:flaml.automl: at 327.8s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "[flaml.automl: 12-26 11:59:04] {3166} INFO - iteration 17, current learner lgbm\n",
            "INFO:flaml.automl:iteration 17, current learner lgbm\n",
            "[flaml.automl: 12-26 12:00:35] {3343} INFO -  at 418.9s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "INFO:flaml.automl: at 418.9s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "[flaml.automl: 12-26 12:00:35] {3166} INFO - iteration 18, current learner lgbm\n",
            "INFO:flaml.automl:iteration 18, current learner lgbm\n",
            "[flaml.automl: 12-26 12:01:27] {3343} INFO -  at 470.6s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "INFO:flaml.automl: at 470.6s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "[flaml.automl: 12-26 12:01:27] {3166} INFO - iteration 19, current learner lgbm\n",
            "INFO:flaml.automl:iteration 19, current learner lgbm\n",
            "[flaml.automl: 12-26 12:01:47] {3343} INFO -  at 490.3s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "INFO:flaml.automl: at 490.3s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "[flaml.automl: 12-26 12:01:47] {3166} INFO - iteration 20, current learner lgbm\n",
            "INFO:flaml.automl:iteration 20, current learner lgbm\n",
            "[flaml.automl: 12-26 12:06:24] {3343} INFO -  at 767.9s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "INFO:flaml.automl: at 767.9s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "[flaml.automl: 12-26 12:06:24] {3166} INFO - iteration 21, current learner lgbm\n",
            "INFO:flaml.automl:iteration 21, current learner lgbm\n",
            "[flaml.automl: 12-26 12:11:38] {3343} INFO -  at 1081.2s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 1081.2s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-26 12:11:38] {3166} INFO - iteration 22, current learner lgbm\n",
            "INFO:flaml.automl:iteration 22, current learner lgbm\n",
            "[flaml.automl: 12-26 12:15:52] {3343} INFO -  at 1335.9s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 1335.9s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-26 12:15:52] {3166} INFO - iteration 23, current learner lgbm\n",
            "INFO:flaml.automl:iteration 23, current learner lgbm\n",
            "[flaml.automl: 12-26 12:19:39] {3343} INFO -  at 1562.5s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 1562.5s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-26 12:19:39] {3166} INFO - iteration 24, current learner lgbm\n",
            "INFO:flaml.automl:iteration 24, current learner lgbm\n",
            "[flaml.automl: 12-26 12:26:15] {3343} INFO -  at 1958.5s,\testimator lgbm's best error=0.4573,\tbest estimator lgbm's best error=0.4573\n",
            "INFO:flaml.automl: at 1958.5s,\testimator lgbm's best error=0.4573,\tbest estimator lgbm's best error=0.4573\n",
            "[flaml.automl: 12-26 12:26:15] {3166} INFO - iteration 25, current learner lgbm\n",
            "INFO:flaml.automl:iteration 25, current learner lgbm\n",
            "[flaml.automl: 12-26 12:35:52] {3343} INFO -  at 2535.5s,\testimator lgbm's best error=0.4573,\tbest estimator lgbm's best error=0.4573\n",
            "INFO:flaml.automl: at 2535.5s,\testimator lgbm's best error=0.4573,\tbest estimator lgbm's best error=0.4573\n",
            "[flaml.automl: 12-26 12:35:52] {3166} INFO - iteration 26, current learner lgbm\n",
            "INFO:flaml.automl:iteration 26, current learner lgbm\n",
            "[flaml.automl: 12-26 12:41:06] {3343} INFO -  at 2849.5s,\testimator lgbm's best error=0.4573,\tbest estimator lgbm's best error=0.4573\n",
            "INFO:flaml.automl: at 2849.5s,\testimator lgbm's best error=0.4573,\tbest estimator lgbm's best error=0.4573\n",
            "[flaml.automl: 12-26 12:41:06] {3166} INFO - iteration 27, current learner lgbm\n",
            "INFO:flaml.automl:iteration 27, current learner lgbm\n",
            "[flaml.automl: 12-26 12:52:23] {3343} INFO -  at 3527.0s,\testimator lgbm's best error=0.4573,\tbest estimator lgbm's best error=0.4573\n",
            "INFO:flaml.automl: at 3527.0s,\testimator lgbm's best error=0.4573,\tbest estimator lgbm's best error=0.4573\n",
            "[flaml.automl: 12-26 13:00:00] {3602} INFO - retrain lgbm for 456.2s\n",
            "INFO:flaml.automl:retrain lgbm for 456.2s\n",
            "[flaml.automl: 12-26 13:00:00] {3609} INFO - retrained model: LGBMClassifier(colsample_bytree=0.39039795455449644,\n",
            "               learning_rate=0.08838576141784195, max_bin=511,\n",
            "               min_child_samples=3, n_estimators=200, num_leaves=184,\n",
            "               reg_alpha=0.014844095616079196, reg_lambda=0.05525097930389173,\n",
            "               verbose=-1)\n",
            "INFO:flaml.automl:retrained model: LGBMClassifier(colsample_bytree=0.39039795455449644,\n",
            "               learning_rate=0.08838576141784195, max_bin=511,\n",
            "               min_child_samples=3, n_estimators=200, num_leaves=184,\n",
            "               reg_alpha=0.014844095616079196, reg_lambda=0.05525097930389173,\n",
            "               verbose=-1)\n",
            "[flaml.automl: 12-26 13:00:00] {2901} INFO - fit succeeded\n",
            "INFO:flaml.automl:fit succeeded\n",
            "[flaml.automl: 12-26 13:00:00] {2902} INFO - Time taken to find the best model: 1958.51203083992\n",
            "INFO:flaml.automl:Time taken to find the best model: 1958.51203083992\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparmeter config: {'n_estimators': 200, 'num_leaves': 184, 'min_child_samples': 3, 'learning_rate': 0.08838576141784195, 'log_max_bin': 9, 'colsample_bytree': 0.39039795455449644, 'reg_alpha': 0.014844095616079196, 'reg_lambda': 0.05525097930389173}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrQT3G_MQF8E"
      },
      "source": [
        "TfidfVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gf9Fgxr155Q",
        "outputId": "84d7f3cc-3c48-4641-fdf9-10816c27de73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[flaml.automl: 12-29 10:24:53] {2599} INFO - task = classification\n",
            "INFO:flaml.automl:task = classification\n",
            "[flaml.automl: 12-29 10:24:53] {2601} INFO - Data split method: stratified\n",
            "INFO:flaml.automl:Data split method: stratified\n",
            "[flaml.automl: 12-29 10:24:53] {2604} INFO - Evaluation method: holdout\n",
            "INFO:flaml.automl:Evaluation method: holdout\n",
            "[flaml.automl: 12-29 10:24:54] {2726} INFO - Minimizing error metric: 1-accuracy\n",
            "INFO:flaml.automl:Minimizing error metric: 1-accuracy\n",
            "[flaml.automl: 12-29 10:24:54] {2870} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
            "INFO:flaml.automl:List of ML learners in AutoML Run: ['lgbm']\n",
            "[flaml.automl: 12-29 10:24:54] {3166} INFO - iteration 0, current learner lgbm\n",
            "INFO:flaml.automl:iteration 0, current learner lgbm\n",
            "[flaml.automl: 12-29 10:24:57] {3296} INFO - Estimated sufficient time budget=35974s. Estimated necessary time budget=36s.\n",
            "INFO:flaml.automl:Estimated sufficient time budget=35974s. Estimated necessary time budget=36s.\n",
            "[flaml.automl: 12-29 10:24:57] {3343} INFO -  at 3.7s,\testimator lgbm's best error=0.7172,\tbest estimator lgbm's best error=0.7172\n",
            "INFO:flaml.automl: at 3.7s,\testimator lgbm's best error=0.7172,\tbest estimator lgbm's best error=0.7172\n",
            "[flaml.automl: 12-29 10:24:57] {3166} INFO - iteration 1, current learner lgbm\n",
            "INFO:flaml.automl:iteration 1, current learner lgbm\n",
            "[flaml.automl: 12-29 10:25:00] {3343} INFO -  at 6.8s,\testimator lgbm's best error=0.7172,\tbest estimator lgbm's best error=0.7172\n",
            "INFO:flaml.automl: at 6.8s,\testimator lgbm's best error=0.7172,\tbest estimator lgbm's best error=0.7172\n",
            "[flaml.automl: 12-29 10:25:00] {3166} INFO - iteration 2, current learner lgbm\n",
            "INFO:flaml.automl:iteration 2, current learner lgbm\n",
            "[flaml.automl: 12-29 10:25:04] {3343} INFO -  at 10.8s,\testimator lgbm's best error=0.6943,\tbest estimator lgbm's best error=0.6943\n",
            "INFO:flaml.automl: at 10.8s,\testimator lgbm's best error=0.6943,\tbest estimator lgbm's best error=0.6943\n",
            "[flaml.automl: 12-29 10:25:04] {3166} INFO - iteration 3, current learner lgbm\n",
            "INFO:flaml.automl:iteration 3, current learner lgbm\n",
            "[flaml.automl: 12-29 10:25:13] {3343} INFO -  at 19.1s,\testimator lgbm's best error=0.6405,\tbest estimator lgbm's best error=0.6405\n",
            "INFO:flaml.automl: at 19.1s,\testimator lgbm's best error=0.6405,\tbest estimator lgbm's best error=0.6405\n",
            "[flaml.automl: 12-29 10:25:13] {3166} INFO - iteration 4, current learner lgbm\n",
            "INFO:flaml.automl:iteration 4, current learner lgbm\n",
            "[flaml.automl: 12-29 10:25:17] {3343} INFO -  at 23.6s,\testimator lgbm's best error=0.6405,\tbest estimator lgbm's best error=0.6405\n",
            "INFO:flaml.automl: at 23.6s,\testimator lgbm's best error=0.6405,\tbest estimator lgbm's best error=0.6405\n",
            "[flaml.automl: 12-29 10:25:17] {3166} INFO - iteration 5, current learner lgbm\n",
            "INFO:flaml.automl:iteration 5, current learner lgbm\n",
            "[flaml.automl: 12-29 10:25:28] {3343} INFO -  at 35.0s,\testimator lgbm's best error=0.6405,\tbest estimator lgbm's best error=0.6405\n",
            "INFO:flaml.automl: at 35.0s,\testimator lgbm's best error=0.6405,\tbest estimator lgbm's best error=0.6405\n",
            "[flaml.automl: 12-29 10:25:28] {3166} INFO - iteration 6, current learner lgbm\n",
            "INFO:flaml.automl:iteration 6, current learner lgbm\n",
            "[flaml.automl: 12-29 10:25:40] {3343} INFO -  at 46.3s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "INFO:flaml.automl: at 46.3s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "[flaml.automl: 12-29 10:25:40] {3166} INFO - iteration 7, current learner lgbm\n",
            "INFO:flaml.automl:iteration 7, current learner lgbm\n",
            "[flaml.automl: 12-29 10:25:44] {3343} INFO -  at 50.9s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "INFO:flaml.automl: at 50.9s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "[flaml.automl: 12-29 10:25:44] {3166} INFO - iteration 8, current learner lgbm\n",
            "INFO:flaml.automl:iteration 8, current learner lgbm\n",
            "[flaml.automl: 12-29 10:26:04] {3343} INFO -  at 70.5s,\testimator lgbm's best error=0.5761,\tbest estimator lgbm's best error=0.5761\n",
            "INFO:flaml.automl: at 70.5s,\testimator lgbm's best error=0.5761,\tbest estimator lgbm's best error=0.5761\n",
            "[flaml.automl: 12-29 10:26:04] {3166} INFO - iteration 9, current learner lgbm\n",
            "INFO:flaml.automl:iteration 9, current learner lgbm\n",
            "[flaml.automl: 12-29 10:26:12] {3343} INFO -  at 79.1s,\testimator lgbm's best error=0.5761,\tbest estimator lgbm's best error=0.5761\n",
            "INFO:flaml.automl: at 79.1s,\testimator lgbm's best error=0.5761,\tbest estimator lgbm's best error=0.5761\n",
            "[flaml.automl: 12-29 10:26:13] {3166} INFO - iteration 10, current learner lgbm\n",
            "INFO:flaml.automl:iteration 10, current learner lgbm\n",
            "[flaml.automl: 12-29 10:29:04] {3343} INFO -  at 250.1s,\testimator lgbm's best error=0.4870,\tbest estimator lgbm's best error=0.4870\n",
            "INFO:flaml.automl: at 250.1s,\testimator lgbm's best error=0.4870,\tbest estimator lgbm's best error=0.4870\n",
            "[flaml.automl: 12-29 10:29:04] {3166} INFO - iteration 11, current learner lgbm\n",
            "INFO:flaml.automl:iteration 11, current learner lgbm\n",
            "[flaml.automl: 12-29 10:33:12] {3343} INFO -  at 498.3s,\testimator lgbm's best error=0.4870,\tbest estimator lgbm's best error=0.4870\n",
            "INFO:flaml.automl: at 498.3s,\testimator lgbm's best error=0.4870,\tbest estimator lgbm's best error=0.4870\n",
            "[flaml.automl: 12-29 10:33:12] {3166} INFO - iteration 12, current learner lgbm\n",
            "INFO:flaml.automl:iteration 12, current learner lgbm\n",
            "[flaml.automl: 12-29 10:35:02] {3343} INFO -  at 608.3s,\testimator lgbm's best error=0.4870,\tbest estimator lgbm's best error=0.4870\n",
            "INFO:flaml.automl: at 608.3s,\testimator lgbm's best error=0.4870,\tbest estimator lgbm's best error=0.4870\n",
            "[flaml.automl: 12-29 10:35:02] {3166} INFO - iteration 13, current learner lgbm\n",
            "INFO:flaml.automl:iteration 13, current learner lgbm\n",
            "[flaml.automl: 12-29 10:36:24] {3343} INFO -  at 691.1s,\testimator lgbm's best error=0.4870,\tbest estimator lgbm's best error=0.4870\n",
            "INFO:flaml.automl: at 691.1s,\testimator lgbm's best error=0.4870,\tbest estimator lgbm's best error=0.4870\n",
            "[flaml.automl: 12-29 10:36:24] {3166} INFO - iteration 14, current learner lgbm\n",
            "INFO:flaml.automl:iteration 14, current learner lgbm\n",
            "[flaml.automl: 12-29 10:40:43] {3343} INFO -  at 949.7s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "INFO:flaml.automl: at 949.7s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "[flaml.automl: 12-29 10:40:43] {3166} INFO - iteration 15, current learner lgbm\n",
            "INFO:flaml.automl:iteration 15, current learner lgbm\n",
            "[flaml.automl: 12-29 10:49:20] {3343} INFO -  at 1466.7s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "INFO:flaml.automl: at 1466.7s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "[flaml.automl: 12-29 10:49:20] {3166} INFO - iteration 16, current learner lgbm\n",
            "INFO:flaml.automl:iteration 16, current learner lgbm\n",
            "[flaml.automl: 12-29 10:50:55] {3343} INFO -  at 1561.8s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "INFO:flaml.automl: at 1561.8s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "[flaml.automl: 12-29 10:50:55] {3166} INFO - iteration 17, current learner lgbm\n",
            "INFO:flaml.automl:iteration 17, current learner lgbm\n",
            "[flaml.automl: 12-29 10:58:44] {3343} INFO -  at 2030.2s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "INFO:flaml.automl: at 2030.2s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "[flaml.automl: 12-29 10:58:44] {3166} INFO - iteration 18, current learner lgbm\n",
            "INFO:flaml.automl:iteration 18, current learner lgbm\n",
            "[flaml.automl: 12-29 11:00:25] {3343} INFO -  at 2131.4s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "INFO:flaml.automl: at 2131.4s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "[flaml.automl: 12-29 11:00:25] {3166} INFO - iteration 19, current learner lgbm\n",
            "INFO:flaml.automl:iteration 19, current learner lgbm\n",
            "[flaml.automl: 12-29 11:02:11] {3343} INFO -  at 2238.1s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "INFO:flaml.automl: at 2238.1s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "[flaml.automl: 12-29 11:02:11] {3166} INFO - iteration 20, current learner lgbm\n",
            "INFO:flaml.automl:iteration 20, current learner lgbm\n",
            "[flaml.automl: 12-29 11:12:02] {3343} INFO -  at 2828.4s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "INFO:flaml.automl: at 2828.4s,\testimator lgbm's best error=0.4728,\tbest estimator lgbm's best error=0.4728\n",
            "[flaml.automl: 12-29 11:12:02] {3166} INFO - iteration 21, current learner lgbm\n",
            "INFO:flaml.automl:iteration 21, current learner lgbm\n",
            "[flaml.automl: 12-29 11:19:07] {3343} INFO -  at 3254.0s,\testimator lgbm's best error=0.4709,\tbest estimator lgbm's best error=0.4709\n",
            "INFO:flaml.automl: at 3254.0s,\testimator lgbm's best error=0.4709,\tbest estimator lgbm's best error=0.4709\n",
            "[flaml.automl: 12-29 11:19:07] {3166} INFO - iteration 22, current learner lgbm\n",
            "INFO:flaml.automl:iteration 22, current learner lgbm\n",
            "[flaml.automl: 12-29 11:23:45] {3343} INFO -  at 3531.6s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "INFO:flaml.automl: at 3531.6s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "[flaml.automl: 12-29 11:23:45] {3166} INFO - iteration 23, current learner lgbm\n",
            "INFO:flaml.automl:iteration 23, current learner lgbm\n",
            "[flaml.automl: 12-29 11:27:30] {3343} INFO -  at 3756.7s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "INFO:flaml.automl: at 3756.7s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "[flaml.automl: 12-29 11:27:30] {3166} INFO - iteration 24, current learner lgbm\n",
            "INFO:flaml.automl:iteration 24, current learner lgbm\n",
            "[flaml.automl: 12-29 11:31:58] {3343} INFO -  at 4024.7s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "INFO:flaml.automl: at 4024.7s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "[flaml.automl: 12-29 11:31:58] {3166} INFO - iteration 25, current learner lgbm\n",
            "INFO:flaml.automl:iteration 25, current learner lgbm\n",
            "[flaml.automl: 12-29 11:41:18] {3343} INFO -  at 4584.3s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "INFO:flaml.automl: at 4584.3s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "[flaml.automl: 12-29 11:41:18] {3166} INFO - iteration 26, current learner lgbm\n",
            "INFO:flaml.automl:iteration 26, current learner lgbm\n",
            "[flaml.automl: 12-29 11:42:40] {3343} INFO -  at 4666.6s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "INFO:flaml.automl: at 4666.6s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "[flaml.automl: 12-29 11:42:40] {3166} INFO - iteration 27, current learner lgbm\n",
            "INFO:flaml.automl:iteration 27, current learner lgbm\n",
            "[flaml.automl: 12-29 11:51:20] {3343} INFO -  at 5186.6s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "INFO:flaml.automl: at 5186.6s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "[flaml.automl: 12-29 11:51:20] {3166} INFO - iteration 28, current learner lgbm\n",
            "INFO:flaml.automl:iteration 28, current learner lgbm\n",
            "[flaml.automl: 12-29 11:52:50] {3343} INFO -  at 5276.4s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "INFO:flaml.automl: at 5276.4s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "[flaml.automl: 12-29 11:52:50] {3166} INFO - iteration 29, current learner lgbm\n",
            "INFO:flaml.automl:iteration 29, current learner lgbm\n",
            "[flaml.automl: 12-29 11:53:51] {3343} INFO -  at 5337.4s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "INFO:flaml.automl: at 5337.4s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "[flaml.automl: 12-29 11:53:51] {3166} INFO - iteration 30, current learner lgbm\n",
            "INFO:flaml.automl:iteration 30, current learner lgbm\n",
            "[flaml.automl: 12-29 12:03:50] {3343} INFO -  at 5936.5s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "INFO:flaml.automl: at 5936.5s,\testimator lgbm's best error=0.4641,\tbest estimator lgbm's best error=0.4641\n",
            "[flaml.automl: 12-29 12:08:56] {3602} INFO - retrain lgbm for 306.4s\n",
            "INFO:flaml.automl:retrain lgbm for 306.4s\n",
            "[flaml.automl: 12-29 12:08:56] {3609} INFO - retrained model: LGBMClassifier(colsample_bytree=0.7663773657187746,\n",
            "               learning_rate=0.085537978248575, max_bin=511,\n",
            "               min_child_samples=12, n_estimators=67, num_leaves=69,\n",
            "               reg_alpha=0.006958608037974516, reg_lambda=0.4683303882185501,\n",
            "               verbose=-1)\n",
            "INFO:flaml.automl:retrained model: LGBMClassifier(colsample_bytree=0.7663773657187746,\n",
            "               learning_rate=0.085537978248575, max_bin=511,\n",
            "               min_child_samples=12, n_estimators=67, num_leaves=69,\n",
            "               reg_alpha=0.006958608037974516, reg_lambda=0.4683303882185501,\n",
            "               verbose=-1)\n",
            "[flaml.automl: 12-29 12:08:56] {2901} INFO - fit succeeded\n",
            "INFO:flaml.automl:fit succeeded\n",
            "[flaml.automl: 12-29 12:08:56] {2902} INFO - Time taken to find the best model: 3531.633945941925\n",
            "INFO:flaml.automl:Time taken to find the best model: 3531.633945941925\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparmeter config: {'n_estimators': 67, 'num_leaves': 69, 'min_child_samples': 12, 'learning_rate': 0.085537978248575, 'log_max_bin': 9, 'colsample_bytree': 0.7663773657187746, 'reg_alpha': 0.006958608037974516, 'reg_lambda': 0.4683303882185501}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=6000)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bww7_oRPQLcV"
      },
      "source": [
        "Spacy-Vektoren:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cliHcGEYCBQc"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "X = [tokenize_and_lemmatize(text) for text in documents]\n",
        "\n",
        "def document_vector(doc):\n",
        "    return np.mean([token.vector for token in doc], axis=0)\n",
        "X = [document_vector(nlp(text)) for text in documents]\n",
        "\n",
        "X = np.array([x.flatten() for x in X])\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PsyQj-_A0Ng"
      },
      "source": [
        "# Einstellen, welche Lyrik-Spalten verwendet werden sollen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5IseDGqBBJB"
      },
      "outputs": [],
      "source": [
        "# Laden Sie die Textdokumente und die Klassenlabels\n",
        "documents = df['stemmed_lyrics'].tolist() # Liste von Textdokumenten\n",
        "labels = df['Genre'] # Liste von Klassenlabels (eins pro Dokument)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wt8JjpTBajw"
      },
      "source": [
        "# 2. Grid-search SVM mit SKLEARN:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp7zcIimBaj2"
      },
      "source": [
        "WIE ICH IN ZUKUNFT DIE OPTIMALEN PARAMETER RAUSFINDE:\n",
        "Ich taste mich erstmal mit den jetztigen ran, schau mir die optimalen an, dann\n",
        "werde ich eine Reihe von Paramtern die ungefähr um den optimalen Parameter liegen testen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLlCQvHDBaj2"
      },
      "source": [
        "CountVektorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myh4Bj0IBaj2",
        "outputId": "db6b1b2d-1b55-41b4-af40-004e3cfa544e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "[CV 1/5] END ...C=0.2, gamma=0.3, kernel=linear;, score=0.435 total time= 2.5min\n",
            "[CV 2/5] END ...C=0.2, gamma=0.3, kernel=linear;, score=0.441 total time= 2.5min\n",
            "[CV 3/5] END ...C=0.2, gamma=0.3, kernel=linear;, score=0.430 total time= 2.5min\n",
            "[CV 4/5] END ...C=0.2, gamma=0.3, kernel=linear;, score=0.437 total time= 2.7min\n",
            "[CV 5/5] END ...C=0.2, gamma=0.3, kernel=linear;, score=0.423 total time= 2.6min\n",
            "[CV 1/5] END ...C=0.2, gamma=0.5, kernel=linear;, score=0.435 total time= 2.5min\n",
            "[CV 2/5] END ...C=0.2, gamma=0.5, kernel=linear;, score=0.441 total time= 2.5min\n",
            "[CV 3/5] END ...C=0.2, gamma=0.5, kernel=linear;, score=0.430 total time= 2.5min\n",
            "[CV 4/5] END ...C=0.2, gamma=0.5, kernel=linear;, score=0.437 total time= 2.4min\n",
            "[CV 5/5] END ...C=0.2, gamma=0.5, kernel=linear;, score=0.423 total time= 2.5min\n",
            "[CV 1/5] END ..C=0.2, gamma=0.75, kernel=linear;, score=0.435 total time= 2.5min\n",
            "[CV 2/5] END ..C=0.2, gamma=0.75, kernel=linear;, score=0.441 total time= 2.4min\n",
            "[CV 3/5] END ..C=0.2, gamma=0.75, kernel=linear;, score=0.430 total time= 2.5min\n",
            "[CV 4/5] END ..C=0.2, gamma=0.75, kernel=linear;, score=0.437 total time= 2.5min\n",
            "[CV 5/5] END ..C=0.2, gamma=0.75, kernel=linear;, score=0.423 total time= 2.5min\n",
            "[CV 1/5] END ...C=0.2, gamma=0.8, kernel=linear;, score=0.435 total time= 2.5min\n",
            "[CV 2/5] END ...C=0.2, gamma=0.8, kernel=linear;, score=0.441 total time= 2.4min\n",
            "[CV 3/5] END ...C=0.2, gamma=0.8, kernel=linear;, score=0.430 total time= 2.5min\n",
            "[CV 4/5] END ...C=0.2, gamma=0.8, kernel=linear;, score=0.437 total time= 2.4min\n",
            "[CV 5/5] END ...C=0.2, gamma=0.8, kernel=linear;, score=0.423 total time= 2.5min\n",
            "[CV 1/5] END ..C=0.35, gamma=0.3, kernel=linear;, score=0.428 total time= 2.4min\n",
            "[CV 2/5] END ..C=0.35, gamma=0.3, kernel=linear;, score=0.443 total time= 2.4min\n",
            "[CV 3/5] END ..C=0.35, gamma=0.3, kernel=linear;, score=0.428 total time= 2.4min\n",
            "[CV 4/5] END ..C=0.35, gamma=0.3, kernel=linear;, score=0.438 total time= 2.5min\n",
            "[CV 5/5] END ..C=0.35, gamma=0.3, kernel=linear;, score=0.417 total time= 2.5min\n",
            "[CV 1/5] END ..C=0.35, gamma=0.5, kernel=linear;, score=0.428 total time= 2.5min\n",
            "[CV 2/5] END ..C=0.35, gamma=0.5, kernel=linear;, score=0.443 total time= 2.5min\n",
            "[CV 3/5] END ..C=0.35, gamma=0.5, kernel=linear;, score=0.428 total time= 2.5min\n",
            "[CV 4/5] END ..C=0.35, gamma=0.5, kernel=linear;, score=0.438 total time= 2.5min\n",
            "[CV 5/5] END ..C=0.35, gamma=0.5, kernel=linear;, score=0.417 total time= 2.5min\n",
            "[CV 1/5] END .C=0.35, gamma=0.75, kernel=linear;, score=0.428 total time= 2.4min\n",
            "[CV 2/5] END .C=0.35, gamma=0.75, kernel=linear;, score=0.443 total time= 2.5min\n",
            "[CV 3/5] END .C=0.35, gamma=0.75, kernel=linear;, score=0.428 total time= 2.4min\n",
            "[CV 4/5] END .C=0.35, gamma=0.75, kernel=linear;, score=0.438 total time= 2.4min\n",
            "[CV 5/5] END .C=0.35, gamma=0.75, kernel=linear;, score=0.417 total time= 2.4min\n",
            "[CV 1/5] END ..C=0.35, gamma=0.8, kernel=linear;, score=0.428 total time= 2.4min\n",
            "[CV 2/5] END ..C=0.35, gamma=0.8, kernel=linear;, score=0.443 total time= 2.5min\n",
            "[CV 3/5] END ..C=0.35, gamma=0.8, kernel=linear;, score=0.428 total time= 2.4min\n",
            "[CV 4/5] END ..C=0.35, gamma=0.8, kernel=linear;, score=0.438 total time= 2.4min\n",
            "[CV 5/5] END ..C=0.35, gamma=0.8, kernel=linear;, score=0.417 total time= 2.4min\n",
            "[CV 1/5] END ...C=0.5, gamma=0.3, kernel=linear;, score=0.426 total time= 2.4min\n",
            "[CV 2/5] END ...C=0.5, gamma=0.3, kernel=linear;, score=0.439 total time= 2.4min\n",
            "[CV 3/5] END ...C=0.5, gamma=0.3, kernel=linear;, score=0.424 total time= 2.4min\n",
            "[CV 4/5] END ...C=0.5, gamma=0.3, kernel=linear;, score=0.441 total time= 2.5min\n",
            "[CV 5/5] END ...C=0.5, gamma=0.3, kernel=linear;, score=0.418 total time= 2.4min\n",
            "[CV 1/5] END ...C=0.5, gamma=0.5, kernel=linear;, score=0.426 total time= 2.5min\n",
            "[CV 2/5] END ...C=0.5, gamma=0.5, kernel=linear;, score=0.439 total time= 2.5min\n",
            "[CV 3/5] END ...C=0.5, gamma=0.5, kernel=linear;, score=0.424 total time= 2.4min\n",
            "[CV 4/5] END ...C=0.5, gamma=0.5, kernel=linear;, score=0.441 total time= 2.5min\n",
            "[CV 5/5] END ...C=0.5, gamma=0.5, kernel=linear;, score=0.418 total time= 2.4min\n",
            "[CV 1/5] END ..C=0.5, gamma=0.75, kernel=linear;, score=0.426 total time= 2.5min\n",
            "[CV 2/5] END ..C=0.5, gamma=0.75, kernel=linear;, score=0.439 total time= 2.4min\n",
            "[CV 3/5] END ..C=0.5, gamma=0.75, kernel=linear;, score=0.424 total time= 2.4min\n",
            "[CV 4/5] END ..C=0.5, gamma=0.75, kernel=linear;, score=0.441 total time= 2.5min\n",
            "[CV 5/5] END ..C=0.5, gamma=0.75, kernel=linear;, score=0.418 total time= 2.4min\n",
            "[CV 1/5] END ...C=0.5, gamma=0.8, kernel=linear;, score=0.426 total time= 2.4min\n",
            "[CV 2/5] END ...C=0.5, gamma=0.8, kernel=linear;, score=0.439 total time= 2.4min\n",
            "[CV 3/5] END ...C=0.5, gamma=0.8, kernel=linear;, score=0.424 total time= 2.5min\n",
            "[CV 4/5] END ...C=0.5, gamma=0.8, kernel=linear;, score=0.441 total time= 2.5min\n",
            "[CV 5/5] END ...C=0.5, gamma=0.8, kernel=linear;, score=0.418 total time= 2.4min\n",
            "[CV 1/5] END ...C=0.7, gamma=0.3, kernel=linear;, score=0.424 total time= 2.4min\n",
            "[CV 2/5] END ...C=0.7, gamma=0.3, kernel=linear;, score=0.437 total time= 2.4min\n",
            "[CV 3/5] END ...C=0.7, gamma=0.3, kernel=linear;, score=0.425 total time= 2.4min\n",
            "[CV 4/5] END ...C=0.7, gamma=0.3, kernel=linear;, score=0.442 total time= 2.5min\n",
            "[CV 5/5] END ...C=0.7, gamma=0.3, kernel=linear;, score=0.417 total time= 2.4min\n",
            "[CV 1/5] END ...C=0.7, gamma=0.5, kernel=linear;, score=0.424 total time= 2.4min\n",
            "[CV 2/5] END ...C=0.7, gamma=0.5, kernel=linear;, score=0.437 total time= 2.4min\n",
            "[CV 3/5] END ...C=0.7, gamma=0.5, kernel=linear;, score=0.425 total time= 2.4min\n",
            "[CV 4/5] END ...C=0.7, gamma=0.5, kernel=linear;, score=0.442 total time= 2.5min\n",
            "[CV 5/5] END ...C=0.7, gamma=0.5, kernel=linear;, score=0.417 total time= 2.4min\n",
            "[CV 1/5] END ..C=0.7, gamma=0.75, kernel=linear;, score=0.424 total time= 2.4min\n",
            "[CV 2/5] END ..C=0.7, gamma=0.75, kernel=linear;, score=0.437 total time= 2.5min\n",
            "[CV 3/5] END ..C=0.7, gamma=0.75, kernel=linear;, score=0.425 total time= 2.5min\n",
            "[CV 4/5] END ..C=0.7, gamma=0.75, kernel=linear;, score=0.442 total time= 2.5min\n",
            "[CV 5/5] END ..C=0.7, gamma=0.75, kernel=linear;, score=0.417 total time= 2.5min\n",
            "[CV 1/5] END ...C=0.7, gamma=0.8, kernel=linear;, score=0.424 total time= 2.5min\n",
            "[CV 2/5] END ...C=0.7, gamma=0.8, kernel=linear;, score=0.437 total time= 2.5min\n",
            "[CV 3/5] END ...C=0.7, gamma=0.8, kernel=linear;, score=0.425 total time= 2.5min\n",
            "[CV 4/5] END ...C=0.7, gamma=0.8, kernel=linear;, score=0.442 total time= 2.6min\n",
            "[CV 5/5] END ...C=0.7, gamma=0.8, kernel=linear;, score=0.417 total time= 2.5min\n",
            "{'C': 0.2, 'gamma': 0.3, 'kernel': 'linear'}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [0.2, 0.35, 0.5, 0.7], \n",
        "                  'gamma': [0.3, 0.5, 0.75, 0.8],\n",
        "                  'kernel': ['linear']}\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFZ1q0ATBaj3"
      },
      "source": [
        "SVM mit TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "lWVrjc-XBaj3",
        "outputId": "bea0d33b-f56d-4d52-b4b9-1cb21fe3e45c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-667a45a3aa9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem TfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Teilen Sie die Daten in Trainings- und Testdaten auf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [8, 10, 15, 50], \n",
        "                  'gamma': [0.9, 1, 1.2, 2],\n",
        "                  'kernel': ['rbf']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBV7e2C7Baj3"
      },
      "source": [
        "Spacy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j90rPuWUBaj3"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "X = [tokenize_and_lemmatize(text) for text in documents]\n",
        "\n",
        "def document_vector(doc):\n",
        "    return np.mean([token.vector for token in doc], axis=0)\n",
        "\n",
        "X = [document_vector(nlp(text)) for text in documents]\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [1, 10, 100], \n",
        "                  'gamma': [1, 0.1, 0.01],\n",
        "                  'kernel': ['rbf', 'linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_90fW4BB3_p"
      },
      "source": [
        "# 2. Gridsearch LGBM mit FLAML:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WARSiercB3_v"
      },
      "source": [
        "CountVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCnl2vdGB3_v",
        "outputId": "90bb0b6b-c841-490a-d7da-ed1a3cbd0e5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[flaml.automl: 12-26 15:25:15] {2599} INFO - task = classification\n",
            "INFO:flaml.automl:task = classification\n",
            "[flaml.automl: 12-26 15:25:15] {2601} INFO - Data split method: stratified\n",
            "INFO:flaml.automl:Data split method: stratified\n",
            "[flaml.automl: 12-26 15:25:15] {2604} INFO - Evaluation method: holdout\n",
            "INFO:flaml.automl:Evaluation method: holdout\n",
            "[flaml.automl: 12-26 15:25:15] {2726} INFO - Minimizing error metric: 1-accuracy\n",
            "INFO:flaml.automl:Minimizing error metric: 1-accuracy\n",
            "[flaml.automl: 12-26 15:25:15] {2870} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
            "INFO:flaml.automl:List of ML learners in AutoML Run: ['lgbm']\n",
            "[flaml.automl: 12-26 15:25:15] {3166} INFO - iteration 0, current learner lgbm\n",
            "INFO:flaml.automl:iteration 0, current learner lgbm\n",
            "[flaml.automl: 12-26 15:25:17] {3296} INFO - Estimated sufficient time budget=19236s. Estimated necessary time budget=19s.\n",
            "INFO:flaml.automl:Estimated sufficient time budget=19236s. Estimated necessary time budget=19s.\n",
            "[flaml.automl: 12-26 15:25:17] {3343} INFO -  at 2.1s,\testimator lgbm's best error=0.7191,\tbest estimator lgbm's best error=0.7191\n",
            "INFO:flaml.automl: at 2.1s,\testimator lgbm's best error=0.7191,\tbest estimator lgbm's best error=0.7191\n",
            "[flaml.automl: 12-26 15:25:17] {3166} INFO - iteration 1, current learner lgbm\n",
            "INFO:flaml.automl:iteration 1, current learner lgbm\n",
            "[flaml.automl: 12-26 15:25:18] {3343} INFO -  at 3.6s,\testimator lgbm's best error=0.7184,\tbest estimator lgbm's best error=0.7184\n",
            "INFO:flaml.automl: at 3.6s,\testimator lgbm's best error=0.7184,\tbest estimator lgbm's best error=0.7184\n",
            "[flaml.automl: 12-26 15:25:18] {3166} INFO - iteration 2, current learner lgbm\n",
            "INFO:flaml.automl:iteration 2, current learner lgbm\n",
            "[flaml.automl: 12-26 15:25:20] {3343} INFO -  at 5.7s,\testimator lgbm's best error=0.7184,\tbest estimator lgbm's best error=0.7184\n",
            "INFO:flaml.automl: at 5.7s,\testimator lgbm's best error=0.7184,\tbest estimator lgbm's best error=0.7184\n",
            "[flaml.automl: 12-26 15:25:20] {3166} INFO - iteration 3, current learner lgbm\n",
            "INFO:flaml.automl:iteration 3, current learner lgbm\n",
            "[flaml.automl: 12-26 15:25:23] {3343} INFO -  at 7.9s,\testimator lgbm's best error=0.6801,\tbest estimator lgbm's best error=0.6801\n",
            "INFO:flaml.automl: at 7.9s,\testimator lgbm's best error=0.6801,\tbest estimator lgbm's best error=0.6801\n",
            "[flaml.automl: 12-26 15:25:23] {3166} INFO - iteration 4, current learner lgbm\n",
            "INFO:flaml.automl:iteration 4, current learner lgbm\n",
            "[flaml.automl: 12-26 15:25:24] {3343} INFO -  at 9.5s,\testimator lgbm's best error=0.6801,\tbest estimator lgbm's best error=0.6801\n",
            "INFO:flaml.automl: at 9.5s,\testimator lgbm's best error=0.6801,\tbest estimator lgbm's best error=0.6801\n",
            "[flaml.automl: 12-26 15:25:24] {3166} INFO - iteration 5, current learner lgbm\n",
            "INFO:flaml.automl:iteration 5, current learner lgbm\n",
            "[flaml.automl: 12-26 15:25:27] {3343} INFO -  at 12.6s,\testimator lgbm's best error=0.6467,\tbest estimator lgbm's best error=0.6467\n",
            "INFO:flaml.automl: at 12.6s,\testimator lgbm's best error=0.6467,\tbest estimator lgbm's best error=0.6467\n",
            "[flaml.automl: 12-26 15:25:27] {3166} INFO - iteration 6, current learner lgbm\n",
            "INFO:flaml.automl:iteration 6, current learner lgbm\n",
            "[flaml.automl: 12-26 15:25:30] {3343} INFO -  at 14.8s,\testimator lgbm's best error=0.6467,\tbest estimator lgbm's best error=0.6467\n",
            "INFO:flaml.automl: at 14.8s,\testimator lgbm's best error=0.6467,\tbest estimator lgbm's best error=0.6467\n",
            "[flaml.automl: 12-26 15:25:30] {3166} INFO - iteration 7, current learner lgbm\n",
            "INFO:flaml.automl:iteration 7, current learner lgbm\n",
            "[flaml.automl: 12-26 15:25:32] {3343} INFO -  at 16.9s,\testimator lgbm's best error=0.6467,\tbest estimator lgbm's best error=0.6467\n",
            "INFO:flaml.automl: at 16.9s,\testimator lgbm's best error=0.6467,\tbest estimator lgbm's best error=0.6467\n",
            "[flaml.automl: 12-26 15:25:32] {3166} INFO - iteration 8, current learner lgbm\n",
            "INFO:flaml.automl:iteration 8, current learner lgbm\n",
            "[flaml.automl: 12-26 15:25:58] {3343} INFO -  at 43.1s,\testimator lgbm's best error=0.6281,\tbest estimator lgbm's best error=0.6281\n",
            "INFO:flaml.automl: at 43.1s,\testimator lgbm's best error=0.6281,\tbest estimator lgbm's best error=0.6281\n",
            "[flaml.automl: 12-26 15:25:58] {3166} INFO - iteration 9, current learner lgbm\n",
            "INFO:flaml.automl:iteration 9, current learner lgbm\n",
            "[flaml.automl: 12-26 15:26:01] {3343} INFO -  at 46.3s,\testimator lgbm's best error=0.6281,\tbest estimator lgbm's best error=0.6281\n",
            "INFO:flaml.automl: at 46.3s,\testimator lgbm's best error=0.6281,\tbest estimator lgbm's best error=0.6281\n",
            "[flaml.automl: 12-26 15:26:01] {3166} INFO - iteration 10, current learner lgbm\n",
            "INFO:flaml.automl:iteration 10, current learner lgbm\n",
            "[flaml.automl: 12-26 15:27:35] {3343} INFO -  at 139.8s,\testimator lgbm's best error=0.5390,\tbest estimator lgbm's best error=0.5390\n",
            "INFO:flaml.automl: at 139.8s,\testimator lgbm's best error=0.5390,\tbest estimator lgbm's best error=0.5390\n",
            "[flaml.automl: 12-26 15:27:35] {3166} INFO - iteration 11, current learner lgbm\n",
            "INFO:flaml.automl:iteration 11, current learner lgbm\n",
            "[flaml.automl: 12-26 15:29:45] {3343} INFO -  at 270.6s,\testimator lgbm's best error=0.4604,\tbest estimator lgbm's best error=0.4604\n",
            "INFO:flaml.automl: at 270.6s,\testimator lgbm's best error=0.4604,\tbest estimator lgbm's best error=0.4604\n",
            "[flaml.automl: 12-26 15:29:45] {3166} INFO - iteration 12, current learner lgbm\n",
            "INFO:flaml.automl:iteration 12, current learner lgbm\n",
            "[flaml.automl: 12-26 15:31:19] {3343} INFO -  at 364.2s,\testimator lgbm's best error=0.4604,\tbest estimator lgbm's best error=0.4604\n",
            "INFO:flaml.automl: at 364.2s,\testimator lgbm's best error=0.4604,\tbest estimator lgbm's best error=0.4604\n",
            "[flaml.automl: 12-26 15:31:19] {3166} INFO - iteration 13, current learner lgbm\n",
            "INFO:flaml.automl:iteration 13, current learner lgbm\n",
            "[flaml.automl: 12-26 15:34:59] {3343} INFO -  at 584.5s,\testimator lgbm's best error=0.4604,\tbest estimator lgbm's best error=0.4604\n",
            "INFO:flaml.automl: at 584.5s,\testimator lgbm's best error=0.4604,\tbest estimator lgbm's best error=0.4604\n",
            "[flaml.automl: 12-26 15:34:59] {3166} INFO - iteration 14, current learner lgbm\n",
            "INFO:flaml.automl:iteration 14, current learner lgbm\n",
            "[flaml.automl: 12-26 15:36:34] {3343} INFO -  at 679.2s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "INFO:flaml.automl: at 679.2s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl: 12-26 15:36:34] {3166} INFO - iteration 15, current learner lgbm\n",
            "INFO:flaml.automl:iteration 15, current learner lgbm\n",
            "[flaml.automl: 12-26 15:38:30] {3343} INFO -  at 795.1s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "INFO:flaml.automl: at 795.1s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl: 12-26 15:38:30] {3166} INFO - iteration 16, current learner lgbm\n",
            "INFO:flaml.automl:iteration 16, current learner lgbm\n",
            "[flaml.automl: 12-26 15:39:29] {3343} INFO -  at 854.5s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "INFO:flaml.automl: at 854.5s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl: 12-26 15:39:29] {3166} INFO - iteration 17, current learner lgbm\n",
            "INFO:flaml.automl:iteration 17, current learner lgbm\n",
            "[flaml.automl: 12-26 15:41:39] {3343} INFO -  at 984.5s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "INFO:flaml.automl: at 984.5s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl: 12-26 15:41:39] {3166} INFO - iteration 18, current learner lgbm\n",
            "INFO:flaml.automl:iteration 18, current learner lgbm\n",
            "[flaml.automl: 12-26 15:43:01] {3343} INFO -  at 1065.9s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "INFO:flaml.automl: at 1065.9s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl: 12-26 15:43:01] {3166} INFO - iteration 19, current learner lgbm\n",
            "INFO:flaml.automl:iteration 19, current learner lgbm\n",
            "[flaml.automl: 12-26 15:43:50] {3343} INFO -  at 1114.8s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "INFO:flaml.automl: at 1114.8s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl: 12-26 15:43:50] {3166} INFO - iteration 20, current learner lgbm\n",
            "INFO:flaml.automl:iteration 20, current learner lgbm\n",
            "[flaml.automl: 12-26 15:46:42] {3343} INFO -  at 1287.0s,\testimator lgbm's best error=0.4455,\tbest estimator lgbm's best error=0.4455\n",
            "INFO:flaml.automl: at 1287.0s,\testimator lgbm's best error=0.4455,\tbest estimator lgbm's best error=0.4455\n",
            "[flaml.automl: 12-26 15:46:42] {3166} INFO - iteration 21, current learner lgbm\n",
            "INFO:flaml.automl:iteration 21, current learner lgbm\n",
            "[flaml.automl: 12-26 15:53:46] {3343} INFO -  at 1711.6s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "INFO:flaml.automl: at 1711.6s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "[flaml.automl: 12-26 15:53:46] {3166} INFO - iteration 22, current learner lgbm\n",
            "INFO:flaml.automl:iteration 22, current learner lgbm\n",
            "[flaml.automl: 12-26 15:56:40] {3343} INFO -  at 1885.3s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "INFO:flaml.automl: at 1885.3s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "[flaml.automl: 12-26 15:56:40] {3166} INFO - iteration 23, current learner lgbm\n",
            "INFO:flaml.automl:iteration 23, current learner lgbm\n",
            "[flaml.automl: 12-26 16:08:31] {3343} INFO -  at 2595.8s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "INFO:flaml.automl: at 2595.8s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "[flaml.automl: 12-26 16:08:31] {3166} INFO - iteration 24, current learner lgbm\n",
            "INFO:flaml.automl:iteration 24, current learner lgbm\n",
            "[flaml.automl: 12-26 16:10:46] {3343} INFO -  at 2731.4s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "INFO:flaml.automl: at 2731.4s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "[flaml.automl: 12-26 16:10:46] {3166} INFO - iteration 25, current learner lgbm\n",
            "INFO:flaml.automl:iteration 25, current learner lgbm\n",
            "[flaml.automl: 12-26 16:20:19] {3343} INFO -  at 3303.8s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "INFO:flaml.automl: at 3303.8s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "[flaml.automl: 12-26 16:28:05] {3602} INFO - retrain lgbm for 466.0s\n",
            "INFO:flaml.automl:retrain lgbm for 466.0s\n",
            "[flaml.automl: 12-26 16:28:05] {3609} INFO - retrained model: LGBMClassifier(colsample_bytree=0.24489837298160008,\n",
            "               learning_rate=0.022776501888966183, max_bin=511,\n",
            "               min_child_samples=3, n_estimators=426, num_leaves=87,\n",
            "               reg_alpha=0.02046640007359354, reg_lambda=0.0032197962023686427,\n",
            "               verbose=-1)\n",
            "INFO:flaml.automl:retrained model: LGBMClassifier(colsample_bytree=0.24489837298160008,\n",
            "               learning_rate=0.022776501888966183, max_bin=511,\n",
            "               min_child_samples=3, n_estimators=426, num_leaves=87,\n",
            "               reg_alpha=0.02046640007359354, reg_lambda=0.0032197962023686427,\n",
            "               verbose=-1)\n",
            "[flaml.automl: 12-26 16:28:05] {2901} INFO - fit succeeded\n",
            "INFO:flaml.automl:fit succeeded\n",
            "[flaml.automl: 12-26 16:28:05] {2902} INFO - Time taken to find the best model: 1711.5660626888275\n",
            "INFO:flaml.automl:Time taken to find the best model: 1711.5660626888275\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparmeter config: {'n_estimators': 426, 'num_leaves': 87, 'min_child_samples': 3, 'learning_rate': 0.022776501888966183, 'log_max_bin': 9, 'colsample_bytree': 0.24489837298160008, 'reg_alpha': 0.02046640007359354, 'reg_lambda': 0.0032197962023686427}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqvUkwo_B3_w"
      },
      "source": [
        "TfidfVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jLWfUOSB3_w",
        "outputId": "5c5fa911-1623-4ca0-fc3f-8ba797653372"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:flaml.tune.spark.utils:\n",
            "check Spark installation...This line should appear only once.\n",
            "\n",
            "WARNING:flaml.tune.spark.utils:use_spark=True requires installation of PySpark. Please run pip install flaml[spark]\n",
            "        and check [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
            "        for more details about installing Spark.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[flaml.automl.automl: 12-30 14:00:00] {2712} INFO - task = classification\n",
            "[flaml.automl.automl: 12-30 14:00:00] {2714} INFO - Data split method: stratified\n",
            "[flaml.automl.automl: 12-30 14:00:00] {2717} INFO - Evaluation method: holdout\n",
            "[flaml.automl.automl: 12-30 14:00:00] {2844} INFO - Minimizing error metric: 1-accuracy\n",
            "[flaml.automl.automl: 12-30 14:00:00] {2990} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
            "[flaml.automl.automl: 12-30 14:00:00] {3319} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:00:03] {3457} INFO - Estimated sufficient time budget=27740s. Estimated necessary time budget=28s.\n",
            "[flaml.automl.automl: 12-30 14:00:03] {3504} INFO -  at 2.9s,\testimator lgbm's best error=0.7153,\tbest estimator lgbm's best error=0.7153\n",
            "[flaml.automl.automl: 12-30 14:00:03] {3319} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:00:06] {3504} INFO -  at 5.4s,\testimator lgbm's best error=0.7153,\tbest estimator lgbm's best error=0.7153\n",
            "[flaml.automl.automl: 12-30 14:00:06] {3319} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:00:09] {3504} INFO -  at 8.4s,\testimator lgbm's best error=0.6906,\tbest estimator lgbm's best error=0.6906\n",
            "[flaml.automl.automl: 12-30 14:00:09] {3319} INFO - iteration 3, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:00:15] {3504} INFO -  at 14.9s,\testimator lgbm's best error=0.6399,\tbest estimator lgbm's best error=0.6399\n",
            "[flaml.automl.automl: 12-30 14:00:15] {3319} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:00:18] {3504} INFO -  at 18.0s,\testimator lgbm's best error=0.6399,\tbest estimator lgbm's best error=0.6399\n",
            "[flaml.automl.automl: 12-30 14:00:18] {3319} INFO - iteration 5, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:00:26] {3504} INFO -  at 25.9s,\testimator lgbm's best error=0.6287,\tbest estimator lgbm's best error=0.6287\n",
            "[flaml.automl.automl: 12-30 14:00:26] {3319} INFO - iteration 6, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:00:33] {3504} INFO -  at 32.4s,\testimator lgbm's best error=0.6287,\tbest estimator lgbm's best error=0.6287\n",
            "[flaml.automl.automl: 12-30 14:00:33] {3319} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:00:37] {3504} INFO -  at 36.9s,\testimator lgbm's best error=0.6287,\tbest estimator lgbm's best error=0.6287\n",
            "[flaml.automl.automl: 12-30 14:00:37] {3319} INFO - iteration 8, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:00:52] {3504} INFO -  at 52.0s,\testimator lgbm's best error=0.5835,\tbest estimator lgbm's best error=0.5835\n",
            "[flaml.automl.automl: 12-30 14:00:52] {3319} INFO - iteration 9, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:00:59] {3504} INFO -  at 58.9s,\testimator lgbm's best error=0.5835,\tbest estimator lgbm's best error=0.5835\n",
            "[flaml.automl.automl: 12-30 14:00:59] {3319} INFO - iteration 10, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:03:15] {3504} INFO -  at 194.3s,\testimator lgbm's best error=0.4697,\tbest estimator lgbm's best error=0.4697\n",
            "[flaml.automl.automl: 12-30 14:03:15] {3319} INFO - iteration 11, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:06:25] {3504} INFO -  at 384.2s,\testimator lgbm's best error=0.4697,\tbest estimator lgbm's best error=0.4697\n",
            "[flaml.automl.automl: 12-30 14:06:25] {3319} INFO - iteration 12, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:07:44] {3504} INFO -  at 463.4s,\testimator lgbm's best error=0.4697,\tbest estimator lgbm's best error=0.4697\n",
            "[flaml.automl.automl: 12-30 14:07:44] {3319} INFO - iteration 13, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:08:55] {3504} INFO -  at 534.1s,\testimator lgbm's best error=0.4697,\tbest estimator lgbm's best error=0.4697\n",
            "[flaml.automl.automl: 12-30 14:08:55] {3319} INFO - iteration 14, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:12:48] {3504} INFO -  at 767.5s,\testimator lgbm's best error=0.4697,\tbest estimator lgbm's best error=0.4697\n",
            "[flaml.automl.automl: 12-30 14:12:48] {3319} INFO - iteration 15, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:14:48] {3504} INFO -  at 888.0s,\testimator lgbm's best error=0.4697,\tbest estimator lgbm's best error=0.4697\n",
            "[flaml.automl.automl: 12-30 14:14:48] {3319} INFO - iteration 16, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:17:32] {3504} INFO -  at 1051.9s,\testimator lgbm's best error=0.4697,\tbest estimator lgbm's best error=0.4697\n",
            "[flaml.automl.automl: 12-30 14:17:32] {3319} INFO - iteration 17, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:24:56] {3504} INFO -  at 1495.3s,\testimator lgbm's best error=0.4697,\tbest estimator lgbm's best error=0.4697\n",
            "[flaml.automl.automl: 12-30 14:24:56] {3319} INFO - iteration 18, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:27:10] {3504} INFO -  at 1629.7s,\testimator lgbm's best error=0.4697,\tbest estimator lgbm's best error=0.4697\n",
            "[flaml.automl.automl: 12-30 14:27:10] {3319} INFO - iteration 19, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:27:56] {3504} INFO -  at 1675.5s,\testimator lgbm's best error=0.4697,\tbest estimator lgbm's best error=0.4697\n",
            "[flaml.automl.automl: 12-30 14:27:56] {3319} INFO - iteration 20, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:36:53] {3504} INFO -  at 2212.3s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl.automl: 12-30 14:36:53] {3319} INFO - iteration 21, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:52:27] {3504} INFO -  at 3146.6s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl.automl: 12-30 14:52:27] {3319} INFO - iteration 22, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:56:39] {3504} INFO -  at 3399.1s,\testimator lgbm's best error=0.4548,\tbest estimator lgbm's best error=0.4548\n",
            "[flaml.automl.automl: 12-30 14:56:39] {3319} INFO - iteration 23, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 14:59:15] {3504} INFO -  at 3554.5s,\testimator lgbm's best error=0.4548,\tbest estimator lgbm's best error=0.4548\n",
            "[flaml.automl.automl: 12-30 14:59:15] {3319} INFO - iteration 24, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 15:02:16] {3504} INFO -  at 3735.9s,\testimator lgbm's best error=0.4548,\tbest estimator lgbm's best error=0.4548\n",
            "[flaml.automl.automl: 12-30 15:02:16] {3319} INFO - iteration 25, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 15:11:16] {3504} INFO -  at 4275.8s,\testimator lgbm's best error=0.4548,\tbest estimator lgbm's best error=0.4548\n",
            "[flaml.automl.automl: 12-30 15:11:16] {3319} INFO - iteration 26, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 15:13:50] {3504} INFO -  at 4429.2s,\testimator lgbm's best error=0.4548,\tbest estimator lgbm's best error=0.4548\n",
            "[flaml.automl.automl: 12-30 15:13:50] {3319} INFO - iteration 27, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 15:21:01] {3504} INFO -  at 4861.1s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "[flaml.automl.automl: 12-30 15:21:01] {3319} INFO - iteration 28, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 15:24:59] {3504} INFO -  at 5098.3s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "[flaml.automl.automl: 12-30 15:24:59] {3319} INFO - iteration 29, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 15:28:07] {3504} INFO -  at 5286.8s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "[flaml.automl.automl: 12-30 15:28:07] {3319} INFO - iteration 30, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 15:35:06] {3504} INFO -  at 5705.6s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "[flaml.automl.automl: 12-30 15:35:06] {3319} INFO - iteration 31, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 15:40:43] {3504} INFO -  at 6042.3s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "[flaml.automl.automl: 12-30 15:40:43] {3319} INFO - iteration 32, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 15:50:27] {3504} INFO -  at 6626.4s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "[flaml.automl.automl: 12-30 15:50:27] {3319} INFO - iteration 33, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 16:01:19] {3504} INFO -  at 7278.8s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "[flaml.automl.automl: 12-30 16:01:19] {3319} INFO - iteration 34, current learner lgbm\n",
            "[flaml.automl.automl: 12-30 16:10:24] {3504} INFO -  at 7824.0s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "[flaml.automl.automl: 12-30 16:21:28] {3768} INFO - retrain lgbm for 663.4s\n",
            "[flaml.automl.automl: 12-30 16:21:28] {3775} INFO - retrained model: LGBMClassifier(colsample_bytree=0.41892828051953873,\n",
            "               learning_rate=0.07126610147786543, max_bin=31,\n",
            "               min_child_samples=10, n_estimators=712, num_leaves=65,\n",
            "               reg_alpha=0.007704104902643929, reg_lambda=0.008590903883060216,\n",
            "               verbose=-1)\n",
            "[flaml.automl.automl: 12-30 16:21:28] {3020} INFO - fit succeeded\n",
            "[flaml.automl.automl: 12-30 16:21:28] {3021} INFO - Time taken to find the best model: 6626.377639770508\n",
            "Best hyperparmeter config: {'n_estimators': 712, 'num_leaves': 65, 'min_child_samples': 10, 'learning_rate': 0.07126610147786543, 'log_max_bin': 5, 'colsample_bytree': 0.41892828051953873, 'reg_alpha': 0.007704104902643929, 'reg_lambda': 0.008590903883060216}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=8000)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrPFxYSOB3_w"
      },
      "source": [
        "Spacy-Vektoren:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nlDtvojB3_w"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "X = [tokenize_and_lemmatize(text) for text in documents]\n",
        "\n",
        "def document_vector(doc):\n",
        "    return np.mean([token.vector for token in doc], axis=0)\n",
        "X = [document_vector(nlp(text)) for text in documents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "xdKPxJ2x_lXk",
        "outputId": "98601bf7-e9e3-4f02-fc81-1063aa1409aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[flaml.automl.automl: 12-31 11:44:47] {2712} INFO - task = classification\n",
            "[flaml.automl.automl: 12-31 11:44:47] {2714} INFO - Data split method: stratified\n",
            "[flaml.automl.automl: 12-31 11:44:47] {2717} INFO - Evaluation method: cv\n",
            "[flaml.automl.automl: 12-31 11:44:47] {2844} INFO - Minimizing error metric: 1-accuracy\n",
            "[flaml.automl.automl: 12-31 11:44:47] {2990} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
            "[flaml.automl.automl: 12-31 11:44:47] {3319} INFO - iteration 0, current learner lgbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-42-42ec0eec966a>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.array(X.tolist())\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-42ec0eec966a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Das Flaml-Modell deklarieren und trainieren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mautoml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mautoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lgbm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_budget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Die besten Hyperparameter ausgeben\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flaml/automl/automl.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, dataframe, label, metric, task, n_jobs, log_file_name, estimator_list, time_budget, max_iter, sample, ensemble, eval_method, log_type, model_history, split_ratio, n_splits, log_training_metric, mem_thres, pred_time_limit, train_time_limit, X_val, y_val, sample_weight_val, groups_val, groups, verbose, retrain_full, split_type, learner_selector, hpo_method, starting_points, seed, n_concurrent_trials, keep_search_state, preserve_checkpoint, early_stop, append_log, auto_augment, min_sample_size, use_ray, use_spark, free_mem_ratio, metric_constraints, custom_hp, cv_score_agg_func, skip_transform, fit_kwargs_by_estimator, **fit_kwargs)\u001b[0m\n\u001b[1;32m   3016\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3017\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3018\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3019\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_best_estimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit succeeded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flaml/automl/automl.py\u001b[0m in \u001b[0;36m_search\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3605\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3606\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_ray\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_spark\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3607\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search_sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3608\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3609\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flaml/automl/automl.py\u001b[0m in \u001b[0;36m_search_sequential\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3427\u001b[0m                     )\n\u001b[1;32m   3428\u001b[0m             \u001b[0mstart_run_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3429\u001b[0;31m             analysis = tune.run(\n\u001b[0m\u001b[1;32m   3430\u001b[0m                 \u001b[0msearch_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m                 \u001b[0msearch_alg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_alg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flaml/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(evaluation_function, config, low_cost_partial_config, cat_hp_cost, metric, mode, time_budget_s, points_to_evaluate, evaluated_rewards, resource_attr, min_resource, max_resource, reduction_factor, scheduler, search_alg, verbose, local_dir, num_samples, resources_per_trial, config_constraints, metric_constraints, max_failure, use_ray, use_spark, use_incumbent_result_in_evaluation, log_file_name, lexico_objectives, **ray_args)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"trial {num_trials} config: {trial_to_run.config}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_to_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flaml/automl/automl.py\u001b[0m in \u001b[0;36m_compute_with_config_base\u001b[0;34m(config_w_resource, state, estimator, is_report)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mpred_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0msampled_X_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0msampled_y_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flaml/automl/ml.py\u001b[0m in \u001b[0;36mcompute_estimator\u001b[0;34m(X_train, y_train, X_val, y_val, weight_val, groups_val, budget, kf, config_dic, task, estimator_name, eval_method, eval_metric, best_val_loss, n_jobs, estimator_class, cv_score_agg_func, log_training_metric, fit_kwargs, free_mem_ratio)\u001b[0m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         val_loss, metric_for_logging, train_time, pred_time = evaluate_model_CV(\n\u001b[0m\u001b[1;32m    646\u001b[0m             \u001b[0mconfig_dic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flaml/automl/ml.py\u001b[0m in \u001b[0;36mevaluate_model_CV\u001b[0;34m(config, estimator, X_train_all, y_train_all, budget, kf, task, eval_metric, best_val_loss, cv_score_agg_func, log_training_metric, fit_kwargs, free_mem_ratio)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0mgroups_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         val_loss_i, metric_i, train_time_i, pred_time_i = get_val_loss(\n\u001b[0m\u001b[1;32m    556\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flaml/automl/ml.py\u001b[0m in \u001b[0;36mget_val_loss\u001b[0;34m(config, estimator, X_train, y_train, X_val, y_val, weight_val, groups_val, eval_metric, obj, labels, budget, log_training_metric, fit_kwargs, free_mem_ratio)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;31m#     fit_kwargs['X_val'] = X_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;31m#     fit_kwargs['y_val'] = y_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfree_mem_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     val_loss, metric_for_logging, pred_time, _ = _eval_estimator(\n\u001b[1;32m    445\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flaml/automl/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, budget, free_mem_ratio, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m             self._fit(\n\u001b[0m\u001b[1;32m   1148\u001b[0m                 \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flaml/automl/model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X_train, y_train, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# xgboost 1.6 doesn't display all the params in the model str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"flaml.model - {model} fit started with params {self.params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"flaml.model - {model} fit finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    965\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n\u001b[0m\u001b[1;32m    968\u001b[0m                     \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_sample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                     \u001b[0meval_class_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_class_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_init_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd_DataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt_DataTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0m_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LGBMCheckXY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                 \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LGBMCheckSampleWeight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ],
      "source": [
        "X = np.array(X.tolist())\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyH0BNYK9oeg",
        "outputId": "0755f78b-e22c-499e-ec04-04d0745babb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(18900,)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IExLIP3t9y6n",
        "outputId": "82375253-6598-4c21-f8d7-56ccdeffb448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[1 2 3 4 5 6]\n"
          ]
        }
      ],
      "source": [
        "Y = [1,2,3,4,5,6]\n",
        "Y = np.array(Y)\n",
        "print(type(Y))\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRpm6PPNA00O"
      },
      "source": [
        "# Einstellen, welche Lyrik-Spalten verwendet werden sollen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQd0Ei6EBBk6"
      },
      "outputs": [],
      "source": [
        "# Laden Sie die Textdokumente und die Klassenlabels\n",
        "documents = df['wosw_lyrics'].tolist() # Liste von Textdokumenten\n",
        "labels = df['Genre'] # Liste von Klassenlabels (eins pro Dokument)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhYo2eIkBcec"
      },
      "source": [
        "# 3. Grid-search SVM mit SKLEARN:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1trYSBced"
      },
      "source": [
        "WIE ICH IN ZUKUNFT DIE OPTIMALEN PARAMETER RAUSFINDE:\n",
        "Ich taste mich erstmal mit den jetztigen ran, schau mir die optimalen an, dann\n",
        "werde ich eine Reihe von Paramtern die ungefähr um den optimalen Parameter liegen testen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brKfkJobBced"
      },
      "source": [
        "CountVektorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKwP4Lo2jx5R",
        "outputId": "b4d980d0-9535-41f9-e231-4c9427f22e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "[CV 1/5] END C=0.0099, gamma=0.079, kernel=linear;, score=0.442 total time= 2.4min\n",
            "[CV 2/5] END C=0.0099, gamma=0.079, kernel=linear;, score=0.460 total time= 2.4min\n",
            "[CV 3/5] END C=0.0099, gamma=0.079, kernel=linear;, score=0.441 total time= 2.4min\n",
            "[CV 4/5] END C=0.0099, gamma=0.079, kernel=linear;, score=0.459 total time= 2.4min\n",
            "[CV 5/5] END C=0.0099, gamma=0.079, kernel=linear;, score=0.459 total time= 2.6min\n",
            "[CV 1/5] END C=0.0099, gamma=0.08, kernel=linear;, score=0.442 total time= 2.4min\n",
            "[CV 2/5] END C=0.0099, gamma=0.08, kernel=linear;, score=0.460 total time= 2.5min\n",
            "[CV 3/5] END C=0.0099, gamma=0.08, kernel=linear;, score=0.441 total time= 2.4min\n",
            "[CV 4/5] END C=0.0099, gamma=0.08, kernel=linear;, score=0.459 total time= 2.5min\n",
            "[CV 5/5] END C=0.0099, gamma=0.08, kernel=linear;, score=0.459 total time= 2.5min\n",
            "[CV 1/5] END C=0.0099, gamma=0.081, kernel=linear;, score=0.442 total time= 2.5min\n",
            "[CV 2/5] END C=0.0099, gamma=0.081, kernel=linear;, score=0.460 total time= 2.5min\n",
            "[CV 3/5] END C=0.0099, gamma=0.081, kernel=linear;, score=0.441 total time= 2.6min\n",
            "[CV 4/5] END C=0.0099, gamma=0.081, kernel=linear;, score=0.459 total time= 2.5min\n",
            "[CV 5/5] END C=0.0099, gamma=0.081, kernel=linear;, score=0.459 total time= 2.5min\n",
            "[CV 1/5] END C=0.0099, gamma=0.085, kernel=linear;, score=0.442 total time= 2.5min\n",
            "[CV 2/5] END C=0.0099, gamma=0.085, kernel=linear;, score=0.460 total time= 2.5min\n",
            "[CV 3/5] END C=0.0099, gamma=0.085, kernel=linear;, score=0.441 total time= 2.5min\n",
            "[CV 4/5] END C=0.0099, gamma=0.085, kernel=linear;, score=0.459 total time= 2.6min\n",
            "[CV 5/5] END C=0.0099, gamma=0.085, kernel=linear;, score=0.459 total time= 2.5min\n",
            "[CV 1/5] END C=0.01, gamma=0.079, kernel=linear;, score=0.441 total time= 2.5min\n",
            "[CV 2/5] END C=0.01, gamma=0.079, kernel=linear;, score=0.462 total time= 2.5min\n",
            "[CV 3/5] END C=0.01, gamma=0.079, kernel=linear;, score=0.440 total time= 2.5min\n",
            "[CV 4/5] END C=0.01, gamma=0.079, kernel=linear;, score=0.459 total time= 2.5min\n",
            "[CV 5/5] END C=0.01, gamma=0.079, kernel=linear;, score=0.458 total time= 2.6min\n",
            "[CV 1/5] END .C=0.01, gamma=0.08, kernel=linear;, score=0.441 total time= 2.4min\n",
            "[CV 2/5] END .C=0.01, gamma=0.08, kernel=linear;, score=0.462 total time= 2.5min\n",
            "[CV 3/5] END .C=0.01, gamma=0.08, kernel=linear;, score=0.440 total time= 2.5min\n",
            "[CV 4/5] END .C=0.01, gamma=0.08, kernel=linear;, score=0.459 total time= 2.5min\n",
            "[CV 5/5] END .C=0.01, gamma=0.08, kernel=linear;, score=0.458 total time= 2.5min\n",
            "[CV 1/5] END C=0.01, gamma=0.081, kernel=linear;, score=0.441 total time= 2.5min\n",
            "[CV 2/5] END C=0.01, gamma=0.081, kernel=linear;, score=0.462 total time= 2.5min\n",
            "[CV 3/5] END C=0.01, gamma=0.081, kernel=linear;, score=0.440 total time= 2.6min\n",
            "[CV 4/5] END C=0.01, gamma=0.081, kernel=linear;, score=0.459 total time= 2.5min\n",
            "[CV 5/5] END C=0.01, gamma=0.081, kernel=linear;, score=0.458 total time= 2.5min\n",
            "[CV 1/5] END C=0.01, gamma=0.085, kernel=linear;, score=0.441 total time= 2.4min\n",
            "[CV 2/5] END C=0.01, gamma=0.085, kernel=linear;, score=0.462 total time= 2.5min\n",
            "[CV 3/5] END C=0.01, gamma=0.085, kernel=linear;, score=0.440 total time= 2.5min\n",
            "[CV 4/5] END C=0.01, gamma=0.085, kernel=linear;, score=0.459 total time= 2.5min\n",
            "[CV 5/5] END C=0.01, gamma=0.085, kernel=linear;, score=0.458 total time= 2.6min\n",
            "[CV 1/5] END C=0.011, gamma=0.079, kernel=linear;, score=0.444 total time= 2.6min\n",
            "[CV 2/5] END C=0.011, gamma=0.079, kernel=linear;, score=0.465 total time= 2.5min\n",
            "[CV 3/5] END C=0.011, gamma=0.079, kernel=linear;, score=0.441 total time= 2.5min\n",
            "[CV 4/5] END C=0.011, gamma=0.079, kernel=linear;, score=0.459 total time= 2.5min\n",
            "[CV 5/5] END C=0.011, gamma=0.079, kernel=linear;, score=0.460 total time= 2.5min\n",
            "[CV 1/5] END C=0.011, gamma=0.08, kernel=linear;, score=0.444 total time= 2.5min\n",
            "[CV 2/5] END C=0.011, gamma=0.08, kernel=linear;, score=0.465 total time= 2.6min\n",
            "[CV 3/5] END C=0.011, gamma=0.08, kernel=linear;, score=0.441 total time= 2.6min\n",
            "[CV 4/5] END C=0.011, gamma=0.08, kernel=linear;, score=0.459 total time= 2.6min\n",
            "[CV 5/5] END C=0.011, gamma=0.08, kernel=linear;, score=0.460 total time= 2.5min\n",
            "[CV 1/5] END C=0.011, gamma=0.081, kernel=linear;, score=0.444 total time= 2.5min\n",
            "[CV 2/5] END C=0.011, gamma=0.081, kernel=linear;, score=0.465 total time= 2.4min\n",
            "[CV 3/5] END C=0.011, gamma=0.081, kernel=linear;, score=0.441 total time= 2.5min\n",
            "[CV 4/5] END C=0.011, gamma=0.081, kernel=linear;, score=0.459 total time= 2.4min\n",
            "[CV 5/5] END C=0.011, gamma=0.081, kernel=linear;, score=0.460 total time= 2.4min\n",
            "[CV 1/5] END C=0.011, gamma=0.085, kernel=linear;, score=0.444 total time= 2.5min\n",
            "[CV 2/5] END C=0.011, gamma=0.085, kernel=linear;, score=0.465 total time= 2.6min\n",
            "[CV 3/5] END C=0.011, gamma=0.085, kernel=linear;, score=0.441 total time= 2.5min\n",
            "[CV 4/5] END C=0.011, gamma=0.085, kernel=linear;, score=0.459 total time= 2.5min\n",
            "[CV 5/5] END C=0.011, gamma=0.085, kernel=linear;, score=0.460 total time= 2.5min\n",
            "{'C': 0.011, 'gamma': 0.079, 'kernel': 'linear'}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [0.011, 0.012, 0.014], \n",
        "                  'gamma': [0.075, 0.079, 0.0795],\n",
        "                  'kernel': ['linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KYBnEosBced"
      },
      "source": [
        "SVM mit TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkIa0_r4Bcee",
        "outputId": "f8d58adc-a28e-42eb-c1be-ba72ed274e38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "[CV 1/5] END .....C=3.5, gamma=0.95, kernel=rbf;, score=0.487 total time= 4.5min\n",
            "[CV 2/5] END .....C=3.5, gamma=0.95, kernel=rbf;, score=0.507 total time= 4.4min\n",
            "[CV 3/5] END .....C=3.5, gamma=0.95, kernel=rbf;, score=0.495 total time= 4.4min\n",
            "[CV 4/5] END .....C=3.5, gamma=0.95, kernel=rbf;, score=0.504 total time= 4.4min\n",
            "[CV 5/5] END .....C=3.5, gamma=0.95, kernel=rbf;, score=0.504 total time= 4.4min\n",
            "[CV 1/5] END ........C=3.5, gamma=1, kernel=rbf;, score=0.488 total time= 4.5min\n",
            "[CV 2/5] END ........C=3.5, gamma=1, kernel=rbf;, score=0.507 total time= 4.4min\n",
            "[CV 3/5] END ........C=3.5, gamma=1, kernel=rbf;, score=0.495 total time= 4.7min\n",
            "[CV 4/5] END ........C=3.5, gamma=1, kernel=rbf;, score=0.503 total time= 4.4min\n",
            "[CV 5/5] END ........C=3.5, gamma=1, kernel=rbf;, score=0.504 total time= 4.4min\n",
            "[CV 1/5] END ........C=3.5, gamma=4, kernel=rbf;, score=0.412 total time= 4.6min\n",
            "[CV 2/5] END ........C=3.5, gamma=4, kernel=rbf;, score=0.414 total time= 4.6min\n",
            "[CV 3/5] END ........C=3.5, gamma=4, kernel=rbf;, score=0.404 total time= 4.6min\n",
            "[CV 4/5] END ........C=3.5, gamma=4, kernel=rbf;, score=0.413 total time= 4.6min\n",
            "[CV 5/5] END ........C=3.5, gamma=4, kernel=rbf;, score=0.407 total time= 4.8min\n",
            "[CV 1/5] END .......C=3.5, gamma=10, kernel=rbf;, score=0.264 total time= 4.7min\n",
            "[CV 2/5] END .......C=3.5, gamma=10, kernel=rbf;, score=0.269 total time= 4.7min\n",
            "[CV 3/5] END .......C=3.5, gamma=10, kernel=rbf;, score=0.256 total time= 4.7min\n",
            "[CV 4/5] END .......C=3.5, gamma=10, kernel=rbf;, score=0.262 total time= 4.7min\n",
            "[CV 5/5] END .......C=3.5, gamma=10, kernel=rbf;, score=0.264 total time= 4.5min\n",
            "[CV 1/5] END .......C=4, gamma=0.95, kernel=rbf;, score=0.488 total time= 4.4min\n",
            "[CV 2/5] END .......C=4, gamma=0.95, kernel=rbf;, score=0.507 total time= 4.4min\n",
            "[CV 3/5] END .......C=4, gamma=0.95, kernel=rbf;, score=0.496 total time= 4.5min\n",
            "[CV 4/5] END .......C=4, gamma=0.95, kernel=rbf;, score=0.505 total time= 4.6min\n",
            "[CV 5/5] END .......C=4, gamma=0.95, kernel=rbf;, score=0.505 total time= 4.5min\n",
            "[CV 1/5] END ..........C=4, gamma=1, kernel=rbf;, score=0.488 total time= 4.4min\n",
            "[CV 2/5] END ..........C=4, gamma=1, kernel=rbf;, score=0.507 total time= 4.5min\n",
            "[CV 3/5] END ..........C=4, gamma=1, kernel=rbf;, score=0.495 total time= 4.4min\n",
            "[CV 4/5] END ..........C=4, gamma=1, kernel=rbf;, score=0.504 total time= 4.5min\n",
            "[CV 5/5] END ..........C=4, gamma=1, kernel=rbf;, score=0.506 total time= 4.4min\n",
            "[CV 1/5] END ..........C=4, gamma=4, kernel=rbf;, score=0.412 total time= 4.6min\n",
            "[CV 2/5] END ..........C=4, gamma=4, kernel=rbf;, score=0.414 total time= 4.6min\n",
            "[CV 3/5] END ..........C=4, gamma=4, kernel=rbf;, score=0.404 total time= 4.6min\n",
            "[CV 4/5] END ..........C=4, gamma=4, kernel=rbf;, score=0.413 total time= 4.6min\n",
            "[CV 5/5] END ..........C=4, gamma=4, kernel=rbf;, score=0.407 total time= 4.6min\n",
            "[CV 1/5] END .........C=4, gamma=10, kernel=rbf;, score=0.264 total time= 4.6min\n",
            "[CV 2/5] END .........C=4, gamma=10, kernel=rbf;, score=0.269 total time= 4.7min\n",
            "[CV 3/5] END .........C=4, gamma=10, kernel=rbf;, score=0.256 total time= 4.6min\n",
            "[CV 4/5] END .........C=4, gamma=10, kernel=rbf;, score=0.262 total time= 4.6min\n",
            "[CV 5/5] END .........C=4, gamma=10, kernel=rbf;, score=0.264 total time= 4.5min\n",
            "[CV 1/5] END .....C=4.5, gamma=0.95, kernel=rbf;, score=0.489 total time= 4.4min\n",
            "[CV 2/5] END .....C=4.5, gamma=0.95, kernel=rbf;, score=0.509 total time= 4.5min\n",
            "[CV 3/5] END .....C=4.5, gamma=0.95, kernel=rbf;, score=0.496 total time= 4.5min\n",
            "[CV 4/5] END .....C=4.5, gamma=0.95, kernel=rbf;, score=0.505 total time= 4.5min\n",
            "[CV 5/5] END .....C=4.5, gamma=0.95, kernel=rbf;, score=0.505 total time= 4.6min\n",
            "[CV 1/5] END ........C=4.5, gamma=1, kernel=rbf;, score=0.488 total time= 4.5min\n",
            "[CV 2/5] END ........C=4.5, gamma=1, kernel=rbf;, score=0.508 total time= 4.5min\n",
            "[CV 3/5] END ........C=4.5, gamma=1, kernel=rbf;, score=0.495 total time= 4.6min\n",
            "[CV 4/5] END ........C=4.5, gamma=1, kernel=rbf;, score=0.504 total time= 4.7min\n",
            "[CV 5/5] END ........C=4.5, gamma=1, kernel=rbf;, score=0.506 total time= 4.6min\n",
            "[CV 1/5] END ........C=4.5, gamma=4, kernel=rbf;, score=0.412 total time= 4.7min\n",
            "[CV 2/5] END ........C=4.5, gamma=4, kernel=rbf;, score=0.414 total time= 4.7min\n",
            "[CV 3/5] END ........C=4.5, gamma=4, kernel=rbf;, score=0.404 total time= 4.6min\n",
            "[CV 4/5] END ........C=4.5, gamma=4, kernel=rbf;, score=0.413 total time= 4.6min\n",
            "[CV 5/5] END ........C=4.5, gamma=4, kernel=rbf;, score=0.407 total time= 4.6min\n",
            "[CV 1/5] END .......C=4.5, gamma=10, kernel=rbf;, score=0.264 total time= 4.7min\n",
            "[CV 2/5] END .......C=4.5, gamma=10, kernel=rbf;, score=0.269 total time= 4.9min\n",
            "[CV 3/5] END .......C=4.5, gamma=10, kernel=rbf;, score=0.256 total time= 4.9min\n",
            "[CV 4/5] END .......C=4.5, gamma=10, kernel=rbf;, score=0.263 total time= 4.8min\n",
            "[CV 5/5] END .......C=4.5, gamma=10, kernel=rbf;, score=0.264 total time= 4.6min\n",
            "[CV 1/5] END .......C=5, gamma=0.95, kernel=rbf;, score=0.490 total time= 4.5min\n",
            "[CV 2/5] END .......C=5, gamma=0.95, kernel=rbf;, score=0.508 total time= 4.5min\n",
            "[CV 3/5] END .......C=5, gamma=0.95, kernel=rbf;, score=0.496 total time= 4.4min\n",
            "[CV 4/5] END .......C=5, gamma=0.95, kernel=rbf;, score=0.504 total time= 4.5min\n",
            "[CV 5/5] END .......C=5, gamma=0.95, kernel=rbf;, score=0.505 total time= 4.6min\n",
            "[CV 1/5] END ..........C=5, gamma=1, kernel=rbf;, score=0.488 total time= 4.5min\n",
            "[CV 2/5] END ..........C=5, gamma=1, kernel=rbf;, score=0.507 total time= 4.6min\n",
            "[CV 3/5] END ..........C=5, gamma=1, kernel=rbf;, score=0.495 total time= 4.6min\n",
            "[CV 4/5] END ..........C=5, gamma=1, kernel=rbf;, score=0.504 total time= 4.6min\n",
            "[CV 5/5] END ..........C=5, gamma=1, kernel=rbf;, score=0.506 total time= 4.6min\n",
            "[CV 1/5] END ..........C=5, gamma=4, kernel=rbf;, score=0.412 total time= 4.6min\n",
            "[CV 2/5] END ..........C=5, gamma=4, kernel=rbf;, score=0.414 total time= 4.6min\n",
            "[CV 3/5] END ..........C=5, gamma=4, kernel=rbf;, score=0.404 total time= 4.6min\n",
            "[CV 4/5] END ..........C=5, gamma=4, kernel=rbf;, score=0.413 total time= 4.6min\n",
            "[CV 5/5] END ..........C=5, gamma=4, kernel=rbf;, score=0.407 total time= 4.6min\n",
            "[CV 1/5] END .........C=5, gamma=10, kernel=rbf;, score=0.264 total time= 4.6min\n",
            "[CV 2/5] END .........C=5, gamma=10, kernel=rbf;, score=0.269 total time= 4.7min\n",
            "[CV 3/5] END .........C=5, gamma=10, kernel=rbf;, score=0.256 total time= 4.6min\n",
            "[CV 4/5] END .........C=5, gamma=10, kernel=rbf;, score=0.263 total time= 4.6min\n",
            "[CV 5/5] END .........C=5, gamma=10, kernel=rbf;, score=0.264 total time= 4.6min\n",
            "{'C': 4.5, 'gamma': 0.95, 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [3.5, 4, 4.5, 5], \n",
        "                  'gamma': [0.95,1,4,10],\n",
        "                  'kernel': ['rbf']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npjNFl0tBcee"
      },
      "source": [
        "Spacy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "E_nL0KYtBcee",
        "outputId": "cd724133-3c01-4a0d-d961-0799098e21aa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5a36199937ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Tokenize and lemmatize the documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize_and_lemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Compute the document vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
          ]
        }
      ],
      "source": [
        " def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "def document_vector(docs):\n",
        "    vectors = [token.vector for text in docs for token in nlp(text)]\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# Tokenize and lemmatize the documents\n",
        "X = [tokenize_and_lemmatize(text) for text in documents]\n",
        "\n",
        "# Compute the document vectors\n",
        "X = [document_vector(X) for X in X]\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [1, 10, 100], \n",
        "                  'gamma': [1, 0.1, 0.01],\n",
        "                  'kernel': ['rbf', 'linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDfs9MKTB4NT"
      },
      "source": [
        "# 3. Gridsearch LGBM mit FLAML:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q53vUc5TB4NU"
      },
      "source": [
        "CountVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCNF9IALB4NU",
        "outputId": "2d1f698a-956b-44a5-d4d5-30c312c8da80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[flaml.automl: 12-26 17:44:48] {2599} INFO - task = classification\n",
            "INFO:flaml.automl:task = classification\n",
            "[flaml.automl: 12-26 17:44:48] {2601} INFO - Data split method: stratified\n",
            "INFO:flaml.automl:Data split method: stratified\n",
            "[flaml.automl: 12-26 17:44:48] {2604} INFO - Evaluation method: holdout\n",
            "INFO:flaml.automl:Evaluation method: holdout\n",
            "[flaml.automl: 12-26 17:44:48] {2726} INFO - Minimizing error metric: 1-accuracy\n",
            "INFO:flaml.automl:Minimizing error metric: 1-accuracy\n",
            "[flaml.automl: 12-26 17:44:48] {2870} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
            "INFO:flaml.automl:List of ML learners in AutoML Run: ['lgbm']\n",
            "[flaml.automl: 12-26 17:44:48] {3166} INFO - iteration 0, current learner lgbm\n",
            "INFO:flaml.automl:iteration 0, current learner lgbm\n",
            "[flaml.automl: 12-26 17:44:50] {3296} INFO - Estimated sufficient time budget=20883s. Estimated necessary time budget=21s.\n",
            "INFO:flaml.automl:Estimated sufficient time budget=20883s. Estimated necessary time budget=21s.\n",
            "[flaml.automl: 12-26 17:44:50] {3343} INFO -  at 2.2s,\testimator lgbm's best error=0.7191,\tbest estimator lgbm's best error=0.7191\n",
            "INFO:flaml.automl: at 2.2s,\testimator lgbm's best error=0.7191,\tbest estimator lgbm's best error=0.7191\n",
            "[flaml.automl: 12-26 17:44:50] {3166} INFO - iteration 1, current learner lgbm\n",
            "INFO:flaml.automl:iteration 1, current learner lgbm\n",
            "[flaml.automl: 12-26 17:44:52] {3343} INFO -  at 3.9s,\testimator lgbm's best error=0.7184,\tbest estimator lgbm's best error=0.7184\n",
            "INFO:flaml.automl: at 3.9s,\testimator lgbm's best error=0.7184,\tbest estimator lgbm's best error=0.7184\n",
            "[flaml.automl: 12-26 17:44:52] {3166} INFO - iteration 2, current learner lgbm\n",
            "INFO:flaml.automl:iteration 2, current learner lgbm\n",
            "[flaml.automl: 12-26 17:44:54] {3343} INFO -  at 5.9s,\testimator lgbm's best error=0.7184,\tbest estimator lgbm's best error=0.7184\n",
            "INFO:flaml.automl: at 5.9s,\testimator lgbm's best error=0.7184,\tbest estimator lgbm's best error=0.7184\n",
            "[flaml.automl: 12-26 17:44:54] {3166} INFO - iteration 3, current learner lgbm\n",
            "INFO:flaml.automl:iteration 3, current learner lgbm\n",
            "[flaml.automl: 12-26 17:44:56] {3343} INFO -  at 8.2s,\testimator lgbm's best error=0.6801,\tbest estimator lgbm's best error=0.6801\n",
            "INFO:flaml.automl: at 8.2s,\testimator lgbm's best error=0.6801,\tbest estimator lgbm's best error=0.6801\n",
            "[flaml.automl: 12-26 17:44:56] {3166} INFO - iteration 4, current learner lgbm\n",
            "INFO:flaml.automl:iteration 4, current learner lgbm\n",
            "[flaml.automl: 12-26 17:44:58] {3343} INFO -  at 9.9s,\testimator lgbm's best error=0.6801,\tbest estimator lgbm's best error=0.6801\n",
            "INFO:flaml.automl: at 9.9s,\testimator lgbm's best error=0.6801,\tbest estimator lgbm's best error=0.6801\n",
            "[flaml.automl: 12-26 17:44:58] {3166} INFO - iteration 5, current learner lgbm\n",
            "INFO:flaml.automl:iteration 5, current learner lgbm\n",
            "[flaml.automl: 12-26 17:45:01] {3343} INFO -  at 13.0s,\testimator lgbm's best error=0.6467,\tbest estimator lgbm's best error=0.6467\n",
            "INFO:flaml.automl: at 13.0s,\testimator lgbm's best error=0.6467,\tbest estimator lgbm's best error=0.6467\n",
            "[flaml.automl: 12-26 17:45:01] {3166} INFO - iteration 6, current learner lgbm\n",
            "INFO:flaml.automl:iteration 6, current learner lgbm\n",
            "[flaml.automl: 12-26 17:45:03] {3343} INFO -  at 15.3s,\testimator lgbm's best error=0.6467,\tbest estimator lgbm's best error=0.6467\n",
            "INFO:flaml.automl: at 15.3s,\testimator lgbm's best error=0.6467,\tbest estimator lgbm's best error=0.6467\n",
            "[flaml.automl: 12-26 17:45:03] {3166} INFO - iteration 7, current learner lgbm\n",
            "INFO:flaml.automl:iteration 7, current learner lgbm\n",
            "[flaml.automl: 12-26 17:45:05] {3343} INFO -  at 17.3s,\testimator lgbm's best error=0.6467,\tbest estimator lgbm's best error=0.6467\n",
            "INFO:flaml.automl: at 17.3s,\testimator lgbm's best error=0.6467,\tbest estimator lgbm's best error=0.6467\n",
            "[flaml.automl: 12-26 17:45:05] {3166} INFO - iteration 8, current learner lgbm\n",
            "INFO:flaml.automl:iteration 8, current learner lgbm\n",
            "[flaml.automl: 12-26 17:45:31] {3343} INFO -  at 43.7s,\testimator lgbm's best error=0.6281,\tbest estimator lgbm's best error=0.6281\n",
            "INFO:flaml.automl: at 43.7s,\testimator lgbm's best error=0.6281,\tbest estimator lgbm's best error=0.6281\n",
            "[flaml.automl: 12-26 17:45:32] {3166} INFO - iteration 9, current learner lgbm\n",
            "INFO:flaml.automl:iteration 9, current learner lgbm\n",
            "[flaml.automl: 12-26 17:45:35] {3343} INFO -  at 46.8s,\testimator lgbm's best error=0.6281,\tbest estimator lgbm's best error=0.6281\n",
            "INFO:flaml.automl: at 46.8s,\testimator lgbm's best error=0.6281,\tbest estimator lgbm's best error=0.6281\n",
            "[flaml.automl: 12-26 17:45:35] {3166} INFO - iteration 10, current learner lgbm\n",
            "INFO:flaml.automl:iteration 10, current learner lgbm\n",
            "[flaml.automl: 12-26 17:47:10] {3343} INFO -  at 142.4s,\testimator lgbm's best error=0.5390,\tbest estimator lgbm's best error=0.5390\n",
            "INFO:flaml.automl: at 142.4s,\testimator lgbm's best error=0.5390,\tbest estimator lgbm's best error=0.5390\n",
            "[flaml.automl: 12-26 17:47:10] {3166} INFO - iteration 11, current learner lgbm\n",
            "INFO:flaml.automl:iteration 11, current learner lgbm\n",
            "[flaml.automl: 12-26 17:49:18] {3343} INFO -  at 270.5s,\testimator lgbm's best error=0.4604,\tbest estimator lgbm's best error=0.4604\n",
            "INFO:flaml.automl: at 270.5s,\testimator lgbm's best error=0.4604,\tbest estimator lgbm's best error=0.4604\n",
            "[flaml.automl: 12-26 17:49:18] {3166} INFO - iteration 12, current learner lgbm\n",
            "INFO:flaml.automl:iteration 12, current learner lgbm\n",
            "[flaml.automl: 12-26 17:50:52] {3343} INFO -  at 364.2s,\testimator lgbm's best error=0.4604,\tbest estimator lgbm's best error=0.4604\n",
            "INFO:flaml.automl: at 364.2s,\testimator lgbm's best error=0.4604,\tbest estimator lgbm's best error=0.4604\n",
            "[flaml.automl: 12-26 17:50:52] {3166} INFO - iteration 13, current learner lgbm\n",
            "INFO:flaml.automl:iteration 13, current learner lgbm\n",
            "[flaml.automl: 12-26 17:54:19] {3343} INFO -  at 571.2s,\testimator lgbm's best error=0.4604,\tbest estimator lgbm's best error=0.4604\n",
            "INFO:flaml.automl: at 571.2s,\testimator lgbm's best error=0.4604,\tbest estimator lgbm's best error=0.4604\n",
            "[flaml.automl: 12-26 17:54:19] {3166} INFO - iteration 14, current learner lgbm\n",
            "INFO:flaml.automl:iteration 14, current learner lgbm\n",
            "[flaml.automl: 12-26 17:55:55] {3343} INFO -  at 667.6s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "INFO:flaml.automl: at 667.6s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl: 12-26 17:55:55] {3166} INFO - iteration 15, current learner lgbm\n",
            "INFO:flaml.automl:iteration 15, current learner lgbm\n",
            "[flaml.automl: 12-26 17:57:50] {3343} INFO -  at 782.1s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "INFO:flaml.automl: at 782.1s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl: 12-26 17:57:50] {3166} INFO - iteration 16, current learner lgbm\n",
            "INFO:flaml.automl:iteration 16, current learner lgbm\n",
            "[flaml.automl: 12-26 17:58:53] {3343} INFO -  at 844.7s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "INFO:flaml.automl: at 844.7s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl: 12-26 17:58:53] {3166} INFO - iteration 17, current learner lgbm\n",
            "INFO:flaml.automl:iteration 17, current learner lgbm\n",
            "[flaml.automl: 12-26 18:01:05] {3343} INFO -  at 976.7s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "INFO:flaml.automl: at 976.7s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl: 12-26 18:01:05] {3166} INFO - iteration 18, current learner lgbm\n",
            "INFO:flaml.automl:iteration 18, current learner lgbm\n",
            "[flaml.automl: 12-26 18:02:24] {3343} INFO -  at 1056.0s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "INFO:flaml.automl: at 1056.0s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl: 12-26 18:02:24] {3166} INFO - iteration 19, current learner lgbm\n",
            "INFO:flaml.automl:iteration 19, current learner lgbm\n",
            "[flaml.automl: 12-26 18:03:15] {3343} INFO -  at 1106.8s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "INFO:flaml.automl: at 1106.8s,\testimator lgbm's best error=0.4585,\tbest estimator lgbm's best error=0.4585\n",
            "[flaml.automl: 12-26 18:03:15] {3166} INFO - iteration 20, current learner lgbm\n",
            "INFO:flaml.automl:iteration 20, current learner lgbm\n",
            "[flaml.automl: 12-26 18:06:08] {3343} INFO -  at 1280.4s,\testimator lgbm's best error=0.4455,\tbest estimator lgbm's best error=0.4455\n",
            "INFO:flaml.automl: at 1280.4s,\testimator lgbm's best error=0.4455,\tbest estimator lgbm's best error=0.4455\n",
            "[flaml.automl: 12-26 18:06:08] {3166} INFO - iteration 21, current learner lgbm\n",
            "INFO:flaml.automl:iteration 21, current learner lgbm\n",
            "[flaml.automl: 12-26 18:13:10] {3343} INFO -  at 1702.0s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "INFO:flaml.automl: at 1702.0s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "[flaml.automl: 12-26 18:13:10] {3166} INFO - iteration 22, current learner lgbm\n",
            "INFO:flaml.automl:iteration 22, current learner lgbm\n",
            "[flaml.automl: 12-26 18:16:03] {3343} INFO -  at 1875.7s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "INFO:flaml.automl: at 1875.7s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "[flaml.automl: 12-26 18:16:03] {3166} INFO - iteration 23, current learner lgbm\n",
            "INFO:flaml.automl:iteration 23, current learner lgbm\n",
            "[flaml.automl: 12-26 18:27:59] {3343} INFO -  at 2591.4s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "INFO:flaml.automl: at 2591.4s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "[flaml.automl: 12-26 18:27:59] {3166} INFO - iteration 24, current learner lgbm\n",
            "INFO:flaml.automl:iteration 24, current learner lgbm\n",
            "[flaml.automl: 12-26 18:30:16] {3343} INFO -  at 2728.1s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "INFO:flaml.automl: at 2728.1s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "[flaml.automl: 12-26 18:30:16] {3166} INFO - iteration 25, current learner lgbm\n",
            "INFO:flaml.automl:iteration 25, current learner lgbm\n",
            "[flaml.automl: 12-26 18:39:51] {3343} INFO -  at 3302.8s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "INFO:flaml.automl: at 3302.8s,\testimator lgbm's best error=0.4431,\tbest estimator lgbm's best error=0.4431\n",
            "[flaml.automl: 12-26 18:47:35] {3602} INFO - retrain lgbm for 463.9s\n",
            "INFO:flaml.automl:retrain lgbm for 463.9s\n",
            "[flaml.automl: 12-26 18:47:35] {3609} INFO - retrained model: LGBMClassifier(colsample_bytree=0.24489837298160008,\n",
            "               learning_rate=0.022776501888966183, max_bin=511,\n",
            "               min_child_samples=3, n_estimators=426, num_leaves=87,\n",
            "               reg_alpha=0.02046640007359354, reg_lambda=0.0032197962023686427,\n",
            "               verbose=-1)\n",
            "INFO:flaml.automl:retrained model: LGBMClassifier(colsample_bytree=0.24489837298160008,\n",
            "               learning_rate=0.022776501888966183, max_bin=511,\n",
            "               min_child_samples=3, n_estimators=426, num_leaves=87,\n",
            "               reg_alpha=0.02046640007359354, reg_lambda=0.0032197962023686427,\n",
            "               verbose=-1)\n",
            "[flaml.automl: 12-26 18:47:35] {2901} INFO - fit succeeded\n",
            "INFO:flaml.automl:fit succeeded\n",
            "[flaml.automl: 12-26 18:47:35] {2902} INFO - Time taken to find the best model: 1702.0308966636658\n",
            "INFO:flaml.automl:Time taken to find the best model: 1702.0308966636658\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparmeter config: {'n_estimators': 426, 'num_leaves': 87, 'min_child_samples': 3, 'learning_rate': 0.022776501888966183, 'log_max_bin': 9, 'colsample_bytree': 0.24489837298160008, 'reg_alpha': 0.02046640007359354, 'reg_lambda': 0.0032197962023686427}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1R6DEQB4NU"
      },
      "source": [
        "TfidfVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czTgM0s8B4NU",
        "outputId": "fefd5a07-11d6-4e5d-e1ff-344a5454b82e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[flaml.automl: 12-28 13:38:37] {2599} INFO - task = classification\n",
            "INFO:flaml.automl:task = classification\n",
            "[flaml.automl: 12-28 13:38:37] {2601} INFO - Data split method: stratified\n",
            "INFO:flaml.automl:Data split method: stratified\n",
            "[flaml.automl: 12-28 13:38:37] {2604} INFO - Evaluation method: holdout\n",
            "INFO:flaml.automl:Evaluation method: holdout\n",
            "[flaml.automl: 12-28 13:38:37] {2726} INFO - Minimizing error metric: 1-accuracy\n",
            "INFO:flaml.automl:Minimizing error metric: 1-accuracy\n",
            "[flaml.automl: 12-28 13:38:37] {2870} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
            "INFO:flaml.automl:List of ML learners in AutoML Run: ['lgbm']\n",
            "[flaml.automl: 12-28 13:38:37] {3166} INFO - iteration 0, current learner lgbm\n",
            "INFO:flaml.automl:iteration 0, current learner lgbm\n",
            "[flaml.automl: 12-28 13:38:41] {3296} INFO - Estimated sufficient time budget=34444s. Estimated necessary time budget=34s.\n",
            "INFO:flaml.automl:Estimated sufficient time budget=34444s. Estimated necessary time budget=34s.\n",
            "[flaml.automl: 12-28 13:38:41] {3343} INFO -  at 3.6s,\testimator lgbm's best error=0.7160,\tbest estimator lgbm's best error=0.7160\n",
            "INFO:flaml.automl: at 3.6s,\testimator lgbm's best error=0.7160,\tbest estimator lgbm's best error=0.7160\n",
            "[flaml.automl: 12-28 13:38:41] {3166} INFO - iteration 1, current learner lgbm\n",
            "INFO:flaml.automl:iteration 1, current learner lgbm\n",
            "[flaml.automl: 12-28 13:38:45] {3343} INFO -  at 8.1s,\testimator lgbm's best error=0.7160,\tbest estimator lgbm's best error=0.7160\n",
            "INFO:flaml.automl: at 8.1s,\testimator lgbm's best error=0.7160,\tbest estimator lgbm's best error=0.7160\n",
            "[flaml.automl: 12-28 13:38:45] {3166} INFO - iteration 2, current learner lgbm\n",
            "INFO:flaml.automl:iteration 2, current learner lgbm\n",
            "[flaml.automl: 12-28 13:38:49] {3343} INFO -  at 12.3s,\testimator lgbm's best error=0.6918,\tbest estimator lgbm's best error=0.6918\n",
            "INFO:flaml.automl: at 12.3s,\testimator lgbm's best error=0.6918,\tbest estimator lgbm's best error=0.6918\n",
            "[flaml.automl: 12-28 13:38:49] {3166} INFO - iteration 3, current learner lgbm\n",
            "INFO:flaml.automl:iteration 3, current learner lgbm\n",
            "[flaml.automl: 12-28 13:38:57] {3343} INFO -  at 20.1s,\testimator lgbm's best error=0.6553,\tbest estimator lgbm's best error=0.6553\n",
            "INFO:flaml.automl: at 20.1s,\testimator lgbm's best error=0.6553,\tbest estimator lgbm's best error=0.6553\n",
            "[flaml.automl: 12-28 13:38:57] {3166} INFO - iteration 4, current learner lgbm\n",
            "INFO:flaml.automl:iteration 4, current learner lgbm\n",
            "[flaml.automl: 12-28 13:39:01] {3343} INFO -  at 24.3s,\testimator lgbm's best error=0.6553,\tbest estimator lgbm's best error=0.6553\n",
            "INFO:flaml.automl: at 24.3s,\testimator lgbm's best error=0.6553,\tbest estimator lgbm's best error=0.6553\n",
            "[flaml.automl: 12-28 13:39:01] {3166} INFO - iteration 5, current learner lgbm\n",
            "INFO:flaml.automl:iteration 5, current learner lgbm\n",
            "[flaml.automl: 12-28 13:39:11] {3343} INFO -  at 34.3s,\testimator lgbm's best error=0.6306,\tbest estimator lgbm's best error=0.6306\n",
            "INFO:flaml.automl: at 34.3s,\testimator lgbm's best error=0.6306,\tbest estimator lgbm's best error=0.6306\n",
            "[flaml.automl: 12-28 13:39:11] {3166} INFO - iteration 6, current learner lgbm\n",
            "INFO:flaml.automl:iteration 6, current learner lgbm\n",
            "[flaml.automl: 12-28 13:39:19] {3343} INFO -  at 42.1s,\testimator lgbm's best error=0.6306,\tbest estimator lgbm's best error=0.6306\n",
            "INFO:flaml.automl: at 42.1s,\testimator lgbm's best error=0.6306,\tbest estimator lgbm's best error=0.6306\n",
            "[flaml.automl: 12-28 13:39:19] {3166} INFO - iteration 7, current learner lgbm\n",
            "INFO:flaml.automl:iteration 7, current learner lgbm\n",
            "[flaml.automl: 12-28 13:39:25] {3343} INFO -  at 48.1s,\testimator lgbm's best error=0.6306,\tbest estimator lgbm's best error=0.6306\n",
            "INFO:flaml.automl: at 48.1s,\testimator lgbm's best error=0.6306,\tbest estimator lgbm's best error=0.6306\n",
            "[flaml.automl: 12-28 13:39:25] {3166} INFO - iteration 8, current learner lgbm\n",
            "INFO:flaml.automl:iteration 8, current learner lgbm\n",
            "[flaml.automl: 12-28 13:39:45] {3343} INFO -  at 68.0s,\testimator lgbm's best error=0.5848,\tbest estimator lgbm's best error=0.5848\n",
            "INFO:flaml.automl: at 68.0s,\testimator lgbm's best error=0.5848,\tbest estimator lgbm's best error=0.5848\n",
            "[flaml.automl: 12-28 13:39:45] {3166} INFO - iteration 9, current learner lgbm\n",
            "INFO:flaml.automl:iteration 9, current learner lgbm\n",
            "[flaml.automl: 12-28 13:39:54] {3343} INFO -  at 77.3s,\testimator lgbm's best error=0.5848,\tbest estimator lgbm's best error=0.5848\n",
            "INFO:flaml.automl: at 77.3s,\testimator lgbm's best error=0.5848,\tbest estimator lgbm's best error=0.5848\n",
            "[flaml.automl: 12-28 13:39:54] {3166} INFO - iteration 10, current learner lgbm\n",
            "INFO:flaml.automl:iteration 10, current learner lgbm\n",
            "[flaml.automl: 12-28 13:42:41] {3343} INFO -  at 244.2s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "INFO:flaml.automl: at 244.2s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "[flaml.automl: 12-28 13:42:41] {3166} INFO - iteration 11, current learner lgbm\n",
            "INFO:flaml.automl:iteration 11, current learner lgbm\n",
            "[flaml.automl: 12-28 13:46:48] {3343} INFO -  at 490.6s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "INFO:flaml.automl: at 490.6s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "[flaml.automl: 12-28 13:46:48] {3166} INFO - iteration 12, current learner lgbm\n",
            "INFO:flaml.automl:iteration 12, current learner lgbm\n",
            "[flaml.automl: 12-28 13:48:28] {3343} INFO -  at 590.8s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "INFO:flaml.automl: at 590.8s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "[flaml.automl: 12-28 13:48:28] {3166} INFO - iteration 13, current learner lgbm\n",
            "INFO:flaml.automl:iteration 13, current learner lgbm\n",
            "[flaml.automl: 12-28 13:49:56] {3343} INFO -  at 679.3s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "INFO:flaml.automl: at 679.3s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "[flaml.automl: 12-28 13:49:56] {3166} INFO - iteration 14, current learner lgbm\n",
            "INFO:flaml.automl:iteration 14, current learner lgbm\n",
            "[flaml.automl: 12-28 13:54:56] {3343} INFO -  at 978.5s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "INFO:flaml.automl: at 978.5s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "[flaml.automl: 12-28 13:54:56] {3166} INFO - iteration 15, current learner lgbm\n",
            "INFO:flaml.automl:iteration 15, current learner lgbm\n",
            "[flaml.automl: 12-28 13:57:03] {3343} INFO -  at 1105.4s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "INFO:flaml.automl: at 1105.4s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "[flaml.automl: 12-28 13:57:03] {3166} INFO - iteration 16, current learner lgbm\n",
            "INFO:flaml.automl:iteration 16, current learner lgbm\n",
            "[flaml.automl: 12-28 14:00:35] {3343} INFO -  at 1317.7s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "INFO:flaml.automl: at 1317.7s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "[flaml.automl: 12-28 14:00:35] {3166} INFO - iteration 17, current learner lgbm\n",
            "INFO:flaml.automl:iteration 17, current learner lgbm\n",
            "[flaml.automl: 12-28 14:08:24] {3343} INFO -  at 1786.8s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "INFO:flaml.automl: at 1786.8s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "[flaml.automl: 12-28 14:08:24] {3166} INFO - iteration 18, current learner lgbm\n",
            "INFO:flaml.automl:iteration 18, current learner lgbm\n",
            "[flaml.automl: 12-28 14:12:01] {3343} INFO -  at 2004.0s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "INFO:flaml.automl: at 2004.0s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "[flaml.automl: 12-28 14:12:01] {3166} INFO - iteration 19, current learner lgbm\n",
            "INFO:flaml.automl:iteration 19, current learner lgbm\n",
            "[flaml.automl: 12-28 14:12:52] {3343} INFO -  at 2055.3s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "INFO:flaml.automl: at 2055.3s,\testimator lgbm's best error=0.4783,\tbest estimator lgbm's best error=0.4783\n",
            "[flaml.automl: 12-28 14:12:52] {3166} INFO - iteration 20, current learner lgbm\n",
            "INFO:flaml.automl:iteration 20, current learner lgbm\n",
            "[flaml.automl: 12-28 14:24:23] {3343} INFO -  at 2745.6s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "INFO:flaml.automl: at 2745.6s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "[flaml.automl: 12-28 14:24:23] {3166} INFO - iteration 21, current learner lgbm\n",
            "INFO:flaml.automl:iteration 21, current learner lgbm\n",
            "[flaml.automl: 12-28 14:45:18] {3343} INFO -  at 4001.2s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "INFO:flaml.automl: at 4001.2s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "[flaml.automl: 12-28 14:45:18] {3166} INFO - iteration 22, current learner lgbm\n",
            "INFO:flaml.automl:iteration 22, current learner lgbm\n",
            "[flaml.automl: 12-28 14:55:11] {3343} INFO -  at 4594.2s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "INFO:flaml.automl: at 4594.2s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "[flaml.automl: 12-28 14:55:11] {3166} INFO - iteration 23, current learner lgbm\n",
            "INFO:flaml.automl:iteration 23, current learner lgbm\n",
            "[flaml.automl: 12-28 15:02:48] {3343} INFO -  at 5050.8s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "INFO:flaml.automl: at 5050.8s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "[flaml.automl: 12-28 15:25:32] {3602} INFO - retrain lgbm for 1364.0s\n",
            "INFO:flaml.automl:retrain lgbm for 1364.0s\n",
            "[flaml.automl: 12-28 15:25:32] {3609} INFO - retrained model: LGBMClassifier(colsample_bytree=0.5680522969353233,\n",
            "               learning_rate=0.0697500610424688, max_bin=255,\n",
            "               min_child_samples=2, n_estimators=518, num_leaves=17,\n",
            "               reg_alpha=0.004577823970660193, reg_lambda=0.022292694161977773,\n",
            "               verbose=-1)\n",
            "INFO:flaml.automl:retrained model: LGBMClassifier(colsample_bytree=0.5680522969353233,\n",
            "               learning_rate=0.0697500610424688, max_bin=255,\n",
            "               min_child_samples=2, n_estimators=518, num_leaves=17,\n",
            "               reg_alpha=0.004577823970660193, reg_lambda=0.022292694161977773,\n",
            "               verbose=-1)\n",
            "[flaml.automl: 12-28 15:25:32] {2901} INFO - fit succeeded\n",
            "INFO:flaml.automl:fit succeeded\n",
            "[flaml.automl: 12-28 15:25:32] {2902} INFO - Time taken to find the best model: 4001.1676032543182\n",
            "INFO:flaml.automl:Time taken to find the best model: 4001.1676032543182\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparmeter config: {'n_estimators': 518, 'num_leaves': 17, 'min_child_samples': 2, 'learning_rate': 0.0697500610424688, 'log_max_bin': 8, 'colsample_bytree': 0.5680522969353233, 'reg_alpha': 0.004577823970660193, 'reg_lambda': 0.022292694161977773}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=6000)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTjuXUEWB4NU"
      },
      "source": [
        "Spacy-Vektoren:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y00uqUnJB4NU"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "X = [tokenize_and_lemmatize(text) for text in documents]\n",
        "\n",
        "def document_vector(doc):\n",
        "    return np.mean([token.vector for token in doc], axis=0)\n",
        "X = [document_vector(nlp(text)) for text in documents]\n",
        "\n",
        "X = np.array(X)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlv8kQ_3xqDM"
      },
      "outputs": [],
      "source": [
        "# Dokumente und Labels\n",
        "documents = df['wosw_lyrics'].tolist()\n",
        "labels = df['Genre']\n",
        "\n",
        "# Tokenisieren und Lemmatisieren der Dokumente\n",
        "def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "X = [tokenize_and_lemmatize(text) for text in documents]\n",
        "\n",
        "# Erstellen von Dokumentenvektoren\n",
        "def document_vector(doc):\n",
        "    return np.mean([token.vector for token in doc], axis=0)\n",
        "\n",
        "X = [document_vector(nlp(text)) for text in documents]\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testdaten\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen des AutoML-Objekts\n",
        "automl = AutoML(model_type='lightgbm')\n",
        "\n",
        "# Training des Modells\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Beste Parameter finden\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Nm8HxkuA1Sx"
      },
      "source": [
        "# Einstellen, welche Lyrik-Spalten verwendet werden sollen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk8jqrqaBCFh"
      },
      "outputs": [],
      "source": [
        "# Laden Sie die Textdokumente und die Klassenlabels\n",
        "documents = df['stemmed_wosw_lyrics'].tolist() # Liste von Textdokumenten\n",
        "labels = df['Genre'] # Liste von Klassenlabels (eins pro Dokument)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aLj0O1HBdG3"
      },
      "source": [
        "# 4. Grid-search SVM mit SKLEARN:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxBx1RIsBdG3"
      },
      "source": [
        "WIE ICH IN ZUKUNFT DIE OPTIMALEN PARAMETER RAUSFINDE:\n",
        "Ich taste mich erstmal mit den jetztigen ran, schau mir die optimalen an, dann\n",
        "werde ich eine Reihe von Paramtern die ungefähr um den optimalen Parameter liegen testen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLp4YSj9BdG3"
      },
      "source": [
        "CountVektorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ9ekm8rBdG3",
        "outputId": "4bea4f1e-ffff-4137-b16a-b2228dd20219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.226 total time= 2.8min\n",
            "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.228 total time= 2.8min\n",
            "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.213 total time= 2.8min\n",
            "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.226 total time= 2.8min\n",
            "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.218 total time= 2.8min\n",
            "[CV 1/5] END .......C=1, gamma=1, kernel=linear;, score=0.432 total time= 2.0min\n",
            "[CV 2/5] END .......C=1, gamma=1, kernel=linear;, score=0.437 total time= 2.0min\n",
            "[CV 3/5] END .......C=1, gamma=1, kernel=linear;, score=0.422 total time= 2.0min\n",
            "[CV 4/5] END .......C=1, gamma=1, kernel=linear;, score=0.428 total time= 2.1min\n",
            "[CV 5/5] END .......C=1, gamma=1, kernel=linear;, score=0.424 total time= 2.0min\n",
            "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.240 total time= 2.8min\n",
            "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.241 total time= 2.9min\n",
            "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.229 total time= 3.1min\n",
            "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.237 total time= 3.0min\n",
            "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.233 total time= 3.1min\n",
            "[CV 1/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.432 total time= 2.1min\n",
            "[CV 2/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.437 total time= 2.1min\n",
            "[CV 3/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.422 total time= 2.1min\n",
            "[CV 4/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.428 total time= 2.1min\n",
            "[CV 5/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.424 total time= 2.1min\n",
            "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.360 total time= 2.8min\n",
            "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.345 total time= 2.8min\n",
            "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.356 total time= 2.8min\n",
            "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.374 total time= 2.8min\n",
            "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.357 total time= 2.8min\n",
            "[CV 1/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.432 total time= 2.0min\n",
            "[CV 2/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.437 total time= 2.1min\n",
            "[CV 3/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.422 total time= 2.1min\n",
            "[CV 4/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.428 total time= 2.1min\n",
            "[CV 5/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.424 total time= 2.1min\n",
            "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=0.227 total time= 2.8min\n",
            "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=0.228 total time= 2.9min\n",
            "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=0.213 total time= 3.1min\n",
            "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=0.226 total time= 2.9min\n",
            "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=0.220 total time= 2.8min\n",
            "[CV 1/5] END ......C=10, gamma=1, kernel=linear;, score=0.428 total time= 2.1min\n",
            "[CV 2/5] END ......C=10, gamma=1, kernel=linear;, score=0.425 total time= 2.0min\n",
            "[CV 3/5] END ......C=10, gamma=1, kernel=linear;, score=0.411 total time= 2.1min\n",
            "[CV 4/5] END ......C=10, gamma=1, kernel=linear;, score=0.408 total time= 2.1min\n",
            "[CV 5/5] END ......C=10, gamma=1, kernel=linear;, score=0.406 total time= 2.1min\n",
            "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.243 total time= 2.7min\n",
            "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.241 total time= 2.7min\n",
            "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.229 total time= 2.7min\n",
            "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.237 total time= 2.7min\n",
            "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.231 total time= 2.7min\n",
            "[CV 1/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.428 total time= 2.1min\n",
            "[CV 2/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.425 total time= 2.0min\n",
            "[CV 3/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.411 total time= 2.1min\n",
            "[CV 4/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.408 total time= 2.2min\n",
            "[CV 5/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.406 total time= 2.2min\n",
            "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.379 total time= 3.1min\n",
            "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.349 total time= 2.9min\n",
            "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.364 total time= 2.9min\n",
            "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.380 total time= 2.9min\n",
            "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.366 total time= 2.9min\n",
            "[CV 1/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.428 total time= 2.1min\n",
            "[CV 2/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.425 total time= 2.1min\n",
            "[CV 3/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.411 total time= 2.1min\n",
            "[CV 4/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.408 total time= 2.2min\n",
            "[CV 5/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.406 total time= 2.2min\n",
            "[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=0.227 total time= 3.0min\n",
            "[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=0.228 total time= 2.9min\n",
            "[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=0.213 total time= 2.9min\n",
            "[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=0.226 total time= 2.8min\n",
            "[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=0.220 total time= 2.8min\n",
            "[CV 1/5] END .....C=100, gamma=1, kernel=linear;, score=0.428 total time= 2.0min\n",
            "[CV 2/5] END .....C=100, gamma=1, kernel=linear;, score=0.426 total time= 2.1min\n",
            "[CV 3/5] END .....C=100, gamma=1, kernel=linear;, score=0.406 total time= 2.1min\n",
            "[CV 4/5] END .....C=100, gamma=1, kernel=linear;, score=0.406 total time= 2.1min\n",
            "[CV 5/5] END .....C=100, gamma=1, kernel=linear;, score=0.407 total time= 2.0min\n",
            "[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.243 total time= 2.8min\n",
            "[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.241 total time= 2.8min\n",
            "[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.229 total time= 2.8min\n",
            "[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.237 total time= 2.8min\n",
            "[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.231 total time= 2.8min\n",
            "[CV 1/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.428 total time= 2.1min\n",
            "[CV 2/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.426 total time= 2.2min\n",
            "[CV 3/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.406 total time= 2.2min\n",
            "[CV 4/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.406 total time= 2.3min\n",
            "[CV 5/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.407 total time= 2.2min\n",
            "[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.381 total time= 3.0min\n",
            "[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.350 total time= 2.9min\n",
            "[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.363 total time= 2.8min\n",
            "[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.382 total time= 2.8min\n",
            "[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.365 total time= 2.8min\n",
            "[CV 1/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.428 total time= 2.1min\n",
            "[CV 2/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.426 total time= 2.1min\n",
            "[CV 3/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.406 total time= 2.1min\n",
            "[CV 4/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.406 total time= 2.1min\n",
            "[CV 5/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.407 total time= 2.1min\n",
            "{'C': 1, 'gamma': 1, 'kernel': 'linear'}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [1, 10, 100], \n",
        "                  'gamma': [1, 0.1, 0.01],\n",
        "                  'kernel': ['rbf', 'linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APy6cnfKBdG4"
      },
      "source": [
        "SVM mit TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY03GzH8BdG4",
        "outputId": "1f3b9502-a4ce-4f02-9942-de21c556424f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.480 total time= 2.7min\n",
            "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.462 total time= 2.7min\n",
            "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.482 total time= 2.7min\n",
            "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.483 total time= 2.7min\n",
            "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.471 total time= 2.7min\n",
            "[CV 1/5] END .......C=1, gamma=1, kernel=linear;, score=0.462 total time= 2.0min\n",
            "[CV 2/5] END .......C=1, gamma=1, kernel=linear;, score=0.443 total time= 2.0min\n",
            "[CV 3/5] END .......C=1, gamma=1, kernel=linear;, score=0.456 total time= 2.0min\n",
            "[CV 4/5] END .......C=1, gamma=1, kernel=linear;, score=0.452 total time= 2.0min\n",
            "[CV 5/5] END .......C=1, gamma=1, kernel=linear;, score=0.449 total time= 2.0min\n",
            "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.395 total time= 2.5min\n",
            "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.385 total time= 2.5min\n",
            "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.400 total time= 2.5min\n",
            "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.410 total time= 2.5min\n",
            "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.403 total time= 2.5min\n",
            "[CV 1/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.462 total time= 2.0min\n",
            "[CV 2/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.443 total time= 2.0min\n",
            "[CV 3/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.456 total time= 2.0min\n",
            "[CV 4/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.452 total time= 2.0min\n",
            "[CV 5/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.449 total time= 2.0min\n",
            "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.124 total time= 2.9min\n",
            "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.126 total time= 2.9min\n",
            "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.126 total time= 2.9min\n",
            "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.128 total time= 2.9min\n",
            "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.128 total time= 2.9min\n",
            "[CV 1/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.462 total time= 2.0min\n",
            "[CV 2/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.443 total time= 2.0min\n",
            "[CV 3/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.456 total time= 2.0min\n",
            "[CV 4/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.452 total time= 2.0min\n",
            "[CV 5/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.449 total time= 2.0min\n",
            "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=0.493 total time= 2.8min\n",
            "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=0.483 total time= 2.8min\n",
            "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=0.487 total time= 2.7min\n",
            "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=0.483 total time= 2.7min\n",
            "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=0.474 total time= 2.7min\n",
            "[CV 1/5] END ......C=10, gamma=1, kernel=linear;, score=0.449 total time= 2.2min\n",
            "[CV 2/5] END ......C=10, gamma=1, kernel=linear;, score=0.437 total time= 2.2min\n",
            "[CV 3/5] END ......C=10, gamma=1, kernel=linear;, score=0.443 total time= 2.3min\n",
            "[CV 4/5] END ......C=10, gamma=1, kernel=linear;, score=0.443 total time= 2.2min\n",
            "[CV 5/5] END ......C=10, gamma=1, kernel=linear;, score=0.444 total time= 2.3min\n",
            "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.476 total time= 2.4min\n",
            "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.457 total time= 2.3min\n",
            "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.467 total time= 2.3min\n",
            "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.462 total time= 2.4min\n",
            "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.451 total time= 2.3min\n",
            "[CV 1/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.449 total time= 2.3min\n",
            "[CV 2/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.437 total time= 2.3min\n",
            "[CV 3/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.443 total time= 2.3min\n",
            "[CV 4/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.443 total time= 2.3min\n",
            "[CV 5/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.444 total time= 2.2min\n",
            "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.400 total time= 2.5min\n",
            "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.392 total time= 2.5min\n",
            "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.414 total time= 2.5min\n",
            "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.418 total time= 2.5min\n",
            "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.412 total time= 2.5min\n",
            "[CV 1/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.449 total time= 2.3min\n",
            "[CV 2/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.437 total time= 2.3min\n",
            "[CV 3/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.443 total time= 2.3min\n",
            "[CV 4/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.443 total time= 2.3min\n",
            "[CV 5/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.444 total time= 2.2min\n",
            "[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=0.493 total time= 2.8min\n",
            "[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=0.480 total time= 2.7min\n",
            "[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=0.486 total time= 2.7min\n",
            "[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=0.482 total time= 2.8min\n",
            "[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=0.474 total time= 2.7min\n",
            "[CV 1/5] END .....C=100, gamma=1, kernel=linear;, score=0.448 total time= 2.3min\n",
            "[CV 2/5] END .....C=100, gamma=1, kernel=linear;, score=0.441 total time= 2.2min\n",
            "[CV 3/5] END .....C=100, gamma=1, kernel=linear;, score=0.439 total time= 2.3min\n",
            "[CV 4/5] END .....C=100, gamma=1, kernel=linear;, score=0.442 total time= 2.3min\n",
            "[CV 5/5] END .....C=100, gamma=1, kernel=linear;, score=0.437 total time= 2.3min\n",
            "[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.466 total time= 2.5min\n",
            "[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.446 total time= 2.5min\n",
            "[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.454 total time= 2.5min\n",
            "[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.456 total time= 2.5min\n",
            "[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.450 total time= 2.5min\n",
            "[CV 1/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.448 total time= 2.3min\n",
            "[CV 2/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.441 total time= 2.3min\n",
            "[CV 3/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.439 total time= 2.3min\n",
            "[CV 4/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.442 total time= 2.3min\n",
            "[CV 5/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.437 total time= 2.3min\n",
            "[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.467 total time= 2.2min\n",
            "[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.449 total time= 2.2min\n",
            "[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.456 total time= 2.2min\n",
            "[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.456 total time= 2.2min\n",
            "[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.443 total time= 2.2min\n",
            "[CV 1/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.448 total time= 2.3min\n",
            "[CV 2/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.441 total time= 2.2min\n",
            "[CV 3/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.439 total time= 2.3min\n",
            "[CV 4/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.442 total time= 2.3min\n",
            "[CV 5/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.437 total time= 2.3min\n",
            "{'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [1, 10, 100], \n",
        "                  'gamma': [1, 0.1, 0.01],\n",
        "                  'kernel': ['rbf', 'linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sOG5SPZBdG4"
      },
      "source": [
        "Spacy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avhg8KvnBdG4"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "X = [tokenize_and_lemmatize(text) for text in documents]\n",
        "\n",
        "def document_vector(doc):\n",
        "    return np.mean([token.vector for token in doc], axis=0)\n",
        "\n",
        "X = [document_vector(nlp(text)) for text in documents]\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [1, 10, 100], \n",
        "                  'gamma': [1, 0.1, 0.01],\n",
        "                  'kernel': ['rbf', 'linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzun1qzrB4aq"
      },
      "source": [
        "# 4. Gridsearch LGBM mit FLAML:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpyjc46cB4ar"
      },
      "source": [
        "CountVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NfgwoOdB4ar",
        "outputId": "f45e8d14-d021-40d2-e2d6-1b70e2912974"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[flaml.automl: 12-27 12:10:21] {2599} INFO - task = classification\n",
            "INFO:flaml.automl:task = classification\n",
            "[flaml.automl: 12-27 12:10:21] {2601} INFO - Data split method: stratified\n",
            "INFO:flaml.automl:Data split method: stratified\n",
            "[flaml.automl: 12-27 12:10:21] {2604} INFO - Evaluation method: holdout\n",
            "INFO:flaml.automl:Evaluation method: holdout\n",
            "[flaml.automl: 12-27 12:10:21] {2726} INFO - Minimizing error metric: 1-accuracy\n",
            "INFO:flaml.automl:Minimizing error metric: 1-accuracy\n",
            "[flaml.automl: 12-27 12:10:21] {2870} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
            "INFO:flaml.automl:List of ML learners in AutoML Run: ['lgbm']\n",
            "[flaml.automl: 12-27 12:10:21] {3166} INFO - iteration 0, current learner lgbm\n",
            "INFO:flaml.automl:iteration 0, current learner lgbm\n",
            "[flaml.automl: 12-27 12:10:23] {3296} INFO - Estimated sufficient time budget=15334s. Estimated necessary time budget=15s.\n",
            "INFO:flaml.automl:Estimated sufficient time budget=15334s. Estimated necessary time budget=15s.\n",
            "[flaml.automl: 12-27 12:10:23] {3343} INFO -  at 1.6s,\testimator lgbm's best error=0.7085,\tbest estimator lgbm's best error=0.7085\n",
            "INFO:flaml.automl: at 1.6s,\testimator lgbm's best error=0.7085,\tbest estimator lgbm's best error=0.7085\n",
            "[flaml.automl: 12-27 12:10:23] {3166} INFO - iteration 1, current learner lgbm\n",
            "INFO:flaml.automl:iteration 1, current learner lgbm\n",
            "[flaml.automl: 12-27 12:10:24] {3343} INFO -  at 2.9s,\testimator lgbm's best error=0.7085,\tbest estimator lgbm's best error=0.7085\n",
            "INFO:flaml.automl: at 2.9s,\testimator lgbm's best error=0.7085,\tbest estimator lgbm's best error=0.7085\n",
            "[flaml.automl: 12-27 12:10:24] {3166} INFO - iteration 2, current learner lgbm\n",
            "INFO:flaml.automl:iteration 2, current learner lgbm\n",
            "[flaml.automl: 12-27 12:10:26] {3343} INFO -  at 4.9s,\testimator lgbm's best error=0.6850,\tbest estimator lgbm's best error=0.6850\n",
            "INFO:flaml.automl: at 4.9s,\testimator lgbm's best error=0.6850,\tbest estimator lgbm's best error=0.6850\n",
            "[flaml.automl: 12-27 12:10:26] {3166} INFO - iteration 3, current learner lgbm\n",
            "INFO:flaml.automl:iteration 3, current learner lgbm\n",
            "[flaml.automl: 12-27 12:10:28] {3343} INFO -  at 7.4s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "INFO:flaml.automl: at 7.4s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "[flaml.automl: 12-27 12:10:28] {3166} INFO - iteration 4, current learner lgbm\n",
            "INFO:flaml.automl:iteration 4, current learner lgbm\n",
            "[flaml.automl: 12-27 12:10:30] {3343} INFO -  at 9.4s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "INFO:flaml.automl: at 9.4s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "[flaml.automl: 12-27 12:10:30] {3166} INFO - iteration 5, current learner lgbm\n",
            "INFO:flaml.automl:iteration 5, current learner lgbm\n",
            "[flaml.automl: 12-27 12:10:34] {3343} INFO -  at 13.1s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "INFO:flaml.automl: at 13.1s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "[flaml.automl: 12-27 12:10:34] {3166} INFO - iteration 6, current learner lgbm\n",
            "INFO:flaml.automl:iteration 6, current learner lgbm\n",
            "[flaml.automl: 12-27 12:10:36] {3343} INFO -  at 15.1s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "INFO:flaml.automl: at 15.1s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "[flaml.automl: 12-27 12:10:36] {3166} INFO - iteration 7, current learner lgbm\n",
            "INFO:flaml.automl:iteration 7, current learner lgbm\n",
            "[flaml.automl: 12-27 12:10:38] {3343} INFO -  at 17.2s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "INFO:flaml.automl: at 17.2s,\testimator lgbm's best error=0.6380,\tbest estimator lgbm's best error=0.6380\n",
            "[flaml.automl: 12-27 12:10:38] {3166} INFO - iteration 8, current learner lgbm\n",
            "INFO:flaml.automl:iteration 8, current learner lgbm\n",
            "[flaml.automl: 12-27 12:10:43] {3343} INFO -  at 21.6s,\testimator lgbm's best error=0.5922,\tbest estimator lgbm's best error=0.5922\n",
            "INFO:flaml.automl: at 21.6s,\testimator lgbm's best error=0.5922,\tbest estimator lgbm's best error=0.5922\n",
            "[flaml.automl: 12-27 12:10:43] {3166} INFO - iteration 9, current learner lgbm\n",
            "INFO:flaml.automl:iteration 9, current learner lgbm\n",
            "[flaml.automl: 12-27 12:10:45] {3343} INFO -  at 24.4s,\testimator lgbm's best error=0.5922,\tbest estimator lgbm's best error=0.5922\n",
            "INFO:flaml.automl: at 24.4s,\testimator lgbm's best error=0.5922,\tbest estimator lgbm's best error=0.5922\n",
            "[flaml.automl: 12-27 12:10:45] {3166} INFO - iteration 10, current learner lgbm\n",
            "INFO:flaml.automl:iteration 10, current learner lgbm\n",
            "[flaml.automl: 12-27 12:11:03] {3343} INFO -  at 42.1s,\testimator lgbm's best error=0.4802,\tbest estimator lgbm's best error=0.4802\n",
            "INFO:flaml.automl: at 42.1s,\testimator lgbm's best error=0.4802,\tbest estimator lgbm's best error=0.4802\n",
            "[flaml.automl: 12-27 12:11:03] {3166} INFO - iteration 11, current learner lgbm\n",
            "INFO:flaml.automl:iteration 11, current learner lgbm\n",
            "[flaml.automl: 12-27 12:11:18] {3343} INFO -  at 57.3s,\testimator lgbm's best error=0.4802,\tbest estimator lgbm's best error=0.4802\n",
            "INFO:flaml.automl: at 57.3s,\testimator lgbm's best error=0.4802,\tbest estimator lgbm's best error=0.4802\n",
            "[flaml.automl: 12-27 12:11:18] {3166} INFO - iteration 12, current learner lgbm\n",
            "INFO:flaml.automl:iteration 12, current learner lgbm\n",
            "[flaml.automl: 12-27 12:11:32] {3343} INFO -  at 70.9s,\testimator lgbm's best error=0.4802,\tbest estimator lgbm's best error=0.4802\n",
            "INFO:flaml.automl: at 70.9s,\testimator lgbm's best error=0.4802,\tbest estimator lgbm's best error=0.4802\n",
            "[flaml.automl: 12-27 12:11:32] {3166} INFO - iteration 13, current learner lgbm\n",
            "INFO:flaml.automl:iteration 13, current learner lgbm\n",
            "[flaml.automl: 12-27 12:11:45] {3343} INFO -  at 83.8s,\testimator lgbm's best error=0.4802,\tbest estimator lgbm's best error=0.4802\n",
            "INFO:flaml.automl: at 83.8s,\testimator lgbm's best error=0.4802,\tbest estimator lgbm's best error=0.4802\n",
            "[flaml.automl: 12-27 12:11:45] {3166} INFO - iteration 14, current learner lgbm\n",
            "INFO:flaml.automl:iteration 14, current learner lgbm\n",
            "[flaml.automl: 12-27 12:12:15] {3343} INFO -  at 113.9s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "INFO:flaml.automl: at 113.9s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "[flaml.automl: 12-27 12:12:15] {3166} INFO - iteration 15, current learner lgbm\n",
            "INFO:flaml.automl:iteration 15, current learner lgbm\n",
            "[flaml.automl: 12-27 12:12:49] {3343} INFO -  at 148.4s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "INFO:flaml.automl: at 148.4s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "[flaml.automl: 12-27 12:12:49] {3166} INFO - iteration 16, current learner lgbm\n",
            "INFO:flaml.automl:iteration 16, current learner lgbm\n",
            "[flaml.automl: 12-27 12:13:19] {3343} INFO -  at 177.7s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "INFO:flaml.automl: at 177.7s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "[flaml.automl: 12-27 12:13:19] {3166} INFO - iteration 17, current learner lgbm\n",
            "INFO:flaml.automl:iteration 17, current learner lgbm\n",
            "[flaml.automl: 12-27 12:14:02] {3343} INFO -  at 220.7s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "INFO:flaml.automl: at 220.7s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "[flaml.automl: 12-27 12:14:02] {3166} INFO - iteration 18, current learner lgbm\n",
            "INFO:flaml.automl:iteration 18, current learner lgbm\n",
            "[flaml.automl: 12-27 12:14:37] {3343} INFO -  at 255.8s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "INFO:flaml.automl: at 255.8s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "[flaml.automl: 12-27 12:14:37] {3166} INFO - iteration 19, current learner lgbm\n",
            "INFO:flaml.automl:iteration 19, current learner lgbm\n",
            "[flaml.automl: 12-27 12:14:47] {3343} INFO -  at 266.1s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "INFO:flaml.automl: at 266.1s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "[flaml.automl: 12-27 12:14:47] {3166} INFO - iteration 20, current learner lgbm\n",
            "INFO:flaml.automl:iteration 20, current learner lgbm\n",
            "[flaml.automl: 12-27 12:17:37] {3343} INFO -  at 436.4s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "INFO:flaml.automl: at 436.4s,\testimator lgbm's best error=0.4722,\tbest estimator lgbm's best error=0.4722\n",
            "[flaml.automl: 12-27 12:17:37] {3166} INFO - iteration 21, current learner lgbm\n",
            "INFO:flaml.automl:iteration 21, current learner lgbm\n",
            "[flaml.automl: 12-27 12:18:23] {3343} INFO -  at 481.7s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 481.7s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:18:23] {3166} INFO - iteration 22, current learner lgbm\n",
            "INFO:flaml.automl:iteration 22, current learner lgbm\n",
            "[flaml.automl: 12-27 12:19:01] {3343} INFO -  at 520.3s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 520.3s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:19:01] {3166} INFO - iteration 23, current learner lgbm\n",
            "INFO:flaml.automl:iteration 23, current learner lgbm\n",
            "[flaml.automl: 12-27 12:21:18] {3343} INFO -  at 656.9s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 656.9s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:21:18] {3166} INFO - iteration 24, current learner lgbm\n",
            "INFO:flaml.automl:iteration 24, current learner lgbm\n",
            "[flaml.automl: 12-27 12:22:49] {3343} INFO -  at 748.1s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 748.1s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:22:49] {3166} INFO - iteration 25, current learner lgbm\n",
            "INFO:flaml.automl:iteration 25, current learner lgbm\n",
            "[flaml.automl: 12-27 12:23:06] {3343} INFO -  at 765.1s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 765.1s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:23:06] {3166} INFO - iteration 26, current learner lgbm\n",
            "INFO:flaml.automl:iteration 26, current learner lgbm\n",
            "[flaml.automl: 12-27 12:24:58] {3343} INFO -  at 877.6s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 877.6s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:24:58] {3166} INFO - iteration 27, current learner lgbm\n",
            "INFO:flaml.automl:iteration 27, current learner lgbm\n",
            "[flaml.automl: 12-27 12:25:13] {3343} INFO -  at 892.1s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 892.1s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:25:13] {3166} INFO - iteration 28, current learner lgbm\n",
            "INFO:flaml.automl:iteration 28, current learner lgbm\n",
            "[flaml.automl: 12-27 12:25:30] {3343} INFO -  at 908.9s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 908.9s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:25:30] {3166} INFO - iteration 29, current learner lgbm\n",
            "INFO:flaml.automl:iteration 29, current learner lgbm\n",
            "[flaml.automl: 12-27 12:26:26] {3343} INFO -  at 965.1s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 965.1s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:26:26] {3166} INFO - iteration 30, current learner lgbm\n",
            "INFO:flaml.automl:iteration 30, current learner lgbm\n",
            "[flaml.automl: 12-27 12:27:08] {3343} INFO -  at 1007.0s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 1007.0s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:27:08] {3166} INFO - iteration 31, current learner lgbm\n",
            "INFO:flaml.automl:iteration 31, current learner lgbm\n",
            "[flaml.automl: 12-27 12:27:51] {3343} INFO -  at 1050.5s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 1050.5s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:27:51] {3166} INFO - iteration 32, current learner lgbm\n",
            "INFO:flaml.automl:iteration 32, current learner lgbm\n",
            "[flaml.automl: 12-27 12:29:02] {3343} INFO -  at 1120.9s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 1120.9s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:29:02] {3166} INFO - iteration 33, current learner lgbm\n",
            "INFO:flaml.automl:iteration 33, current learner lgbm\n",
            "[flaml.automl: 12-27 12:29:35] {3343} INFO -  at 1154.1s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 1154.1s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:29:35] {3166} INFO - iteration 34, current learner lgbm\n",
            "INFO:flaml.automl:iteration 34, current learner lgbm\n",
            "[flaml.automl: 12-27 12:32:17] {3343} INFO -  at 1315.9s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 1315.9s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:32:17] {3166} INFO - iteration 35, current learner lgbm\n",
            "INFO:flaml.automl:iteration 35, current learner lgbm\n",
            "[flaml.automl: 12-27 12:32:28] {3343} INFO -  at 1326.7s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "INFO:flaml.automl: at 1326.7s,\testimator lgbm's best error=0.4598,\tbest estimator lgbm's best error=0.4598\n",
            "[flaml.automl: 12-27 12:32:28] {3166} INFO - iteration 36, current learner lgbm\n",
            "INFO:flaml.automl:iteration 36, current learner lgbm\n",
            "[flaml.automl: 12-27 12:33:30] {3343} INFO -  at 1389.3s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 1389.3s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 12:33:30] {3166} INFO - iteration 37, current learner lgbm\n",
            "INFO:flaml.automl:iteration 37, current learner lgbm\n",
            "[flaml.automl: 12-27 12:34:15] {3343} INFO -  at 1434.2s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 1434.2s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 12:34:15] {3166} INFO - iteration 38, current learner lgbm\n",
            "INFO:flaml.automl:iteration 38, current learner lgbm\n",
            "[flaml.automl: 12-27 12:37:03] {3343} INFO -  at 1602.6s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 1602.6s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 12:37:04] {3166} INFO - iteration 39, current learner lgbm\n",
            "INFO:flaml.automl:iteration 39, current learner lgbm\n",
            "[flaml.automl: 12-27 12:37:18] {3343} INFO -  at 1616.9s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 1616.9s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 12:37:18] {3166} INFO - iteration 40, current learner lgbm\n",
            "INFO:flaml.automl:iteration 40, current learner lgbm\n",
            "[flaml.automl: 12-27 12:37:34] {3343} INFO -  at 1633.2s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 1633.2s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 12:37:34] {3166} INFO - iteration 41, current learner lgbm\n",
            "INFO:flaml.automl:iteration 41, current learner lgbm\n",
            "[flaml.automl: 12-27 12:41:45] {3343} INFO -  at 1884.3s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 1884.3s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 12:41:45] {3166} INFO - iteration 42, current learner lgbm\n",
            "INFO:flaml.automl:iteration 42, current learner lgbm\n",
            "[flaml.automl: 12-27 12:48:19] {3343} INFO -  at 2278.0s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 2278.0s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 12:48:19] {3166} INFO - iteration 43, current learner lgbm\n",
            "INFO:flaml.automl:iteration 43, current learner lgbm\n",
            "[flaml.automl: 12-27 12:48:47] {3343} INFO -  at 2306.2s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 2306.2s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 12:48:47] {3166} INFO - iteration 44, current learner lgbm\n",
            "INFO:flaml.automl:iteration 44, current learner lgbm\n",
            "[flaml.automl: 12-27 12:49:01] {3343} INFO -  at 2320.0s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 2320.0s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 12:49:01] {3166} INFO - iteration 45, current learner lgbm\n",
            "INFO:flaml.automl:iteration 45, current learner lgbm\n",
            "[flaml.automl: 12-27 12:53:28] {3343} INFO -  at 2587.5s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 2587.5s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 12:53:28] {3166} INFO - iteration 46, current learner lgbm\n",
            "INFO:flaml.automl:iteration 46, current learner lgbm\n",
            "[flaml.automl: 12-27 12:53:39] {3343} INFO -  at 2598.2s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 2598.2s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 12:53:39] {3166} INFO - iteration 47, current learner lgbm\n",
            "INFO:flaml.automl:iteration 47, current learner lgbm\n",
            "[flaml.automl: 12-27 12:59:53] {3343} INFO -  at 2971.9s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 2971.9s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 12:59:53] {3166} INFO - iteration 48, current learner lgbm\n",
            "INFO:flaml.automl:iteration 48, current learner lgbm\n",
            "[flaml.automl: 12-27 13:00:01] {3343} INFO -  at 2980.0s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 2980.0s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 13:00:01] {3166} INFO - iteration 49, current learner lgbm\n",
            "INFO:flaml.automl:iteration 49, current learner lgbm\n",
            "[flaml.automl: 12-27 13:08:02] {3343} INFO -  at 3461.5s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 3461.5s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 13:08:02] {3166} INFO - iteration 50, current learner lgbm\n",
            "INFO:flaml.automl:iteration 50, current learner lgbm\n",
            "[flaml.automl: 12-27 13:10:20] {3343} INFO -  at 3599.2s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "INFO:flaml.automl: at 3599.2s,\testimator lgbm's best error=0.4554,\tbest estimator lgbm's best error=0.4554\n",
            "[flaml.automl: 12-27 13:11:24] {3602} INFO - retrain lgbm for 64.1s\n",
            "INFO:flaml.automl:retrain lgbm for 64.1s\n",
            "[flaml.automl: 12-27 13:11:24] {3609} INFO - retrained model: LGBMClassifier(colsample_bytree=0.7280118688270512,\n",
            "               learning_rate=0.05629360243085908, max_bin=1023,\n",
            "               min_child_samples=6, n_estimators=285, num_leaves=30,\n",
            "               reg_alpha=0.003560020570039985, reg_lambda=0.2390587021297491,\n",
            "               verbose=-1)\n",
            "INFO:flaml.automl:retrained model: LGBMClassifier(colsample_bytree=0.7280118688270512,\n",
            "               learning_rate=0.05629360243085908, max_bin=1023,\n",
            "               min_child_samples=6, n_estimators=285, num_leaves=30,\n",
            "               reg_alpha=0.003560020570039985, reg_lambda=0.2390587021297491,\n",
            "               verbose=-1)\n",
            "[flaml.automl: 12-27 13:11:24] {2901} INFO - fit succeeded\n",
            "INFO:flaml.automl:fit succeeded\n",
            "[flaml.automl: 12-27 13:11:24] {2902} INFO - Time taken to find the best model: 1389.3141596317291\n",
            "INFO:flaml.automl:Time taken to find the best model: 1389.3141596317291\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparmeter config: {'n_estimators': 285, 'num_leaves': 30, 'min_child_samples': 6, 'learning_rate': 0.05629360243085908, 'log_max_bin': 10, 'colsample_bytree': 0.7280118688270512, 'reg_alpha': 0.003560020570039985, 'reg_lambda': 0.2390587021297491}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzA8kgvXB4ar"
      },
      "source": [
        "TfidfVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMHBdHFMB4ar",
        "outputId": "889d8cf2-a316-47c7-dd81-7b4715d7ce4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[flaml.automl: 12-27 13:11:26] {2599} INFO - task = classification\n",
            "INFO:flaml.automl:task = classification\n",
            "[flaml.automl: 12-27 13:11:26] {2601} INFO - Data split method: stratified\n",
            "INFO:flaml.automl:Data split method: stratified\n",
            "[flaml.automl: 12-27 13:11:26] {2604} INFO - Evaluation method: holdout\n",
            "INFO:flaml.automl:Evaluation method: holdout\n",
            "[flaml.automl: 12-27 13:11:26] {2726} INFO - Minimizing error metric: 1-accuracy\n",
            "INFO:flaml.automl:Minimizing error metric: 1-accuracy\n",
            "[flaml.automl: 12-27 13:11:26] {2870} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
            "INFO:flaml.automl:List of ML learners in AutoML Run: ['lgbm']\n",
            "[flaml.automl: 12-27 13:11:26] {3166} INFO - iteration 0, current learner lgbm\n",
            "INFO:flaml.automl:iteration 0, current learner lgbm\n",
            "[flaml.automl: 12-27 13:11:28] {3296} INFO - Estimated sufficient time budget=20373s. Estimated necessary time budget=20s.\n",
            "INFO:flaml.automl:Estimated sufficient time budget=20373s. Estimated necessary time budget=20s.\n",
            "[flaml.automl: 12-27 13:11:28] {3343} INFO -  at 2.1s,\testimator lgbm's best error=0.7166,\tbest estimator lgbm's best error=0.7166\n",
            "INFO:flaml.automl: at 2.1s,\testimator lgbm's best error=0.7166,\tbest estimator lgbm's best error=0.7166\n",
            "[flaml.automl: 12-27 13:11:28] {3166} INFO - iteration 1, current learner lgbm\n",
            "INFO:flaml.automl:iteration 1, current learner lgbm\n",
            "[flaml.automl: 12-27 13:11:30] {3343} INFO -  at 3.9s,\testimator lgbm's best error=0.7166,\tbest estimator lgbm's best error=0.7166\n",
            "INFO:flaml.automl: at 3.9s,\testimator lgbm's best error=0.7166,\tbest estimator lgbm's best error=0.7166\n",
            "[flaml.automl: 12-27 13:11:30] {3166} INFO - iteration 2, current learner lgbm\n",
            "INFO:flaml.automl:iteration 2, current learner lgbm\n",
            "[flaml.automl: 12-27 13:11:32] {3343} INFO -  at 6.4s,\testimator lgbm's best error=0.6745,\tbest estimator lgbm's best error=0.6745\n",
            "INFO:flaml.automl: at 6.4s,\testimator lgbm's best error=0.6745,\tbest estimator lgbm's best error=0.6745\n",
            "[flaml.automl: 12-27 13:11:32] {3166} INFO - iteration 3, current learner lgbm\n",
            "INFO:flaml.automl:iteration 3, current learner lgbm\n",
            "[flaml.automl: 12-27 13:11:37] {3343} INFO -  at 10.8s,\testimator lgbm's best error=0.6429,\tbest estimator lgbm's best error=0.6429\n",
            "INFO:flaml.automl: at 10.8s,\testimator lgbm's best error=0.6429,\tbest estimator lgbm's best error=0.6429\n",
            "[flaml.automl: 12-27 13:11:37] {3166} INFO - iteration 4, current learner lgbm\n",
            "INFO:flaml.automl:iteration 4, current learner lgbm\n",
            "[flaml.automl: 12-27 13:11:39] {3343} INFO -  at 13.3s,\testimator lgbm's best error=0.6429,\tbest estimator lgbm's best error=0.6429\n",
            "INFO:flaml.automl: at 13.3s,\testimator lgbm's best error=0.6429,\tbest estimator lgbm's best error=0.6429\n",
            "[flaml.automl: 12-27 13:11:39] {3166} INFO - iteration 5, current learner lgbm\n",
            "INFO:flaml.automl:iteration 5, current learner lgbm\n",
            "[flaml.automl: 12-27 13:11:44] {3343} INFO -  at 18.6s,\testimator lgbm's best error=0.6176,\tbest estimator lgbm's best error=0.6176\n",
            "INFO:flaml.automl: at 18.6s,\testimator lgbm's best error=0.6176,\tbest estimator lgbm's best error=0.6176\n",
            "[flaml.automl: 12-27 13:11:44] {3166} INFO - iteration 6, current learner lgbm\n",
            "INFO:flaml.automl:iteration 6, current learner lgbm\n",
            "[flaml.automl: 12-27 13:11:49] {3343} INFO -  at 23.4s,\testimator lgbm's best error=0.6176,\tbest estimator lgbm's best error=0.6176\n",
            "INFO:flaml.automl: at 23.4s,\testimator lgbm's best error=0.6176,\tbest estimator lgbm's best error=0.6176\n",
            "[flaml.automl: 12-27 13:11:49] {3166} INFO - iteration 7, current learner lgbm\n",
            "INFO:flaml.automl:iteration 7, current learner lgbm\n",
            "[flaml.automl: 12-27 13:11:55] {3343} INFO -  at 28.9s,\testimator lgbm's best error=0.6176,\tbest estimator lgbm's best error=0.6176\n",
            "INFO:flaml.automl: at 28.9s,\testimator lgbm's best error=0.6176,\tbest estimator lgbm's best error=0.6176\n",
            "[flaml.automl: 12-27 13:11:55] {3166} INFO - iteration 8, current learner lgbm\n",
            "INFO:flaml.automl:iteration 8, current learner lgbm\n",
            "[flaml.automl: 12-27 13:12:05] {3343} INFO -  at 39.0s,\testimator lgbm's best error=0.5965,\tbest estimator lgbm's best error=0.5965\n",
            "INFO:flaml.automl: at 39.0s,\testimator lgbm's best error=0.5965,\tbest estimator lgbm's best error=0.5965\n",
            "[flaml.automl: 12-27 13:12:05] {3166} INFO - iteration 9, current learner lgbm\n",
            "INFO:flaml.automl:iteration 9, current learner lgbm\n",
            "[flaml.automl: 12-27 13:12:10] {3343} INFO -  at 44.1s,\testimator lgbm's best error=0.5965,\tbest estimator lgbm's best error=0.5965\n",
            "INFO:flaml.automl: at 44.1s,\testimator lgbm's best error=0.5965,\tbest estimator lgbm's best error=0.5965\n",
            "[flaml.automl: 12-27 13:12:10] {3166} INFO - iteration 10, current learner lgbm\n",
            "INFO:flaml.automl:iteration 10, current learner lgbm\n",
            "[flaml.automl: 12-27 13:13:30] {3343} INFO -  at 124.2s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "INFO:flaml.automl: at 124.2s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "[flaml.automl: 12-27 13:13:30] {3166} INFO - iteration 11, current learner lgbm\n",
            "INFO:flaml.automl:iteration 11, current learner lgbm\n",
            "[flaml.automl: 12-27 13:15:19] {3343} INFO -  at 233.6s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "INFO:flaml.automl: at 233.6s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "[flaml.automl: 12-27 13:15:19] {3166} INFO - iteration 12, current learner lgbm\n",
            "INFO:flaml.automl:iteration 12, current learner lgbm\n",
            "[flaml.automl: 12-27 13:16:10] {3343} INFO -  at 284.1s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "INFO:flaml.automl: at 284.1s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "[flaml.automl: 12-27 13:16:10] {3166} INFO - iteration 13, current learner lgbm\n",
            "INFO:flaml.automl:iteration 13, current learner lgbm\n",
            "[flaml.automl: 12-27 13:16:54] {3343} INFO -  at 328.5s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "INFO:flaml.automl: at 328.5s,\testimator lgbm's best error=0.4752,\tbest estimator lgbm's best error=0.4752\n",
            "[flaml.automl: 12-27 13:16:54] {3166} INFO - iteration 14, current learner lgbm\n",
            "INFO:flaml.automl:iteration 14, current learner lgbm\n",
            "[flaml.automl: 12-27 13:19:22] {3343} INFO -  at 476.7s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "INFO:flaml.automl: at 476.7s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "[flaml.automl: 12-27 13:19:22] {3166} INFO - iteration 15, current learner lgbm\n",
            "INFO:flaml.automl:iteration 15, current learner lgbm\n",
            "[flaml.automl: 12-27 13:21:19] {3343} INFO -  at 592.8s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "INFO:flaml.automl: at 592.8s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "[flaml.automl: 12-27 13:21:19] {3166} INFO - iteration 16, current learner lgbm\n",
            "INFO:flaml.automl:iteration 16, current learner lgbm\n",
            "[flaml.automl: 12-27 13:22:36] {3343} INFO -  at 670.2s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "INFO:flaml.automl: at 670.2s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "[flaml.automl: 12-27 13:22:36] {3166} INFO - iteration 17, current learner lgbm\n",
            "INFO:flaml.automl:iteration 17, current learner lgbm\n",
            "[flaml.automl: 12-27 13:28:25] {3343} INFO -  at 1019.6s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "INFO:flaml.automl: at 1019.6s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "[flaml.automl: 12-27 13:28:25] {3166} INFO - iteration 18, current learner lgbm\n",
            "INFO:flaml.automl:iteration 18, current learner lgbm\n",
            "[flaml.automl: 12-27 13:30:05] {3343} INFO -  at 1118.9s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "INFO:flaml.automl: at 1118.9s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "[flaml.automl: 12-27 13:30:05] {3166} INFO - iteration 19, current learner lgbm\n",
            "INFO:flaml.automl:iteration 19, current learner lgbm\n",
            "[flaml.automl: 12-27 13:30:55] {3343} INFO -  at 1169.6s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "INFO:flaml.automl: at 1169.6s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "[flaml.automl: 12-27 13:30:55] {3166} INFO - iteration 20, current learner lgbm\n",
            "INFO:flaml.automl:iteration 20, current learner lgbm\n",
            "[flaml.automl: 12-27 13:36:25] {3343} INFO -  at 1499.2s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "INFO:flaml.automl: at 1499.2s,\testimator lgbm's best error=0.4734,\tbest estimator lgbm's best error=0.4734\n",
            "[flaml.automl: 12-27 13:36:25] {3166} INFO - iteration 21, current learner lgbm\n",
            "INFO:flaml.automl:iteration 21, current learner lgbm\n",
            "[flaml.automl: 12-27 13:39:29] {3343} INFO -  at 1683.6s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "INFO:flaml.automl: at 1683.6s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "[flaml.automl: 12-27 13:39:29] {3166} INFO - iteration 22, current learner lgbm\n",
            "INFO:flaml.automl:iteration 22, current learner lgbm\n",
            "[flaml.automl: 12-27 13:41:52] {3343} INFO -  at 1826.7s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "INFO:flaml.automl: at 1826.7s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "[flaml.automl: 12-27 13:41:52] {3166} INFO - iteration 23, current learner lgbm\n",
            "INFO:flaml.automl:iteration 23, current learner lgbm\n",
            "[flaml.automl: 12-27 13:44:13] {3343} INFO -  at 1967.4s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "INFO:flaml.automl: at 1967.4s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "[flaml.automl: 12-27 13:44:13] {3166} INFO - iteration 24, current learner lgbm\n",
            "INFO:flaml.automl:iteration 24, current learner lgbm\n",
            "[flaml.automl: 12-27 13:47:34] {3343} INFO -  at 2168.4s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "INFO:flaml.automl: at 2168.4s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "[flaml.automl: 12-27 13:47:34] {3166} INFO - iteration 25, current learner lgbm\n",
            "INFO:flaml.automl:iteration 25, current learner lgbm\n",
            "[flaml.automl: 12-27 13:56:28] {3343} INFO -  at 2702.8s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "INFO:flaml.automl: at 2702.8s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "[flaml.automl: 12-27 13:56:28] {3166} INFO - iteration 26, current learner lgbm\n",
            "INFO:flaml.automl:iteration 26, current learner lgbm\n",
            "[flaml.automl: 12-27 13:57:20] {3343} INFO -  at 2754.6s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "INFO:flaml.automl: at 2754.6s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "[flaml.automl: 12-27 13:57:20] {3166} INFO - iteration 27, current learner lgbm\n",
            "INFO:flaml.automl:iteration 27, current learner lgbm\n",
            "[flaml.automl: 12-27 14:04:49] {3343} INFO -  at 3203.5s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "INFO:flaml.automl: at 3203.5s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "[flaml.automl: 12-27 14:04:49] {3166} INFO - iteration 28, current learner lgbm\n",
            "INFO:flaml.automl:iteration 28, current learner lgbm\n",
            "[flaml.automl: 12-27 14:06:04] {3343} INFO -  at 3278.7s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "INFO:flaml.automl: at 3278.7s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "[flaml.automl: 12-27 14:06:04] {3166} INFO - iteration 29, current learner lgbm\n",
            "INFO:flaml.automl:iteration 29, current learner lgbm\n",
            "[flaml.automl: 12-27 14:07:24] {3343} INFO -  at 3358.0s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "INFO:flaml.automl: at 3358.0s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "[flaml.automl: 12-27 14:07:24] {3166} INFO - iteration 30, current learner lgbm\n",
            "INFO:flaml.automl:iteration 30, current learner lgbm\n",
            "[flaml.automl: 12-27 14:10:34] {3343} INFO -  at 3548.3s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "INFO:flaml.automl: at 3548.3s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "[flaml.automl: 12-27 14:10:34] {3166} INFO - iteration 31, current learner lgbm\n",
            "INFO:flaml.automl:iteration 31, current learner lgbm\n",
            "[flaml.automl: 12-27 14:13:05] {3343} INFO -  at 3699.1s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "INFO:flaml.automl: at 3699.1s,\testimator lgbm's best error=0.4672,\tbest estimator lgbm's best error=0.4672\n",
            "[flaml.automl: 12-27 14:13:05] {3166} INFO - iteration 32, current learner lgbm\n",
            "INFO:flaml.automl:iteration 32, current learner lgbm\n",
            "[flaml.automl: 12-27 14:16:56] {3343} INFO -  at 3930.4s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "INFO:flaml.automl: at 3930.4s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "[flaml.automl: 12-27 14:16:56] {3166} INFO - iteration 33, current learner lgbm\n",
            "INFO:flaml.automl:iteration 33, current learner lgbm\n",
            "[flaml.automl: 12-27 14:22:18] {3343} INFO -  at 4252.5s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "INFO:flaml.automl: at 4252.5s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "[flaml.automl: 12-27 14:22:18] {3166} INFO - iteration 34, current learner lgbm\n",
            "INFO:flaml.automl:iteration 34, current learner lgbm\n",
            "[flaml.automl: 12-27 14:27:14] {3343} INFO -  at 4548.5s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "INFO:flaml.automl: at 4548.5s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "[flaml.automl: 12-27 14:27:14] {3166} INFO - iteration 35, current learner lgbm\n",
            "INFO:flaml.automl:iteration 35, current learner lgbm\n",
            "[flaml.automl: 12-27 14:35:08] {3343} INFO -  at 5022.6s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "INFO:flaml.automl: at 5022.6s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "[flaml.automl: 12-27 14:35:08] {3166} INFO - iteration 36, current learner lgbm\n",
            "INFO:flaml.automl:iteration 36, current learner lgbm\n",
            "[flaml.automl: 12-27 14:35:55] {3343} INFO -  at 5069.8s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "INFO:flaml.automl: at 5069.8s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "[flaml.automl: 12-27 14:35:55] {3166} INFO - iteration 37, current learner lgbm\n",
            "INFO:flaml.automl:iteration 37, current learner lgbm\n",
            "[flaml.automl: 12-27 14:42:58] {3343} INFO -  at 5492.1s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "INFO:flaml.automl: at 5492.1s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "[flaml.automl: 12-27 14:42:58] {3166} INFO - iteration 38, current learner lgbm\n",
            "INFO:flaml.automl:iteration 38, current learner lgbm\n",
            "[flaml.automl: 12-27 14:44:25] {3343} INFO -  at 5579.2s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "INFO:flaml.automl: at 5579.2s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "[flaml.automl: 12-27 14:44:25] {3166} INFO - iteration 39, current learner lgbm\n",
            "INFO:flaml.automl:iteration 39, current learner lgbm\n",
            "[flaml.automl: 12-27 14:49:31] {3343} INFO -  at 5884.8s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "INFO:flaml.automl: at 5884.8s,\testimator lgbm's best error=0.4592,\tbest estimator lgbm's best error=0.4592\n",
            "[flaml.automl: 12-27 14:53:38] {3602} INFO - retrain lgbm for 247.1s\n",
            "INFO:flaml.automl:retrain lgbm for 247.1s\n",
            "[flaml.automl: 12-27 14:53:38] {3609} INFO - retrained model: LGBMClassifier(colsample_bytree=0.7055270442796215,\n",
            "               learning_rate=0.09718960160234287, max_bin=511,\n",
            "               min_child_samples=4, n_estimators=383, num_leaves=16,\n",
            "               reg_alpha=0.2573377884230388, reg_lambda=0.004211675541031258,\n",
            "               verbose=-1)\n",
            "INFO:flaml.automl:retrained model: LGBMClassifier(colsample_bytree=0.7055270442796215,\n",
            "               learning_rate=0.09718960160234287, max_bin=511,\n",
            "               min_child_samples=4, n_estimators=383, num_leaves=16,\n",
            "               reg_alpha=0.2573377884230388, reg_lambda=0.004211675541031258,\n",
            "               verbose=-1)\n",
            "[flaml.automl: 12-27 14:53:38] {2901} INFO - fit succeeded\n",
            "INFO:flaml.automl:fit succeeded\n",
            "[flaml.automl: 12-27 14:53:38] {2902} INFO - Time taken to find the best model: 3930.443162918091\n",
            "INFO:flaml.automl:Time taken to find the best model: 3930.443162918091\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparmeter config: {'n_estimators': 383, 'num_leaves': 16, 'min_child_samples': 4, 'learning_rate': 0.09718960160234287, 'log_max_bin': 9, 'colsample_bytree': 0.7055270442796215, 'reg_alpha': 0.2573377884230388, 'reg_lambda': 0.004211675541031258}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=6000)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM5JkfmCB4ar"
      },
      "source": [
        "Spacy-Vektoren:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko-bxWNdB4ar"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "X = [tokenize_and_lemmatize(text) for text in documents]\n",
        "\n",
        "def document_vector(doc):\n",
        "    return np.mean([token.vector for token in doc], axis=0)\n",
        "X = [document_vector(nlp(text)) for text in documents]\n",
        "\n",
        "X = np.array(X)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46wUov08A10b"
      },
      "source": [
        "# Einstellen, welche Lyrik-Spalten verwendet werden sollen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBCaDSaDBCgA"
      },
      "outputs": [],
      "source": [
        "# Laden Sie die Textdokumente und die Klassenlabels\n",
        "documents = df['wosw_stemmed_lyrics'].tolist() # Liste von Textdokumenten\n",
        "labels = df['Genre'] # Liste von Klassenlabels (eins pro Dokument)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfdLS_UPBdSC"
      },
      "source": [
        "# 5. Grid-search SVM mit SKLEARN:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkyOUyg_BdSD"
      },
      "source": [
        "WIE ICH IN ZUKUNFT DIE OPTIMALEN PARAMETER RAUSFINDE:\n",
        "Ich taste mich erstmal mit den jetztigen ran, schau mir die optimalen an, dann\n",
        "werde ich eine Reihe von Paramtern die ungefähr um den optimalen Parameter liegen testen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al-eyQK6BdSD"
      },
      "source": [
        "CountVektorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuPCAmfGBdSD",
        "outputId": "67f18b3e-2aa5-42ce-bc3c-44538d1cd099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.224 total time= 3.1min\n",
            "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.226 total time= 3.1min\n",
            "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.211 total time= 3.1min\n",
            "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.223 total time= 3.1min\n",
            "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.214 total time= 3.1min\n",
            "[CV 1/5] END .......C=1, gamma=1, kernel=linear;, score=0.426 total time= 2.1min\n",
            "[CV 2/5] END .......C=1, gamma=1, kernel=linear;, score=0.422 total time= 2.2min\n",
            "[CV 3/5] END .......C=1, gamma=1, kernel=linear;, score=0.419 total time= 2.2min\n",
            "[CV 4/5] END .......C=1, gamma=1, kernel=linear;, score=0.429 total time= 2.2min\n",
            "[CV 5/5] END .......C=1, gamma=1, kernel=linear;, score=0.426 total time= 2.2min\n",
            "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.238 total time= 3.1min\n",
            "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.238 total time= 3.1min\n",
            "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.218 total time= 3.1min\n",
            "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.235 total time= 3.1min\n",
            "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.227 total time= 3.1min\n",
            "[CV 1/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.426 total time= 2.1min\n",
            "[CV 2/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.422 total time= 2.2min\n",
            "[CV 3/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.419 total time= 2.2min\n",
            "[CV 4/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.429 total time= 2.3min\n",
            "[CV 5/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.426 total time= 2.2min\n",
            "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.355 total time= 3.0min\n",
            "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.341 total time= 3.0min\n",
            "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.345 total time= 3.0min\n",
            "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.364 total time= 3.1min\n",
            "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.353 total time= 3.0min\n",
            "[CV 1/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.426 total time= 2.1min\n",
            "[CV 2/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.422 total time= 2.2min\n",
            "[CV 3/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.419 total time= 2.2min\n",
            "[CV 4/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.429 total time= 2.2min\n",
            "[CV 5/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.426 total time= 2.2min\n",
            "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=0.225 total time= 3.0min\n",
            "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=0.227 total time= 3.0min\n",
            "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=0.212 total time= 3.0min\n",
            "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=0.223 total time= 3.0min\n",
            "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=0.216 total time= 3.0min\n",
            "[CV 1/5] END ......C=10, gamma=1, kernel=linear;, score=0.423 total time= 2.1min\n",
            "[CV 2/5] END ......C=10, gamma=1, kernel=linear;, score=0.415 total time= 2.2min\n",
            "[CV 3/5] END ......C=10, gamma=1, kernel=linear;, score=0.402 total time= 2.2min\n",
            "[CV 4/5] END ......C=10, gamma=1, kernel=linear;, score=0.411 total time= 2.3min\n",
            "[CV 5/5] END ......C=10, gamma=1, kernel=linear;, score=0.400 total time= 2.3min\n",
            "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.237 total time= 3.0min\n",
            "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.238 total time= 3.0min\n",
            "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.217 total time= 3.0min\n",
            "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.234 total time= 3.0min\n",
            "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.229 total time= 3.0min\n",
            "[CV 1/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.423 total time= 2.2min\n",
            "[CV 2/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.415 total time= 2.2min\n",
            "[CV 3/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.402 total time= 2.2min\n",
            "[CV 4/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.411 total time= 2.3min\n",
            "[CV 5/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.400 total time= 2.3min\n",
            "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.366 total time= 3.0min\n",
            "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.343 total time= 3.0min\n",
            "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.356 total time= 3.0min\n",
            "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.373 total time= 3.0min\n",
            "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.361 total time= 3.1min\n",
            "[CV 1/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.423 total time= 2.2min\n",
            "[CV 2/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.415 total time= 2.2min\n",
            "[CV 3/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.402 total time= 2.2min\n",
            "[CV 4/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.411 total time= 2.3min\n",
            "[CV 5/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.400 total time= 2.3min\n",
            "[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=0.225 total time= 3.1min\n",
            "[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=0.227 total time= 3.1min\n",
            "[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=0.212 total time= 3.1min\n",
            "[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=0.223 total time= 3.1min\n",
            "[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=0.216 total time= 3.1min\n",
            "[CV 1/5] END .....C=100, gamma=1, kernel=linear;, score=0.420 total time= 2.2min\n",
            "[CV 2/5] END .....C=100, gamma=1, kernel=linear;, score=0.415 total time= 2.2min\n",
            "[CV 3/5] END .....C=100, gamma=1, kernel=linear;, score=0.401 total time= 2.3min\n",
            "[CV 4/5] END .....C=100, gamma=1, kernel=linear;, score=0.405 total time= 2.3min\n",
            "[CV 5/5] END .....C=100, gamma=1, kernel=linear;, score=0.398 total time= 2.3min\n",
            "[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.237 total time= 3.1min\n",
            "[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.238 total time= 3.1min\n",
            "[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.217 total time= 3.0min\n",
            "[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.234 total time= 3.0min\n",
            "[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.229 total time= 3.0min\n",
            "[CV 1/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.420 total time= 2.2min\n",
            "[CV 2/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.415 total time= 2.2min\n",
            "[CV 3/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.401 total time= 2.3min\n",
            "[CV 4/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.405 total time= 2.3min\n",
            "[CV 5/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.398 total time= 2.3min\n",
            "[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.367 total time= 3.1min\n",
            "[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.346 total time= 3.0min\n",
            "[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.355 total time= 3.0min\n",
            "[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.373 total time= 3.0min\n",
            "[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.357 total time= 3.0min\n",
            "[CV 1/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.420 total time= 2.2min\n",
            "[CV 2/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.415 total time= 2.2min\n",
            "[CV 3/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.401 total time= 2.2min\n",
            "[CV 4/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.405 total time= 2.3min\n",
            "[CV 5/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.398 total time= 2.3min\n",
            "{'C': 1, 'gamma': 1, 'kernel': 'linear'}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [1, 10, 100], \n",
        "                  'gamma': [1, 0.1, 0.01],\n",
        "                  'kernel': ['rbf', 'linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmMLdL5DBdSD"
      },
      "source": [
        "SVM mit TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkQ8tv-7BdSE",
        "outputId": "78c5aaf5-9a9e-4c46-eed3-479295c87664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.489 total time= 2.8min\n",
            "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.467 total time= 2.7min\n",
            "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.480 total time= 2.8min\n",
            "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.481 total time= 2.8min\n",
            "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.483 total time= 2.8min\n",
            "[CV 1/5] END .......C=1, gamma=1, kernel=linear;, score=0.457 total time= 2.1min\n",
            "[CV 2/5] END .......C=1, gamma=1, kernel=linear;, score=0.446 total time= 2.1min\n",
            "[CV 3/5] END .......C=1, gamma=1, kernel=linear;, score=0.453 total time= 2.1min\n",
            "[CV 4/5] END .......C=1, gamma=1, kernel=linear;, score=0.459 total time= 2.1min\n",
            "[CV 5/5] END .......C=1, gamma=1, kernel=linear;, score=0.454 total time= 2.1min\n",
            "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.400 total time= 2.6min\n",
            "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.386 total time= 2.6min\n",
            "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.403 total time= 2.6min\n",
            "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.413 total time= 2.6min\n",
            "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.409 total time= 2.6min\n",
            "[CV 1/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.457 total time= 2.1min\n",
            "[CV 2/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.446 total time= 2.1min\n",
            "[CV 3/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.453 total time= 2.1min\n",
            "[CV 4/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.459 total time= 2.1min\n",
            "[CV 5/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.454 total time= 2.1min\n",
            "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.122 total time= 3.1min\n",
            "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.123 total time= 3.0min\n",
            "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.125 total time= 3.0min\n",
            "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.127 total time= 3.0min\n",
            "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.126 total time= 3.0min\n",
            "[CV 1/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.457 total time= 2.1min\n",
            "[CV 2/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.446 total time= 2.1min\n",
            "[CV 3/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.453 total time= 2.1min\n",
            "[CV 4/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.459 total time= 2.1min\n",
            "[CV 5/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.454 total time= 2.1min\n",
            "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=0.492 total time= 2.9min\n",
            "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=0.478 total time= 2.9min\n",
            "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=0.486 total time= 2.9min\n",
            "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=0.489 total time= 2.9min\n",
            "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=0.487 total time= 2.9min\n",
            "[CV 1/5] END ......C=10, gamma=1, kernel=linear;, score=0.456 total time= 2.4min\n",
            "[CV 2/5] END ......C=10, gamma=1, kernel=linear;, score=0.442 total time= 2.4min\n",
            "[CV 3/5] END ......C=10, gamma=1, kernel=linear;, score=0.443 total time= 2.4min\n",
            "[CV 4/5] END ......C=10, gamma=1, kernel=linear;, score=0.441 total time= 2.4min\n",
            "[CV 5/5] END ......C=10, gamma=1, kernel=linear;, score=0.438 total time= 2.4min\n",
            "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.474 total time= 2.5min\n",
            "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.458 total time= 2.4min\n",
            "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.458 total time= 2.5min\n",
            "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.460 total time= 2.5min\n",
            "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.453 total time= 2.5min\n",
            "[CV 1/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.456 total time= 2.4min\n",
            "[CV 2/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.442 total time= 2.4min\n",
            "[CV 3/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.443 total time= 2.4min\n",
            "[CV 4/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.441 total time= 2.4min\n",
            "[CV 5/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.438 total time= 2.4min\n",
            "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.407 total time= 2.6min\n",
            "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.399 total time= 2.6min\n",
            "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.413 total time= 2.6min\n",
            "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.419 total time= 2.7min\n",
            "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.416 total time= 2.7min\n",
            "[CV 1/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.456 total time= 2.4min\n",
            "[CV 2/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.442 total time= 2.4min\n",
            "[CV 3/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.443 total time= 2.5min\n",
            "[CV 4/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.441 total time= 2.4min\n",
            "[CV 5/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.438 total time= 2.4min\n",
            "[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=0.494 total time= 3.0min\n",
            "[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=0.479 total time= 2.9min\n",
            "[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=0.486 total time= 2.9min\n",
            "[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=0.488 total time= 2.9min\n",
            "[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=0.485 total time= 2.9min\n",
            "[CV 1/5] END .....C=100, gamma=1, kernel=linear;, score=0.451 total time= 2.4min\n",
            "[CV 2/5] END .....C=100, gamma=1, kernel=linear;, score=0.435 total time= 2.4min\n",
            "[CV 3/5] END .....C=100, gamma=1, kernel=linear;, score=0.436 total time= 2.4min\n",
            "[CV 4/5] END .....C=100, gamma=1, kernel=linear;, score=0.438 total time= 2.4min\n",
            "[CV 5/5] END .....C=100, gamma=1, kernel=linear;, score=0.432 total time= 2.4min\n",
            "[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.470 total time= 2.7min\n",
            "[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.455 total time= 2.6min\n",
            "[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.451 total time= 2.7min\n",
            "[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.453 total time= 2.7min\n",
            "[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.452 total time= 2.6min\n",
            "[CV 1/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.451 total time= 2.4min\n",
            "[CV 2/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.435 total time= 2.4min\n",
            "[CV 3/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.436 total time= 2.4min\n",
            "[CV 4/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.438 total time= 2.4min\n",
            "[CV 5/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.432 total time= 2.4min\n",
            "[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.468 total time= 2.3min\n",
            "[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.454 total time= 2.3min\n",
            "[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.452 total time= 2.3min\n",
            "[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.454 total time= 2.4min\n",
            "[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.451 total time= 2.7min\n",
            "[CV 1/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.451 total time= 2.6min\n",
            "[CV 2/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.435 total time= 2.4min\n",
            "[CV 3/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.436 total time= 2.4min\n",
            "[CV 4/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.438 total time= 2.4min\n",
            "[CV 5/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.432 total time= 2.4min\n",
            "{'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [1, 10, 100], \n",
        "                  'gamma': [1, 0.1, 0.01],\n",
        "                  'kernel': ['rbf', 'linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GypYqXpLBdSE"
      },
      "source": [
        "Spacy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFTXBj1kBdSE"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "X = [tokenize_and_lemmatize(text) for text in documents]\n",
        "\n",
        "def document_vector(doc):\n",
        "    return np.mean([token.vector for token in doc], axis=0)\n",
        "\n",
        "X = [document_vector(nlp(text)) for text in documents]\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Erstellen Sie ein SVM-Modell\n",
        "svm = SVC()\n",
        "\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "# Definieren Sie die Hyperparameter, die Sie durchsuchen möchten\n",
        "param_grid_svm = {'C': [1, 10, 100], \n",
        "                  'gamma': [1, 0.1, 0.01],\n",
        "                  'kernel': ['rbf', 'linear']}\n",
        "\n",
        "# Erstelle eine GridSearchCV-Instanz \n",
        "grid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
        "\n",
        "# Trainiere dein Modell mit der GridSearchCV\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "# Auswähle den besten Parameter\n",
        "best_params_svm = grid_svm.best_params_\n",
        "\n",
        "# Die besten Parameter ausgeben\n",
        "print(best_params_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj-mISgrB4nG"
      },
      "source": [
        "# 5. Gridsearch LGBM mit FLAML:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAHyuaYHB4nH"
      },
      "source": [
        "CountVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOANSGIjB4nH",
        "outputId": "26ab56ed-20da-41dc-de64-c01ccdd044cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[flaml.automl: 12-27 14:53:39] {2599} INFO - task = classification\n",
            "INFO:flaml.automl:task = classification\n",
            "[flaml.automl: 12-27 14:53:39] {2601} INFO - Data split method: stratified\n",
            "INFO:flaml.automl:Data split method: stratified\n",
            "[flaml.automl: 12-27 14:53:39] {2604} INFO - Evaluation method: holdout\n",
            "INFO:flaml.automl:Evaluation method: holdout\n",
            "[flaml.automl: 12-27 14:53:39] {2726} INFO - Minimizing error metric: 1-accuracy\n",
            "INFO:flaml.automl:Minimizing error metric: 1-accuracy\n",
            "[flaml.automl: 12-27 14:53:39] {2870} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
            "INFO:flaml.automl:List of ML learners in AutoML Run: ['lgbm']\n",
            "[flaml.automl: 12-27 14:53:39] {3166} INFO - iteration 0, current learner lgbm\n",
            "INFO:flaml.automl:iteration 0, current learner lgbm\n",
            "[flaml.automl: 12-27 14:53:41] {3296} INFO - Estimated sufficient time budget=16332s. Estimated necessary time budget=16s.\n",
            "INFO:flaml.automl:Estimated sufficient time budget=16332s. Estimated necessary time budget=16s.\n",
            "[flaml.automl: 12-27 14:53:41] {3343} INFO -  at 1.7s,\testimator lgbm's best error=0.7129,\tbest estimator lgbm's best error=0.7129\n",
            "INFO:flaml.automl: at 1.7s,\testimator lgbm's best error=0.7129,\tbest estimator lgbm's best error=0.7129\n",
            "[flaml.automl: 12-27 14:53:41] {3166} INFO - iteration 1, current learner lgbm\n",
            "INFO:flaml.automl:iteration 1, current learner lgbm\n",
            "[flaml.automl: 12-27 14:53:42] {3343} INFO -  at 3.2s,\testimator lgbm's best error=0.7129,\tbest estimator lgbm's best error=0.7129\n",
            "INFO:flaml.automl: at 3.2s,\testimator lgbm's best error=0.7129,\tbest estimator lgbm's best error=0.7129\n",
            "[flaml.automl: 12-27 14:53:42] {3166} INFO - iteration 2, current learner lgbm\n",
            "INFO:flaml.automl:iteration 2, current learner lgbm\n",
            "[flaml.automl: 12-27 14:53:45] {3343} INFO -  at 5.3s,\testimator lgbm's best error=0.6887,\tbest estimator lgbm's best error=0.6887\n",
            "INFO:flaml.automl: at 5.3s,\testimator lgbm's best error=0.6887,\tbest estimator lgbm's best error=0.6887\n",
            "[flaml.automl: 12-27 14:53:45] {3166} INFO - iteration 3, current learner lgbm\n",
            "INFO:flaml.automl:iteration 3, current learner lgbm\n",
            "[flaml.automl: 12-27 14:53:47] {3343} INFO -  at 8.0s,\testimator lgbm's best error=0.6238,\tbest estimator lgbm's best error=0.6238\n",
            "INFO:flaml.automl: at 8.0s,\testimator lgbm's best error=0.6238,\tbest estimator lgbm's best error=0.6238\n",
            "[flaml.automl: 12-27 14:53:47] {3166} INFO - iteration 4, current learner lgbm\n",
            "INFO:flaml.automl:iteration 4, current learner lgbm\n",
            "[flaml.automl: 12-27 14:53:49] {3343} INFO -  at 10.2s,\testimator lgbm's best error=0.6238,\tbest estimator lgbm's best error=0.6238\n",
            "INFO:flaml.automl: at 10.2s,\testimator lgbm's best error=0.6238,\tbest estimator lgbm's best error=0.6238\n",
            "[flaml.automl: 12-27 14:53:49] {3166} INFO - iteration 5, current learner lgbm\n",
            "INFO:flaml.automl:iteration 5, current learner lgbm\n",
            "[flaml.automl: 12-27 14:53:53] {3343} INFO -  at 14.0s,\testimator lgbm's best error=0.6238,\tbest estimator lgbm's best error=0.6238\n",
            "INFO:flaml.automl: at 14.0s,\testimator lgbm's best error=0.6238,\tbest estimator lgbm's best error=0.6238\n",
            "[flaml.automl: 12-27 14:53:53] {3166} INFO - iteration 6, current learner lgbm\n",
            "INFO:flaml.automl:iteration 6, current learner lgbm\n",
            "[flaml.automl: 12-27 14:53:56] {3343} INFO -  at 16.2s,\testimator lgbm's best error=0.6238,\tbest estimator lgbm's best error=0.6238\n",
            "INFO:flaml.automl: at 16.2s,\testimator lgbm's best error=0.6238,\tbest estimator lgbm's best error=0.6238\n",
            "[flaml.automl: 12-27 14:53:56] {3166} INFO - iteration 7, current learner lgbm\n",
            "INFO:flaml.automl:iteration 7, current learner lgbm\n",
            "[flaml.automl: 12-27 14:53:58] {3343} INFO -  at 18.3s,\testimator lgbm's best error=0.6238,\tbest estimator lgbm's best error=0.6238\n",
            "INFO:flaml.automl: at 18.3s,\testimator lgbm's best error=0.6238,\tbest estimator lgbm's best error=0.6238\n",
            "[flaml.automl: 12-27 14:53:58] {3166} INFO - iteration 8, current learner lgbm\n",
            "INFO:flaml.automl:iteration 8, current learner lgbm\n",
            "[flaml.automl: 12-27 14:54:02] {3343} INFO -  at 22.6s,\testimator lgbm's best error=0.5842,\tbest estimator lgbm's best error=0.5842\n",
            "INFO:flaml.automl: at 22.6s,\testimator lgbm's best error=0.5842,\tbest estimator lgbm's best error=0.5842\n",
            "[flaml.automl: 12-27 14:54:02] {3166} INFO - iteration 9, current learner lgbm\n",
            "INFO:flaml.automl:iteration 9, current learner lgbm\n",
            "[flaml.automl: 12-27 14:54:05] {3343} INFO -  at 25.5s,\testimator lgbm's best error=0.5842,\tbest estimator lgbm's best error=0.5842\n",
            "INFO:flaml.automl: at 25.5s,\testimator lgbm's best error=0.5842,\tbest estimator lgbm's best error=0.5842\n",
            "[flaml.automl: 12-27 14:54:05] {3166} INFO - iteration 10, current learner lgbm\n",
            "INFO:flaml.automl:iteration 10, current learner lgbm\n",
            "[flaml.automl: 12-27 14:54:24] {3343} INFO -  at 44.5s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "INFO:flaml.automl: at 44.5s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "[flaml.automl: 12-27 14:54:24] {3166} INFO - iteration 11, current learner lgbm\n",
            "INFO:flaml.automl:iteration 11, current learner lgbm\n",
            "[flaml.automl: 12-27 14:54:40] {3343} INFO -  at 60.6s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "INFO:flaml.automl: at 60.6s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "[flaml.automl: 12-27 14:54:40] {3166} INFO - iteration 12, current learner lgbm\n",
            "INFO:flaml.automl:iteration 12, current learner lgbm\n",
            "[flaml.automl: 12-27 14:54:54] {3343} INFO -  at 74.6s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "INFO:flaml.automl: at 74.6s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "[flaml.automl: 12-27 14:54:54] {3166} INFO - iteration 13, current learner lgbm\n",
            "INFO:flaml.automl:iteration 13, current learner lgbm\n",
            "[flaml.automl: 12-27 14:55:07] {3343} INFO -  at 87.7s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "INFO:flaml.automl: at 87.7s,\testimator lgbm's best error=0.4827,\tbest estimator lgbm's best error=0.4827\n",
            "[flaml.automl: 12-27 14:55:07] {3166} INFO - iteration 14, current learner lgbm\n",
            "INFO:flaml.automl:iteration 14, current learner lgbm\n",
            "[flaml.automl: 12-27 14:55:34] {3343} INFO -  at 114.8s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "INFO:flaml.automl: at 114.8s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "[flaml.automl: 12-27 14:55:34] {3166} INFO - iteration 15, current learner lgbm\n",
            "INFO:flaml.automl:iteration 15, current learner lgbm\n",
            "[flaml.automl: 12-27 14:56:09] {3343} INFO -  at 149.3s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "INFO:flaml.automl: at 149.3s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "[flaml.automl: 12-27 14:56:09] {3166} INFO - iteration 16, current learner lgbm\n",
            "INFO:flaml.automl:iteration 16, current learner lgbm\n",
            "[flaml.automl: 12-27 14:56:38] {3343} INFO -  at 178.6s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "INFO:flaml.automl: at 178.6s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "[flaml.automl: 12-27 14:56:38] {3166} INFO - iteration 17, current learner lgbm\n",
            "INFO:flaml.automl:iteration 17, current learner lgbm\n",
            "[flaml.automl: 12-27 14:57:25] {3343} INFO -  at 225.3s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "INFO:flaml.automl: at 225.3s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "[flaml.automl: 12-27 14:57:25] {3166} INFO - iteration 18, current learner lgbm\n",
            "INFO:flaml.automl:iteration 18, current learner lgbm\n",
            "[flaml.automl: 12-27 14:57:56] {3343} INFO -  at 256.7s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "INFO:flaml.automl: at 256.7s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "[flaml.automl: 12-27 14:57:56] {3166} INFO - iteration 19, current learner lgbm\n",
            "INFO:flaml.automl:iteration 19, current learner lgbm\n",
            "[flaml.automl: 12-27 14:58:07] {3343} INFO -  at 267.3s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "INFO:flaml.automl: at 267.3s,\testimator lgbm's best error=0.4771,\tbest estimator lgbm's best error=0.4771\n",
            "[flaml.automl: 12-27 14:58:07] {3166} INFO - iteration 20, current learner lgbm\n",
            "INFO:flaml.automl:iteration 20, current learner lgbm\n",
            "[flaml.automl: 12-27 15:00:55] {3343} INFO -  at 436.0s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "INFO:flaml.automl: at 436.0s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "[flaml.automl: 12-27 15:00:55] {3166} INFO - iteration 21, current learner lgbm\n",
            "INFO:flaml.automl:iteration 21, current learner lgbm\n",
            "[flaml.automl: 12-27 15:05:00] {3343} INFO -  at 681.0s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "INFO:flaml.automl: at 681.0s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "[flaml.automl: 12-27 15:05:00] {3166} INFO - iteration 22, current learner lgbm\n",
            "INFO:flaml.automl:iteration 22, current learner lgbm\n",
            "[flaml.automl: 12-27 15:06:11] {3343} INFO -  at 752.1s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "INFO:flaml.automl: at 752.1s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "[flaml.automl: 12-27 15:06:11] {3166} INFO - iteration 23, current learner lgbm\n",
            "INFO:flaml.automl:iteration 23, current learner lgbm\n",
            "[flaml.automl: 12-27 15:07:16] {3343} INFO -  at 817.1s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "INFO:flaml.automl: at 817.1s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "[flaml.automl: 12-27 15:07:16] {3166} INFO - iteration 24, current learner lgbm\n",
            "INFO:flaml.automl:iteration 24, current learner lgbm\n",
            "[flaml.automl: 12-27 15:09:21] {3343} INFO -  at 941.9s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "INFO:flaml.automl: at 941.9s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "[flaml.automl: 12-27 15:09:21] {3166} INFO - iteration 25, current learner lgbm\n",
            "INFO:flaml.automl:iteration 25, current learner lgbm\n",
            "[flaml.automl: 12-27 15:13:46] {3343} INFO -  at 1206.7s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "INFO:flaml.automl: at 1206.7s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "[flaml.automl: 12-27 15:13:46] {3166} INFO - iteration 26, current learner lgbm\n",
            "INFO:flaml.automl:iteration 26, current learner lgbm\n",
            "[flaml.automl: 12-27 15:15:07] {3343} INFO -  at 1287.8s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "INFO:flaml.automl: at 1287.8s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "[flaml.automl: 12-27 15:15:07] {3166} INFO - iteration 27, current learner lgbm\n",
            "INFO:flaml.automl:iteration 27, current learner lgbm\n",
            "[flaml.automl: 12-27 15:21:48] {3343} INFO -  at 1688.8s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "INFO:flaml.automl: at 1688.8s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "[flaml.automl: 12-27 15:21:48] {3166} INFO - iteration 28, current learner lgbm\n",
            "INFO:flaml.automl:iteration 28, current learner lgbm\n",
            "[flaml.automl: 12-27 15:22:11] {3343} INFO -  at 1711.7s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "INFO:flaml.automl: at 1711.7s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "[flaml.automl: 12-27 15:22:11] {3166} INFO - iteration 29, current learner lgbm\n",
            "INFO:flaml.automl:iteration 29, current learner lgbm\n",
            "[flaml.automl: 12-27 15:23:30] {3343} INFO -  at 1791.1s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "INFO:flaml.automl: at 1791.1s,\testimator lgbm's best error=0.4579,\tbest estimator lgbm's best error=0.4579\n",
            "[flaml.automl: 12-27 15:23:30] {3166} INFO - iteration 30, current learner lgbm\n",
            "INFO:flaml.automl:iteration 30, current learner lgbm\n",
            "[flaml.automl: 12-27 15:27:24] {3343} INFO -  at 2024.3s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "INFO:flaml.automl: at 2024.3s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "[flaml.automl: 12-27 15:27:24] {3166} INFO - iteration 31, current learner lgbm\n",
            "INFO:flaml.automl:iteration 31, current learner lgbm\n",
            "[flaml.automl: 12-27 15:31:13] {3343} INFO -  at 2253.4s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "INFO:flaml.automl: at 2253.4s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "[flaml.automl: 12-27 15:31:13] {3166} INFO - iteration 32, current learner lgbm\n",
            "INFO:flaml.automl:iteration 32, current learner lgbm\n",
            "[flaml.automl: 12-27 15:32:51] {3343} INFO -  at 2351.5s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "INFO:flaml.automl: at 2351.5s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "[flaml.automl: 12-27 15:32:51] {3166} INFO - iteration 33, current learner lgbm\n",
            "INFO:flaml.automl:iteration 33, current learner lgbm\n",
            "[flaml.automl: 12-27 15:35:21] {3343} INFO -  at 2501.5s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "INFO:flaml.automl: at 2501.5s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "[flaml.automl: 12-27 15:35:21] {3166} INFO - iteration 34, current learner lgbm\n",
            "INFO:flaml.automl:iteration 34, current learner lgbm\n",
            "[flaml.automl: 12-27 15:40:08] {3343} INFO -  at 2788.2s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "INFO:flaml.automl: at 2788.2s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "[flaml.automl: 12-27 15:40:08] {3166} INFO - iteration 35, current learner lgbm\n",
            "INFO:flaml.automl:iteration 35, current learner lgbm\n",
            "[flaml.automl: 12-27 15:52:52] {3343} INFO -  at 3553.0s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "INFO:flaml.automl: at 3553.0s,\testimator lgbm's best error=0.4517,\tbest estimator lgbm's best error=0.4517\n",
            "[flaml.automl: 12-27 15:57:36] {3602} INFO - retrain lgbm for 283.9s\n",
            "INFO:flaml.automl:retrain lgbm for 283.9s\n",
            "[flaml.automl: 12-27 15:57:36] {3609} INFO - retrained model: LGBMClassifier(colsample_bytree=0.5928354257895176,\n",
            "               learning_rate=0.08159729940633303, max_bin=511,\n",
            "               min_child_samples=6, n_estimators=222, num_leaves=446,\n",
            "               reg_alpha=0.01375448736501291, reg_lambda=0.3499177648353265,\n",
            "               verbose=-1)\n",
            "INFO:flaml.automl:retrained model: LGBMClassifier(colsample_bytree=0.5928354257895176,\n",
            "               learning_rate=0.08159729940633303, max_bin=511,\n",
            "               min_child_samples=6, n_estimators=222, num_leaves=446,\n",
            "               reg_alpha=0.01375448736501291, reg_lambda=0.3499177648353265,\n",
            "               verbose=-1)\n",
            "[flaml.automl: 12-27 15:57:36] {2901} INFO - fit succeeded\n",
            "INFO:flaml.automl:fit succeeded\n",
            "[flaml.automl: 12-27 15:57:36] {2902} INFO - Time taken to find the best model: 2024.252872467041\n",
            "INFO:flaml.automl:Time taken to find the best model: 2024.252872467041\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparmeter config: {'n_estimators': 222, 'num_leaves': 446, 'min_child_samples': 6, 'learning_rate': 0.08159729940633303, 'log_max_bin': 9, 'colsample_bytree': 0.5928354257895176, 'reg_alpha': 0.01375448736501291, 'reg_lambda': 0.3499177648353265}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWxCFe54B4nH"
      },
      "source": [
        "TfidfVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNsygShiB4nH",
        "outputId": "10578dcd-ccd9-4d55-dc73-20de860b524d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[flaml.automl: 12-28 11:49:39] {2599} INFO - task = classification\n",
            "INFO:flaml.automl:task = classification\n",
            "[flaml.automl: 12-28 11:49:39] {2601} INFO - Data split method: stratified\n",
            "INFO:flaml.automl:Data split method: stratified\n",
            "[flaml.automl: 12-28 11:49:39] {2604} INFO - Evaluation method: holdout\n",
            "INFO:flaml.automl:Evaluation method: holdout\n",
            "[flaml.automl: 12-28 11:49:39] {2726} INFO - Minimizing error metric: 1-accuracy\n",
            "INFO:flaml.automl:Minimizing error metric: 1-accuracy\n",
            "[flaml.automl: 12-28 11:49:39] {2870} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
            "INFO:flaml.automl:List of ML learners in AutoML Run: ['lgbm']\n",
            "[flaml.automl: 12-28 11:49:39] {3166} INFO - iteration 0, current learner lgbm\n",
            "INFO:flaml.automl:iteration 0, current learner lgbm\n",
            "[flaml.automl: 12-28 11:49:42] {3296} INFO - Estimated sufficient time budget=27219s. Estimated necessary time budget=27s.\n",
            "INFO:flaml.automl:Estimated sufficient time budget=27219s. Estimated necessary time budget=27s.\n",
            "[flaml.automl: 12-28 11:49:42] {3343} INFO -  at 2.9s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "INFO:flaml.automl: at 2.9s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl: 12-28 11:49:42] {3166} INFO - iteration 1, current learner lgbm\n",
            "INFO:flaml.automl:iteration 1, current learner lgbm\n",
            "[flaml.automl: 12-28 11:49:44] {3343} INFO -  at 5.3s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "INFO:flaml.automl: at 5.3s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl: 12-28 11:49:44] {3166} INFO - iteration 2, current learner lgbm\n",
            "INFO:flaml.automl:iteration 2, current learner lgbm\n",
            "[flaml.automl: 12-28 11:49:47] {3343} INFO -  at 8.2s,\testimator lgbm's best error=0.6807,\tbest estimator lgbm's best error=0.6807\n",
            "INFO:flaml.automl: at 8.2s,\testimator lgbm's best error=0.6807,\tbest estimator lgbm's best error=0.6807\n",
            "[flaml.automl: 12-28 11:49:47] {3166} INFO - iteration 3, current learner lgbm\n",
            "INFO:flaml.automl:iteration 3, current learner lgbm\n",
            "[flaml.automl: 12-28 11:49:53] {3343} INFO -  at 14.3s,\testimator lgbm's best error=0.6213,\tbest estimator lgbm's best error=0.6213\n",
            "INFO:flaml.automl: at 14.3s,\testimator lgbm's best error=0.6213,\tbest estimator lgbm's best error=0.6213\n",
            "[flaml.automl: 12-28 11:49:53] {3166} INFO - iteration 4, current learner lgbm\n",
            "INFO:flaml.automl:iteration 4, current learner lgbm\n",
            "[flaml.automl: 12-28 11:49:56] {3343} INFO -  at 17.3s,\testimator lgbm's best error=0.6213,\tbest estimator lgbm's best error=0.6213\n",
            "INFO:flaml.automl: at 17.3s,\testimator lgbm's best error=0.6213,\tbest estimator lgbm's best error=0.6213\n",
            "[flaml.automl: 12-28 11:49:56] {3166} INFO - iteration 5, current learner lgbm\n",
            "INFO:flaml.automl:iteration 5, current learner lgbm\n",
            "[flaml.automl: 12-28 11:50:04] {3343} INFO -  at 25.0s,\testimator lgbm's best error=0.6213,\tbest estimator lgbm's best error=0.6213\n",
            "INFO:flaml.automl: at 25.0s,\testimator lgbm's best error=0.6213,\tbest estimator lgbm's best error=0.6213\n",
            "[flaml.automl: 12-28 11:50:04] {3166} INFO - iteration 6, current learner lgbm\n",
            "INFO:flaml.automl:iteration 6, current learner lgbm\n",
            "[flaml.automl: 12-28 11:50:10] {3343} INFO -  at 30.9s,\testimator lgbm's best error=0.6213,\tbest estimator lgbm's best error=0.6213\n",
            "INFO:flaml.automl: at 30.9s,\testimator lgbm's best error=0.6213,\tbest estimator lgbm's best error=0.6213\n",
            "[flaml.automl: 12-28 11:50:10] {3166} INFO - iteration 7, current learner lgbm\n",
            "INFO:flaml.automl:iteration 7, current learner lgbm\n",
            "[flaml.automl: 12-28 11:50:14] {3343} INFO -  at 34.9s,\testimator lgbm's best error=0.6213,\tbest estimator lgbm's best error=0.6213\n",
            "INFO:flaml.automl: at 34.9s,\testimator lgbm's best error=0.6213,\tbest estimator lgbm's best error=0.6213\n",
            "[flaml.automl: 12-28 11:50:14] {3166} INFO - iteration 8, current learner lgbm\n",
            "INFO:flaml.automl:iteration 8, current learner lgbm\n",
            "[flaml.automl: 12-28 11:50:30] {3343} INFO -  at 51.3s,\testimator lgbm's best error=0.5916,\tbest estimator lgbm's best error=0.5916\n",
            "INFO:flaml.automl: at 51.3s,\testimator lgbm's best error=0.5916,\tbest estimator lgbm's best error=0.5916\n",
            "[flaml.automl: 12-28 11:50:30] {3166} INFO - iteration 9, current learner lgbm\n",
            "INFO:flaml.automl:iteration 9, current learner lgbm\n",
            "[flaml.automl: 12-28 11:50:37] {3343} INFO -  at 58.4s,\testimator lgbm's best error=0.5916,\tbest estimator lgbm's best error=0.5916\n",
            "INFO:flaml.automl: at 58.4s,\testimator lgbm's best error=0.5916,\tbest estimator lgbm's best error=0.5916\n",
            "[flaml.automl: 12-28 11:50:37] {3166} INFO - iteration 10, current learner lgbm\n",
            "INFO:flaml.automl:iteration 10, current learner lgbm\n",
            "[flaml.automl: 12-28 11:53:06] {3343} INFO -  at 207.4s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "INFO:flaml.automl: at 207.4s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "[flaml.automl: 12-28 11:53:06] {3166} INFO - iteration 11, current learner lgbm\n",
            "INFO:flaml.automl:iteration 11, current learner lgbm\n",
            "[flaml.automl: 12-28 11:55:22] {3343} INFO -  at 343.2s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "INFO:flaml.automl: at 343.2s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "[flaml.automl: 12-28 11:55:22] {3166} INFO - iteration 12, current learner lgbm\n",
            "INFO:flaml.automl:iteration 12, current learner lgbm\n",
            "[flaml.automl: 12-28 11:57:01] {3343} INFO -  at 442.1s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "INFO:flaml.automl: at 442.1s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "[flaml.automl: 12-28 11:57:01] {3166} INFO - iteration 13, current learner lgbm\n",
            "INFO:flaml.automl:iteration 13, current learner lgbm\n",
            "[flaml.automl: 12-28 11:58:10] {3343} INFO -  at 511.8s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "INFO:flaml.automl: at 511.8s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "[flaml.automl: 12-28 11:58:10] {3166} INFO - iteration 14, current learner lgbm\n",
            "INFO:flaml.automl:iteration 14, current learner lgbm\n",
            "[flaml.automl: 12-28 12:02:43] {3343} INFO -  at 784.2s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "INFO:flaml.automl: at 784.2s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "[flaml.automl: 12-28 12:02:43] {3166} INFO - iteration 15, current learner lgbm\n",
            "INFO:flaml.automl:iteration 15, current learner lgbm\n",
            "[flaml.automl: 12-28 12:07:41] {3343} INFO -  at 1082.5s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "INFO:flaml.automl: at 1082.5s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "[flaml.automl: 12-28 12:07:41] {3166} INFO - iteration 16, current learner lgbm\n",
            "INFO:flaml.automl:iteration 16, current learner lgbm\n",
            "[flaml.automl: 12-28 12:08:42] {3343} INFO -  at 1142.9s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "INFO:flaml.automl: at 1142.9s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "[flaml.automl: 12-28 12:08:42] {3166} INFO - iteration 17, current learner lgbm\n",
            "INFO:flaml.automl:iteration 17, current learner lgbm\n",
            "[flaml.automl: 12-28 12:14:29] {3343} INFO -  at 1490.2s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "INFO:flaml.automl: at 1490.2s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "[flaml.automl: 12-28 12:14:29] {3166} INFO - iteration 18, current learner lgbm\n",
            "INFO:flaml.automl:iteration 18, current learner lgbm\n",
            "[flaml.automl: 12-28 12:15:25] {3343} INFO -  at 1546.5s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "INFO:flaml.automl: at 1546.5s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "[flaml.automl: 12-28 12:15:25] {3166} INFO - iteration 19, current learner lgbm\n",
            "INFO:flaml.automl:iteration 19, current learner lgbm\n",
            "[flaml.automl: 12-28 12:16:09] {3343} INFO -  at 1589.8s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "INFO:flaml.automl: at 1589.8s,\testimator lgbm's best error=0.4715,\tbest estimator lgbm's best error=0.4715\n",
            "[flaml.automl: 12-28 12:16:09] {3166} INFO - iteration 20, current learner lgbm\n",
            "INFO:flaml.automl:iteration 20, current learner lgbm\n",
            "[flaml.automl: 12-28 12:24:17] {3343} INFO -  at 2078.5s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "INFO:flaml.automl: at 2078.5s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "[flaml.automl: 12-28 12:24:17] {3166} INFO - iteration 21, current learner lgbm\n",
            "INFO:flaml.automl:iteration 21, current learner lgbm\n",
            "[flaml.automl: 12-28 12:34:05] {3343} INFO -  at 2666.7s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "INFO:flaml.automl: at 2666.7s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "[flaml.automl: 12-28 12:34:05] {3166} INFO - iteration 22, current learner lgbm\n",
            "INFO:flaml.automl:iteration 22, current learner lgbm\n",
            "[flaml.automl: 12-28 12:39:43] {3343} INFO -  at 3004.1s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "INFO:flaml.automl: at 3004.1s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "[flaml.automl: 12-28 12:39:43] {3166} INFO - iteration 23, current learner lgbm\n",
            "INFO:flaml.automl:iteration 23, current learner lgbm\n",
            "[flaml.automl: 12-28 12:45:43] {3343} INFO -  at 3364.2s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "INFO:flaml.automl: at 3364.2s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "[flaml.automl: 12-28 12:45:43] {3166} INFO - iteration 24, current learner lgbm\n",
            "INFO:flaml.automl:iteration 24, current learner lgbm\n",
            "[flaml.automl: 12-28 12:54:38] {3343} INFO -  at 3899.3s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "INFO:flaml.automl: at 3899.3s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "[flaml.automl: 12-28 12:54:38] {3166} INFO - iteration 25, current learner lgbm\n",
            "INFO:flaml.automl:iteration 25, current learner lgbm\n",
            "[flaml.automl: 12-28 13:15:35] {3343} INFO -  at 5156.7s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "INFO:flaml.automl: at 5156.7s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "[flaml.automl: 12-28 13:15:35] {3166} INFO - iteration 26, current learner lgbm\n",
            "INFO:flaml.automl:iteration 26, current learner lgbm\n",
            "[flaml.automl: 12-28 13:17:50] {3343} INFO -  at 5290.8s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "INFO:flaml.automl: at 5290.8s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "[flaml.automl: 12-28 13:17:50] {3166} INFO - iteration 27, current learner lgbm\n",
            "INFO:flaml.automl:iteration 27, current learner lgbm\n",
            "[flaml.automl: 12-28 13:29:33] {3343} INFO -  at 5994.0s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "INFO:flaml.automl: at 5994.0s,\testimator lgbm's best error=0.4542,\tbest estimator lgbm's best error=0.4542\n",
            "[flaml.automl: 12-28 13:38:35] {3602} INFO - retrain lgbm for 542.0s\n",
            "INFO:flaml.automl:retrain lgbm for 542.0s\n",
            "[flaml.automl: 12-28 13:38:35] {3609} INFO - retrained model: LGBMClassifier(colsample_bytree=0.615957270023824,\n",
            "               learning_rate=0.054006958323122314, max_bin=511,\n",
            "               min_child_samples=5, n_estimators=233, num_leaves=35,\n",
            "               reg_alpha=0.0009765625, reg_lambda=0.16340223067803453,\n",
            "               verbose=-1)\n",
            "INFO:flaml.automl:retrained model: LGBMClassifier(colsample_bytree=0.615957270023824,\n",
            "               learning_rate=0.054006958323122314, max_bin=511,\n",
            "               min_child_samples=5, n_estimators=233, num_leaves=35,\n",
            "               reg_alpha=0.0009765625, reg_lambda=0.16340223067803453,\n",
            "               verbose=-1)\n",
            "[flaml.automl: 12-28 13:38:35] {2901} INFO - fit succeeded\n",
            "INFO:flaml.automl:fit succeeded\n",
            "[flaml.automl: 12-28 13:38:35] {2902} INFO - Time taken to find the best model: 2078.5156514644623\n",
            "INFO:flaml.automl:Time taken to find the best model: 2078.5156514644623\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparmeter config: {'n_estimators': 233, 'num_leaves': 35, 'min_child_samples': 5, 'learning_rate': 0.054006958323122314, 'log_max_bin': 9, 'colsample_bytree': 0.615957270023824, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.16340223067803453}\n"
          ]
        }
      ],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=6000)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy-Z5kwmB4nH"
      },
      "source": [
        "Spacy-Vektoren:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K90vKqRGB4nI"
      },
      "outputs": [],
      "source": [
        "# Konvertieren Sie die Textdokumente in numerische Merkmale mit dem CountVectorizer\n",
        "def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "X = [tokenize_and_lemmatize(text) for text in documents]\n",
        "\n",
        "def document_vector(doc):\n",
        "    return np.mean([token.vector for token in doc], axis=0)\n",
        "X = [document_vector(nlp(text)) for text in documents]\n",
        "\n",
        "X = np.array(X)\n",
        "\n",
        "# Teilen Sie die Daten in Trainings- und Testdaten auf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
        "\n",
        "# Das Flaml-Modell deklarieren und trainieren\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, task=\"classification\", metric='accuracy', estimator_list=[\"lgbm\"], time_budget=3600)\n",
        "\n",
        "# Die besten Hyperparameter ausgeben\n",
        "print('Best hyperparmeter config:', automl.best_config)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fyb6mPpCJADk",
        "D48vtzWGJAxi",
        "F1VuxiEPXkXT",
        "QrzHyXpSxgMt",
        "L22lHiC4JA3p",
        "fS06d_5cesIX",
        "Xg9p62FehEgR",
        "uCG7-T1DoyEC",
        "-PsyQj-_A0Ng",
        "5Wt8JjpTBajw",
        "C_90fW4BB3_p",
        "cRpm6PPNA00O",
        "QDfs9MKTB4NT",
        "7Nm8HxkuA1Sx",
        "0aLj0O1HBdG3",
        "uzun1qzrB4aq",
        "46wUov08A10b",
        "RfdLS_UPBdSC",
        "Zj-mISgrB4nG"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyPh9olOXQ7Be7aNUK6+odas",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}